{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **불용어 제거 및 표준화**\n",
    "- 한 행 씩 보면서, 불용어 표준화 진행을 위한 전처리를 먼저 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from ckonlpy.tag import Twitter as Okt\n",
    "from ckonlpy.tag import Postprocessor\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "logging.getLogger('konlpy').setLevel(logging.ERROR)\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "code_path = '/VSC/JJaemni/Code/'\n",
    "csv_path = '/VSC/JJaemni/CSV/'\n",
    "\n",
    "df = pd.read_csv(csv_path + 'wanted_jobplanet_2.csv')\n",
    "\n",
    "\n",
    "\n",
    "# 리스트 형태로 불러오기\n",
    "def text_list_load(text_path):\n",
    "    with open(text_path, 'r', encoding='utf-8-sig') as file:\n",
    "        L = [line.strip() for line in file.readlines()]\n",
    "    return L\n",
    "\n",
    "# 딕셔너리 형태로 불러오기\n",
    "def text_dict_load(text_path):\n",
    "    L = dict()\n",
    "    with open(text_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            key, val = line.strip().split(': ')\n",
    "            L[key] = val\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "# 한글 전처리\n",
    "def Korean_preprocessing(col):\n",
    "    col_list = df[col].tolist()\n",
    "    \n",
    "    # 한글, 영문 대소문자, 숫자, 공백을 제외한 문자를 제거\n",
    "    col_list2 = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        str = col_list[idx]\n",
    "        new_str = re.sub('\\n', '', str)\n",
    "        new_str2 = re.sub(r'[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]', ' ', new_str)\n",
    "        new_str3 = re.split(' ', new_str2)\n",
    "        new_str4 = ' '.join(filter(None, new_str3))\n",
    "        col_list2.append(new_str4)\n",
    "        # print(f'{idx}. {col_list2[idx]}')\n",
    "    col_list2_df = pd.DataFrame({'col_list': col_list2})\n",
    "\n",
    "\n",
    "    # Kor, Eng 분리\n",
    "    eng_list = []\n",
    "    kor_list = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        text = col_list2_df.col_list[idx]\n",
    "        eng_text = ' '.join(re.findall(r'[A-Za-z]+', text))\n",
    "        kor_text = re.sub(r'[A-Za-z]+', '', text).strip()\n",
    "        eng_list.append(eng_text)\n",
    "        kor_list.append(kor_text)\n",
    "\n",
    "\n",
    "    # 영어 띄어쓰기 기준으로 split\n",
    "    eng_list_spl = []\n",
    "    for eng in eng_list:\n",
    "        spl = eng.split()\n",
    "        eng_list_spl.append(spl)\n",
    "    # for idx, eng in enumerate(eng_list_spl):\n",
    "        # print(f'{idx}. {eng}')\n",
    "\n",
    "\n",
    "    # 영어 소문자로 통일\n",
    "    eng_lower = []\n",
    "    for sub in eng_list_spl:\n",
    "        sublist = []\n",
    "        for eng in sub:\n",
    "            eng_ = eng.lower()\n",
    "            sublist.append(eng_)\n",
    "        eng_lower.append(sublist)\n",
    "    # for idx, eng in enumerate(eng_lower):\n",
    "        # print(f'{idx}. {eng}')\n",
    "\n",
    "\n",
    "    # 텍스트 파일 불러오기\n",
    "    text_path = './custom_nouns.txt'\n",
    "    custom_nouns = text_list_load(text_path)\n",
    "    # print(custom_nouns)\n",
    "    text_path = './replace.txt'\n",
    "    replace = text_dict_load(text_path)\n",
    "    # print(replace)\n",
    "    text_path = './stopwords.txt'\n",
    "    stopwords = text_list_load(text_path)\n",
    "    # print(stopwords)\n",
    "\n",
    "\n",
    "    # Okt 시작\n",
    "    okt = Okt()\n",
    "\n",
    "\n",
    "    # 명사 추가\n",
    "    custom_nouns = custom_nouns\n",
    "    for noun in custom_nouns:\n",
    "        okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "\n",
    "    # 단어 치환\n",
    "    replace = replace\n",
    "\n",
    "\n",
    "    # 불용어 처리\n",
    "    stopwords = stopwords\n",
    "    postprocessor = Postprocessor(\n",
    "        base_tagger=okt,\n",
    "        passtags={'Noun'},\n",
    "        replace=replace,\n",
    "        stopwords=stopwords,\n",
    "        # passwords=passwords,\n",
    "        # ngrams=ngrams\n",
    "    )\n",
    "\n",
    "\n",
    "    # Postprocessor 처리 후, 한 글자인 단어 제거\n",
    "    results = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        elem = kor_list[idx]\n",
    "        # print(postprocessor.pos(elem))\n",
    "        post_onechar = postprocessor.pos(elem)\n",
    "        result = []\n",
    "        for word, tag in post_onechar:\n",
    "            if len(word) > 1:\n",
    "                result.append(word)\n",
    "        results.append(result)\n",
    "        # print(f'{idx}. {result}')\n",
    "\n",
    "\n",
    "    # 리스트 중복 요소 제거\n",
    "    kor_final = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        a = set(results[idx])\n",
    "        b = list(a)\n",
    "        kor_final.append(b)\n",
    "        print(f'{idx}. {b}')\n",
    "    # print(len(kor_final))\n",
    "\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    kor_final_df = pd.DataFrame({f'{col}_KOR': kor_final})\n",
    "    kor_final_df[f'{col}_KOR'] = kor_final_df[f'{col}_KOR'].apply(lambda x: ', '.join(x))\n",
    "    # kor_final_df.to_csv(csv_path + f'{col}_KOR.csv', index=False)\n",
    "\n",
    "    return col_list2, col_list2_df, eng_list, kor_list, eng_list_spl, eng_lower, custom_nouns, replace, stopwords, results, kor_final, kor_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Requirement_KOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. ['데이터 분석', '인공지능', '통계분석', '알고리즘 개발', '시계열 데이터', '논문 구현', '머신러닝']\n",
      "1. ['블록체인', '데이터 분석', '온체인 데이터', '데이터 사이언스', '머신러닝']\n",
      "2. ['데이터 분석', '블록체인', '실무경험', '문제 정의 및 해결', '데이터 사이언스', '알고리즘 개발', '논문 구현', '머신러닝']\n",
      "3. ['데이터 분석', '고객 분석', '개발역량', '상용 서비스 구현', '데이터 모델링', '인사이트 도출']\n",
      "4. ['데이터 모델링']\n",
      "5. ['데이터 분석', '적극적', '프레임워크', '커뮤니케이션', '협업', '주도적', '데이터 시각화', '모델 개발 및 배포', '프로젝트 경험', '모델 개발', '딥러닝', '포트폴리오', '개발경험']\n",
      "6. ['고객 분석', '커뮤니케이션', '협업', '실무경험', '수리적 지식', '프로그래밍 언어', '개인화 추천', '도메인 지식', '프로젝트 경험', '데이터 모델링', '데이터 사이언티스트']\n",
      "7. ['데이터 분석', '자연어 처리', '산업공학', '통계분석', '최적화', '알고리즘 개발', '도메인 지식', '컴퓨터공학', '데이터 모델링', '가설 수립 및 검증', '모델 개발']\n",
      "8. ['데이터 분석', '주도적', '실무경험', '문제 정의 및 해결', '모바일 데이터', '끈질김', '프로젝트 경험', '인사이트 도출', '머신러닝', '이슈 해결']\n",
      "9. ['데이터 분석', '열정적', '산업공학', '데이터 시각화', '최신 기술 연구', '통계분석', '컴퓨터공학']\n",
      "10. ['커뮤니케이션', '협업', '실무경험', '문제 정의 및 해결', '모델 개발', '프로젝트 경험', '논문 구현', '머신러닝', '리딩 능력']\n",
      "11. ['데이터 분석', '데이터 추출', '데이터 마트 구축 운영', '데이터 핸들링', '프로젝트 경험', '모델 개발', '데이터 전처리']\n",
      "12. ['데이터 분석', '커뮤니케이션', '협업', '수리적 지식', '문제 정의 및 해결', '시스템 개발', '통계적 지식', '알고리즘 개발', '도메인 지식', '데이터 모델링', '논문 구현', '머신러닝', '이슈 해결']\n",
      "13. ['커뮤니케이션', '협업', '실무경험', '문제 정의 및 해결', '프로젝트 경험', '논문 구현', '머신러닝']\n",
      "14. ['데이터 분석', '통계학과']\n",
      "15. ['데이터 분석', '데이터 사이언티스트', '통계적 지식', '통계분석', '알고리즘 개발', '도메인 지식', '모델 개발', '수리적 지식']\n",
      "16. ['데이터 분석', '커뮤니케이션', '수리적 지식', '최신 기술 연구', '문제 정의 및 해결', '통계적 지식', '알고리즘 개발', '도메인 지식', '데이터 모델링', '머신러닝']\n",
      "17. []\n",
      "18. ['데이터 분석', '인공지능', '통계분석', '알고리즘 개발', '시계열 데이터', '논문 구현', '머신러닝']\n",
      "19. ['데이터 분석', '데이터 사이언티스트', '통계적 지식', '통계분석', '알고리즘 개발', '도메인 지식', '모델 개발', '수리적 지식']\n",
      "20. []\n",
      "21. ['데이터 분석', '데이터 시각화', '머신러닝']\n",
      "22. ['데이터 분석', '통계학과']\n",
      "23. ['인과추론', '데이터 기반 의사결정', '문제 정의 및 해결', '통계분석', '통계적 지식', '도메인 지식', '편향 제거', '테스트 설계 분석']\n",
      "24. ['데이터 분석', '정형 데이터', '데이터 추출', '비정형 데이터', '데이터 모델링']\n",
      "25. ['프레임워크', '데이터 수집', '리포팅 능력', '데이터 가공', '통계적 지식', '데이터 정제', '데이터 추출', '도메인 지식', '데이터 핸들링', '데이터 모델링', '가설 수립 및 검증', '인사이트 도출']\n",
      "26. ['적극적', '문제 정의 및 해결', '논문 구현', '데이터 모델링', '모델 개발']\n",
      "27. ['데이터 분석', '커뮤니케이션', '프레임워크', '문제 정의 및 해결', '프로젝트 경험', '데이터 모델링', '모델 개발', '딥러닝']\n",
      "28. ['데이터 분석', '커뮤니케이션', '데이터 파이프라인 구축', '주도적', '최신 기술 연구', '꼼꼼함', '통계분석', '도메인 지식', '모델 개발']\n",
      "29. ['주도적', '최신 기술 연구', '문제 정의 및 해결', '이미지 데이터', '리드 능력', '도메인 지식', '프로젝트 경험', '책임감', '딥러닝', '머신러닝']\n",
      "30. ['영상 신호처리', '영상 처리', '도메인 지식', '논문 구현']\n",
      "31. ['주도적', '최신 기술 연구', '문제 정의 및 해결', '이미지 데이터', '리드 능력', '도메인 지식', '프로젝트 경험', '책임감', '딥러닝', '머신러닝']\n",
      "32. ['데이터 레이블링']\n",
      "33. ['데이터 분석', '주도적', '데이터 시각화', '가설 수립 및 검증', '프로덕트 데이터 분석', '데이터 전처리', '인사이트 도출']\n",
      "34. ['데이터 분석', '주도적', '데이터 시각화', '최신 기술 연구', '문제 정의 및 해결', '데이터 정제', '리드 능력', '프로젝트 경험', '데이터 모델링', '책임감', '데이터 전처리', '딥러닝', '머신러닝']\n",
      "35. ['데이터 분석', '고객 분석', '커뮤니케이션', '데이터 기반 의사결정', '분석적 사고', '논리적 사고', '통계적 지식', '정량 데이터', '도메인 지식', '지표 정의', '데이터 전처리']\n",
      "36. ['데이터베이스', '시스템 개발']\n",
      "37. ['데이터 분석', '고객 분석', '코호트 분석', '모바일 데이터', '리텐션 분석', '지표 정의', '가설 수립 및 검증', '프로덕트 데이터 분석']\n",
      "38. ['커뮤니케이션', '리포트 작성', '데이터 시각화', '정형 데이터', '문제 정의 및 해결', '데이터 추출', '비정형 데이터', '데이터 전처리', '인사이트 도출']\n",
      "39. ['적극적', '커뮤니케이션', '리포트 작성', '문제 정의 및 해결', '이미지 데이터', '서버 설계']\n",
      "40. ['데이터 분석', '커뮤니케이션', '리포트 작성', '데이터 기반 의사결정', '실무경험', '컨설팅', '통계분석', '모델 개발']\n",
      "41. ['성실함', '적응 능력', '커뮤니케이션']\n",
      "42. ['데이터 분석', '프로그래밍 언어', '문제 정의 및 해결', '데이터 시각화', '데이터 가공', '통계적 지식', '데이터 추출', '데이터 사이언스', '도메인 지식', '딥러닝', '머신러닝']\n",
      "43. ['데이터 분석', '주도적', '데이터 시각화', '최신 기술 연구', '문제 정의 및 해결', '데이터 정제', '리드 능력', '프로젝트 경험', '데이터 모델링', '책임감', '데이터 전처리', '딥러닝', '머신러닝']\n",
      "44. ['데이터 분석', '커뮤니케이션', '협업', '프로젝트 경험', '비정형 데이터', '모델 개발', '머신러닝']\n",
      "45. ['고객 분석', '클라우드 경험', '커뮤니케이션', '리포트 작성', '컨설팅', '영어 커뮤니케이션', '포트폴리오']\n",
      "46. ['데이터 분석', '협업', '리포팅 능력', '프로그래밍 언어', '대시보드 구축', '비즈니스 인텔리전스', '인사이트 도출']\n",
      "47. []\n",
      "48. ['데이터 분석', '데이터 수집', '리포팅 능력', '데이터 기반 의사결정', '실무경험', '데이터 시각화', '문제 정의 및 해결', '데이터 추출', '정량 데이터', '데이터 핸들링', '프로젝트 경험', '데이터 모델링', '가설 수립 및 검증', '빅데이터 분석', '인사이트 도출', '리딩 능력']\n",
      "49. ['데이터 분석', '커뮤니케이션', '협업', '실무경험', '데이터 시각화', '논리적 사고', '데이터 추출', '도메인 지식', '데이터 전처리', '인사이트 도출', '머신러닝']\n",
      "50. ['데이터 분석', '문제 정의 및 해결', '논리적 사고', '데이터 추출', '데이터 전처리', '인사이트 도출']\n",
      "51. ['데이터 분석', '리포트 작성', '데이터 처리', '대시보드 구축', '코드 해석', '리딩 능력']\n",
      "52. ['데이터 분석', '커뮤니케이션', '협업', '캠페인 분석', '웹 분석']\n",
      "53. ['영어 커뮤니케이션', '커뮤니케이션', '꼼꼼함']\n",
      "54. ['데이터 분석', '커뮤니케이션', '엑셀', '파이썬', '리포트 작성', '문제 정의 및 해결']\n",
      "55. []\n",
      "56. []\n",
      "57. ['데이터 분석', '열정적', '프로그래밍 언어', '최신 기술 연구', '도메인 지식', '빅데이터 분석', '수리적 지식']\n",
      "58. ['영어 커뮤니케이션', '책임감', '리포트 작성', '꼼꼼함']\n",
      "59. ['데이터 분석', '퍼널 분석', '코호트 분석', '데이터 시각화', '테스트 설계 분석', '데이터 가공', '모바일 데이터', '데이터 로그 설계', '데이터 추출', '데이터 정제', '리텐션 분석', '통계적 지식', '도메인 지식', '지표 정의', '가설 수립 및 검증', '프로덕트 데이터 분석']\n",
      "60. ['데이터 분석', '데이터 엔지니어링', '실무경험', '프로그래밍 언어', '비동기 데이터', '분산 처리 시스템']\n",
      "61. ['데이터 분석', '파이썬', '협업', '리포팅 능력', '구글스프레드시트', '실무경험', '문제 정의 및 해결', '데이터 가공', '데이터 추출', '대시보드 구축', '지표 정의', '비즈니스 인텔리전스', '인사이트 도출']\n",
      "62. ['데이터 분석', '인과추론', '퍼널 분석', '코호트 분석', '테스트 설계 분석', '데이터 로그 설계', '모바일 데이터', '통계분석', '리텐션 분석', '가설 수립 및 검증', '지표 정의', '프로덕트 데이터 분석', '리딩 능력']\n",
      "63. ['데이터 분석', '커뮤니케이션', '리포트 작성', '문제 정의 및 해결', '데이터 가공', '데이터 추출', '구조적 사고', '가설 수립 및 검증', '쿼리']\n",
      "64. ['성실함', '적응 능력', '커뮤니케이션']\n",
      "65. ['파워포인트', '데이터 분석', '엑셀']\n",
      "66. ['데이터 분석', '인과추론', '실무경험', '문제 정의 및 해결', '논리적 사고', '데이터 마트 구축 운영', '인사이트 도출']\n",
      "67. ['데이터 분석', '데이터 기반 의사결정', '논리적 사고', '데이터 추출', '오너십']\n",
      "68. ['데이터 분석', '커뮤니케이션', '리포트 작성', '데이터 기반 의사결정', '문제 정의 및 해결', '통계분석', '데이터 추출', '데이터 정제', '통계적 지식']\n",
      "69. ['블록체인', '대시보드 구축', '머신러닝', '협업']\n",
      "70. ['데이터 분석', '커뮤니케이션', '데이터 수집', '설득력', '협업', '데이터 시각화', '문제 정의 및 해결', '대시보드 구축', '데이터 추출', '데이터 모델링', '데이터 전처리', '인사이트 도출']\n",
      "71. ['데이터 분석', '커뮤니케이션', '고객 분석', '협업', '프로그래밍 언어', '알고리즘 개발', '데이터 모델링', '데이터 로그 설계']\n",
      "72. ['리포트 작성', '데이터 시각화', '데이터 가공', '통계분석', '데이터 추출', '알고리즘 개발', '도메인 지식', '논문 구현']\n",
      "73. ['데이터 분석', '취약점 분석', '커뮤니케이션', '협업', '리포트 작성', '악성코드 분석']\n",
      "74. ['데이터 분석', '인과추론', '주도적', '실무경험', '문제 정의 및 해결', '논리적 사고', '데이터 추출', '프로젝트 경험', '인사이트 도출']\n",
      "75. ['데이터 분석', '컨설팅', '코드 해석', '모델 개발', '연구개발']\n",
      "76. ['데이터 분석', '모델 개발', '프레임워크', '코드 해석']\n",
      "77. []\n",
      "78. ['데이터 분석', '데이터 시각화', '데이터 추출', '대시보드 구축', '테스트 설계 분석']\n",
      "79. ['데이터 분석', '고객 분석', '프레임워크', '생산관리', '스마트팩토리', '공정관리', '시스템 개발', '프로젝트 경험', '설비관리', '품질관리', '데이터 솔루션']\n",
      "80. ['프레임워크', '최신 기술 연구', '도메인 지식', '논문 구현', '컴퓨터 비전', '딥러닝']\n",
      "81. ['데이터 분석', '취약점 분석', '커뮤니케이션', '협업', '리포트 작성', '악성코드 분석']\n",
      "82. []\n",
      "83. ['프레임워크', '백엔드', '데이터베이스', '데이터 시각화', '데이터 모델링', '빅데이터 분석']\n",
      "84. ['클라우드 경험', '열정적', '데이터 파이프라인 구축', '프로그래밍 언어', '최신 기술 연구', '도전적', '플랫폼 구축', '어플리케이션 개발']\n",
      "85. ['데이터 엔지니어링', '커뮤니케이션', '협업', '웹크롤링', '코드 해석']\n",
      "86. []\n",
      "87. ['데이터 분석', '데이터 어플리케이션 서비스', '고객 분석', '커뮤니케이션', '협업', '데이터 기반 의사결정', '모바일 데이터', '프로젝트 경험', '플랫폼 구축']\n",
      "88. ['적극적', '리포트 작성', '적응 능력', '데이터 모델링', '데이터 솔루션']\n",
      "89. ['고가용성', '오픈소스 경험', '내고장성', '소프트웨어', '시스템 개발', '도메인 지식']\n",
      "90. ['튜닝 역량', '다이어그램', '데이터베이스', '스키마']\n",
      "91. ['끈기', '최신 기술 연구', '도메인 지식', '데이터베이스']\n",
      "92. ['클라우드 경험', '데이터 파이프라인 구축', '커뮤니케이션']\n",
      "93. []\n",
      "94. ['데이터 분석', '플랫폼 구축', '이슈 해결']\n",
      "95. []\n",
      "96. ['클라우드 경험', '문제 정의 및 해결', '도메인 지식', '데이터 모델링', '플랫폼 구축']\n",
      "97. ['적극적', '커뮤니케이션', '업무 자동화', '협업', '데이터 파이프라인 구축', '주도적', '도메인 지식', '프로젝트 경험', '모델 개발', '딥러닝', '포트폴리오', '개발경험']\n",
      "98. []\n",
      "99. ['데이터 엔지니어링', '파이썬', '웹크롤링', '프로젝트 경험', '컴퓨터공학']\n",
      "100. ['전산 지식', '클라우드 경험', '데이터 수집', '커뮤니케이션', '데이터베이스', '최신 기술 연구', '알고리즘 개발', '도메인 지식', '분산 처리 시스템']\n",
      "101. ['데이터 전처리', '데이터 수집']\n",
      "102. ['데이터 분석', '프로그래밍 언어', '기술통계', '플랫폼 구축', '거버넌스']\n",
      "103. ['데이터 분석', '산업공학', '통계적 지식', '데이터 추출', '데이터 마트 구축 운영', '데이터 핸들링', '컴퓨터공학', '프로젝트 경험', '경영정보학', '모델 개발', '데이터 전처리', '경영학']\n",
      "104. ['데이터 분석', '산업공학', '통계적 지식', '데이터 추출', '데이터 마트 구축 운영', '데이터 핸들링', '컴퓨터공학', '프로젝트 경험', '경영정보학', '모델 개발', '데이터 전처리', '경영학']\n",
      "105. ['데이터 파이프라인 구축', '데이터베이스', '프로그래밍 언어', '쿼리', '데이터 스트리밍']\n",
      "106. ['백업 모니터링', '최적화', '장애처리']\n",
      "107. ['전산 지식', '데이터베이스', '인프라 구축 운영', '높은 코드 퀄리티', '알고리즘 개발', '코드 해석', '아키텍쳐', '도메인 지식', '플랫폼 구축']\n",
      "108. ['전산 지식', '도메인 지식', '인프라 구축 운영']\n",
      "109. ['데이터 분석', '취약점 분석', '스크립트 언어', '적극적', '커뮤니케이션', '시스템 개발', '정보보호', '리드 능력', '도메인 지식', '보안']\n",
      "110. ['데이터 수집', '데이터 파이프라인 구축', '실시간 데이터 처리', '배치 프로세싱', '프로그래밍 언어', '데이터 전처리']\n",
      "111. ['리눅스']\n",
      "112. ['데이터 파이프라인 구축', '산업공학', '금융공학', '분석적 사고', '정량 데이터', '물리학', '도메인 지식', '컴퓨터공학', '빅데이터 분석']\n",
      "113. ['적극적', '커뮤니케이션', '장애처리', '리포팅 능력', '데이터베이스']\n",
      "114. ['데이터 분석', '업무 자동화', '데이터 파이프라인 구축', '문제 정의 및 해결', '데이터 마트 구축 운영']\n",
      "115. ['데이터 분석', '클라우드 경험', '컨테이너 경험', '데이터 시각화', '시스템 개발', '가설 수립 및 검증']\n",
      "116. ['클라우드 경험', '데이터 파이프라인 구축', '빅데이터', '데이터 모델링', '튜닝 역량']\n",
      "117. ['데이터 분석', '백엔드', '주도적', '데이터 시각화', '트러블슈팅', '데이터 마트 구축 운영', '서버 설계', '모델 개발']\n",
      "118. ['어플리케이션 개발', '분산 처리 시스템', '실시간 데이터 처리', '시스템 개발']\n",
      "119. ['전산 지식', '데이터베이스', '인프라 구축 운영', '높은 코드 퀄리티', '알고리즘 개발', '코드 해석', '아키텍쳐', '도메인 지식', '플랫폼 구축']\n",
      "120. ['데이터 분석', '데이터 엔지니어링', '백엔드', '플랫폼 구축', '어플리케이션 개발', '이슈 해결']\n",
      "121. ['시스템 개발', '데이터 엔지니어링', '소프트웨어', '테스트 설계 분석']\n",
      "122. ['웹크롤링', '도메인 지식', '지표 정의']\n",
      "123. ['데이터 전처리', '데이터 수집']\n",
      "124. ['데이터 분석', '프로그래밍 언어', '기술통계', '플랫폼 구축', '거버넌스']\n",
      "125. ['클라우드 경험', '데이터 엔지니어링', '협업', '데이터 파이프라인 구축', '컨테이너 경험', '소프트웨어', '문제 정의 및 해결', '데이터 오케스트레이션', '데이터 솔루션']\n",
      "126. ['데이터 분석', '클라우드 경험', '업무 자동화', '데이터 파이프라인 구축', '문제 정의 및 해결', '데이터 마트 구축 운영']\n",
      "127. ['클라우드 경험', '백엔드', '프로그래밍 언어', '워크플로우', '시스템 개발', '플랫폼 구축']\n",
      "128. []\n",
      "129. ['튜닝 역량', '다이어그램', '데이터베이스', '스키마']\n",
      "130. []\n",
      "131. ['네트워크 구축', '텍스트 데이터', '웹서비스', '비동기처리']\n",
      "132. ['웹서비스']\n",
      "133. ['도메인 지식', '소프트웨어', '가설 수립 및 검증']\n",
      "134. ['긍정적', '커뮤니케이션']\n",
      "135. ['데이터 분석', '클라우드 경험', '데이터 가공', '데이터 추출', '플랫폼 구축', '데이터 솔루션']\n",
      "136. ['업무 자동화', '데이터 수집', '데이터 시각화', '시스템 개발', '하둡']\n",
      "137. ['데이터 분석', '문제 정의 및 해결', '시스템 개발', '프로젝트 경험', '플랫폼 구축', '빅데이터 분석', '창의성', '데이터 솔루션']\n",
      "138. ['하이퍼 캐주얼 게임']\n",
      "139. ['최적화', '데이터 프로세싱', '분산 처리 시스템', '백엔드']\n",
      "140. ['클라우드 경험', '열정적', '데이터 파이프라인 구축', '프로그래밍 언어', '최신 기술 연구', '도전적', '플랫폼 구축', '어플리케이션 개발']\n",
      "141. ['데이터 분석', '데이터 파이프라인 구축', '주도적', '인프라 구축 운영', '플랫폼 구축', '프로덕트 데이터 분석']\n",
      "142. ['커뮤니케이션', '협업', '최적화', '서버 설계', '데이터 모델링']\n",
      "143. ['데이터 분석', '머신러닝', '커뮤니케이션', '데이터 수집', '설득력', '협업', '데이터 시각화', '문제 정의 및 해결', '데이터 가공', '대시보드 구축', '데이터 추출', '데이터 모델링', '모델 개발', '데이터 전처리', '수리적 지식']\n",
      "144. ['데이터 분석', '클라우드 경험', '커뮤니케이션', '프로토타이핑', '도메인 지식', '프로젝트 경험', '호기심', '플랫폼 구축', '데이터 사이언티스트', '데이터 솔루션']\n",
      "145. ['데이터 분석', '클라우드 경험', '커뮤니케이션', '백엔드', '데이터베이스', '쿼리']\n",
      "146. ['데이터 분석', '클라우드 경험', '커뮤니케이션', '프로토타이핑', '도메인 지식', '프로젝트 경험', '호기심', '플랫폼 구축', '데이터 사이언티스트', '데이터 솔루션']\n",
      "147. ['커뮤니케이션', '소프트웨어', '프로젝트 경험', '프레젠테이션 기술']\n",
      "148. ['프로그래밍 언어', '데이터 파이프라인 구축']\n",
      "149. ['데이터 분석', '주도적', '프로그래밍 언어', '프로젝트 경험', '인사이트 도출', '모델 개발 및 배포', '머신러닝']\n",
      "150. ['프레임워크', '서버 설계']\n",
      "151. ['프레임워크', '스크립트 언어', '파이썬', '데이터베이스', '프로그래밍 언어', '도메인 지식']\n",
      "152. ['긍정적', '커뮤니케이션', '이미지 데이터', '성실함', '영상 처리', '책임감', '데이터 레이블링']\n",
      "153. ['스트림 프로세싱', '데이터 프로세싱', '데이터 파이프라인 구축']\n",
      "154. ['도전적', '리포트 작성']\n",
      "155. ['최적화', '데이터 파이프라인 구축']\n",
      "156. ['데이터 엔지니어링', '온체인 데이터', '데이터 파이프라인 구축', '실무경험', '시스템 개발', '분산 처리 시스템']\n",
      "157. ['프로그래밍 언어', '인프라 구축 운영', '빅데이터 분석', '플랫폼 구축']\n",
      "158. ['데이터 분석']\n",
      "159. ['산업공학', '통계적 지식']\n",
      "160. ['빅데이터 분석', '책임감']\n",
      "161. ['모델 개발 및 배포', '포트폴리오']\n",
      "162. ['데이터 분석', '산업공학', '통계적 지식', '데이터 추출', '데이터 마트 구축 운영', '데이터 핸들링', '컴퓨터공학', '프로젝트 경험', '경영정보학', '모델 개발', '데이터 전처리', '경영학']\n",
      "163. ['데이터 분석', '산업공학', '통계적 지식', '데이터 추출', '데이터 마트 구축 운영', '데이터 핸들링', '컴퓨터공학', '프로젝트 경험', '경영정보학', '모델 개발', '데이터 전처리', '경영학']\n",
      "164. ['데이터 엔지니어링', '파이썬', '웹크롤링', '프로젝트 경험', '컴퓨터공학']\n",
      "165. ['엑셀']\n",
      "166. ['리눅스']\n",
      "167. ['클라우드 경험', '데이터 엔지니어링', '플랫폼 구축']\n",
      "168. ['배치 프로세싱', '데이터 레이크하우스', '클라우드 경험', '실시간 데이터 처리']\n",
      "169. ['끈기', '최신 기술 연구', '도메인 지식', '데이터베이스']\n",
      "170. ['데이터 엔지니어링', '커뮤니케이션', '협업', '웹크롤링', '코드 해석']\n",
      "171. ['커뮤니케이션', '클라우드 경험', '데이터베이스', '오픈소스 경험', '플랫폼 구축']\n",
      "172. ['데이터 파이프라인 구축', '데이터베이스', '프로그래밍 언어', '쿼리', '데이터 스트리밍']\n",
      "173. ['데이터 엔지니어링', '데이터 수집', '협업', '데이터 파이프라인 구축', '문제 정의 및 해결']\n",
      "174. ['프레임워크', '커뮤니케이션', '프로그래밍 언어', '비판', '코드 해석', '플랫폼 구축']\n",
      "175. ['파이썬', '정규화', '트랜잭션', '쿼리', '데이터 전처리']\n",
      "176. ['프레임워크', '도메인 지식', '데이터베이스', '통계분석']\n",
      "177. ['데이터 엔지니어링', '데이터 파이프라인 구축', '프로그래밍 언어', '문제 정의 및 해결', '플랫폼 구축', '빅데이터 분석']\n",
      "178. ['데이터 분석', '클라우드 경험', '통계적 지식', '시스템 개발', '도메인 지식', '분산 처리 시스템']\n",
      "179. ['분산 처리 시스템', '시스템 개발']\n",
      "180. ['시스템 개발', '보안']\n"
     ]
    }
   ],
   "source": [
    "col = 'Requirement'\n",
    "(col_list2, col_list2_df, \n",
    " eng_list, kor_list, \n",
    " eng_list_spl, eng_lower, \n",
    " custom_nouns, replace, stopwords, \n",
    " results, kor_final, kor_final_df) = Korean_preprocessing(col)\n",
    "\n",
    "df.insert(4, 'Requirement_KOR', kor_final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Requirement_ENG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python'], ['sql', 'hql', 'python', 'r'], ['sql', 'hql', 'python', 'r', 'complex', 'network', 'modeling'], ['machine', 'learnig', 'python', 'java', 'sql', 'r'], [], ['computer', 'vision', 'image', 'video', 'processing', 'tensorflow', 'pytorch', 'or'], ['ml', 'dl', 'python', 'sql', 'ml', 'dl'], ['data', 'science', 'ml', 'dl', 'nlp', 'clustering', 'ml', 'dl', 'sql', 'python'], ['python', 'sql', 'r'], ['data', 'science', 'python', 'r', 'sql', 'tool', 'splunk', 'tableau', 'tool'], ['ai'], ['python', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation', 'r'], ['python', 'sql', 'spark', 'hadoop'], ['ai'], [], ['python', 'r', 'ml', 'dl'], [], ['master', 's', 'degree', 'in', 'a', 'quantitative', 'discipline', 'e', 'g', 'statistics', 'operations', 'research', 'bioinformatics', 'economics', 'computational', 'biology', 'computer', 'science', 'mathematics', 'physics', 'electrical', 'engineering', 'industrial', 'engineering', 'or', 'equivalent', 'research', 'experience', 'years', 'of', 'work', 'experience', 'in', 'data', 'science', 'related', 'fields', 'distinctive', 'problem', 'solving', 'skills', 'good', 'at', 'articulating', 'product', 'questions', 'pulling', 'data', 'from', 'large', 'datasets', 'and', 'using', 'statistics', 'to', 'arrive', 'at', 'a', 'recommendation', 'excellent', 'verbal', 'and', 'written', 'communication', 'skills', 'with', 'ability', 'to', 'present', 'information', 'and', 'analysis', 'results', 'effectively', 'ability', 'to', 'build', 'positive', 'relationships', 'within', 'dsa', 'and', 'with', 'our', 'stakeholders', 'and', 'work', 'effectively', 'with', 'cross', 'functional', 'partners', 'in', 'a', 'global', 'company', 'statistics', 'must', 'have', 'strong', 'knowledge', 'and', 'experience', 'in', 'experimental', 'design', 'hypothesis', 'testing', 'and', 'various', 'statistical', 'analysis', 'techniques', 'such', 'as', 'regression', 'or', 'linear', 'models', 'machine', 'learning', 'must', 'have', 'deep', 'understanding', 'of', 'ml', 'algorithms', 'i', 'e', 'deep', 'learning', 'random', 'forest', 'gradient', 'boosted', 'trees', 'k', 'means', 'clustering', 'etc', 'and', 'its', 'development', 'validation', 'and', 'evaluation', 'programming', 'experience', 'with', 'python', 'r', 'or', 'other', 'scripting', 'language', 'and', 'database', 'language', 'e', 'g', 'sql', 'or', 'data', 'manipulation', 'e', 'g', 'pandas', 'dplyr', 'are', 'required'], ['python'], ['python', 'r', 'ml', 'dl'], ['bi', 'r'], ['sql', 'r', 'python'], [], ['a', 'b'], ['data', 'analyst', 'data', 'scientist', 'rdb', 'sql', 'python', 'ml'], ['python', 'sql', 'tensorflow', 'pytorch', 'raw', 'data', 'sql'], ['ml', 'keras', 'tensorflow', 'pytorch'], ['ml', 'python', 'sql', 'tensorflow', 'keras', 'pytorch'], ['sql', 'python', 'pm'], ['python', 'opencv', 'numpy', 'tensorflow', 'pytorch', 'cnn', 'transformer', 'python', 'opencv', 'numpy', 'pytorch', 'or', 'tensorflow'], ['remote', 'sensing', 'signal', 'processing', 'earth', 'science', 'sar', 'python', 'matlab'], ['python', 'opencv', 'numpy', 'tensorflow', 'pytorch', 'cnn', 'transformer', 'python', 'opencv', 'numpy', 'pytorch', 'or', 'tensorflow'], [], ['sql'], ['python', 'prediction', 'anomaly', 'detection', 'classification', 'tensorflow', 'pytorch', 'pandas', 'matplotlib', 'tensorflow', 'pytorch'], ['sql', 'python', 'saas', 'gmv', 'mrr', 'ltv', 'cohort', 'retention', 'action', 'item'], ['front', 'end', 'redwoodjs', 'storybook', 'graphql', 'tailwindcss', 'app', 'flutter', 'fastlane', 'back', 'end', 'redwoodjs', 'serverless', 'aws', 'lambda', 'postgresql', 'amazon', 'rds', 'python', 'fastapi'], [], ['sql', 'numpy', 'pandas', 'spark', 'powerbi', 'tableau'], [], ['css', 'python', 'sas'], ['sql', 'scripting', 'data', 'handling', 'ms', 'office'], ['python', 'sql', 'framework', 'pandas', 'scikit', 'learn', 'tensorflow', 'pytorch', 'python', 'tableau', 'ml', 'process', 'data', 'collection', 'eda', 'preprocessing', 'feature', 'engineering', 'training', 'evaluation', 'serving'], ['python', 'prediction', 'anomaly', 'detection', 'classification', 'tensorflow', 'pytorch', 'pandas', 'matplotlib', 'tensorflow', 'pytorch'], ['gcp', 'professional', 'machine', 'learning', 'engineer', 'gcp', 'ml', 'ai', 'text', 'image', 'sound'], ['isp', 'ismp', 'pi', 'dx', 'migration', 'ppt'], ['sql', 'r', 'python'], ['essential', 'experiences', 'skillsets', 'personal', 'competencies', 'undertaken', 'years', 'experience', 'in', 'a', 'similar', 'role', 'or', 'data', 'analysis', 'role', 'understanding', 'on', 'the', 'multi', 'cultural', 'multi', 'lingual', 'and', 'diverse', 'environment', 'advanced', 'ms', 'excel', 'and', 'sql', 'skills', 'strong', 'analytical', 'skills', 'a', 'keen', 'eye', 'for', 'details', 'and', 'has', 'a', 'systematic', 'approach', 'in', 'dealing', 'with', 'issues', 'desirable', 'personal', 'competencies', 'and', 'attributes', 'business', 'communication', 'skills', 'in', 'both', 'english', 'and', 'korean', 'ability', 'to', 'be', 'hands', 'on', 'particularly', 'with', 'data', 'collation', 'and', 'preparation'], ['biz', 'insight', 'crm', 'sql', 'mysql', 'mssql', 'python', 'r', 'sql', 'pandas', 'tensorflow', 'pytorch', 'spark', 'mysql', 'mariadb', 'nosql', 'mongodb', 'pm', 'pl'], ['sql', 'raw', 'data', 'tableau', 'amplitude'], ['sql', 'python', 'r'], ['sql', 'tableau', 'google', 'data', 'studio', 'google', 'sheets', 'ms', 'excel', 'python', 'r'], ['reporting', 'ppt', 'excel', 'qa', 'digital', 'marketing', 'or', 'or', 'sales'], ['aa', 'or', 'ga'], ['sql', 'tableau', 'ga'], [], ['bi', 'r'], ['biomedical', 'python', 'r'], ['excel'], ['sql', 'excel', 'python', 'r', 'a', 'b', 'ga', 'amplitude'], ['batch', 'aws', 'architecture'], ['sql', 'excel'], ['sql', 'excel', 'python', 'r', 'a', 'b', 'ga', 'amplitude'], ['da', 'bi', 'sql', 'python', 'cohort', 'funnel', 'a', 'b', 'test', 'ppt', 'documentation'], ['crc', 'data', 'handling', 'ms', 'office'], ['ms', 'office'], ['excel', 'sql', 'b'], ['sql', 'data', 'driven', 'business', 'solution'], ['sas', 'r', 'python', 'sql'], ['layer', 'or', 'layer', 'block', 'transaction', 'business', 'intelligence', 'sql', 'https', 'towardsdatascience', 'com', 'your', 'guide', 'to', 'basic', 'sql', 'while', 'learning', 'ethereum', 'at', 'the', 'same', 'time', 'eac', 'a', 'etherscan'], ['sql', 'nosql', 'python', 'r'], ['python', 'ml', 'dl', 'legacy', 'sql'], ['t', 'o', 'sql', 'python', 'r', 'tableau', 'powerbi', 'bi', 'ml'], [], ['sql', 'excel'], ['ai', 'mlops', 'ai', 'ml', 'big', 'data', 'ai', 'ml', 'ai', 'ml', 'structure'], ['ai', 'ml', 'big', 'data', 'python', 'ml', 'pytorch', 'tensorflow', 'ai', 'ai', 'structure'], ['c', 'c'], ['sql', 'bi', 'google', 'data', 'studio', 'quicksight', 'metabase', 'a', 'b'], ['pl', 'pl', 'it'], ['tensorflow', 'pytorch', 'mxnet'], [], ['bachelor', 's', 'degree', 'required', 'majored', 'in', 'economics', 'statistics', 'risk', 'management', 'preferred', 'strong', 'business', 'acumen', 'and', 'communications', 'skills', 'needed', 'to', 'interface', 'with', 'both', 'executive', 'management', 'and', 'our', 'clients', 'including', 'internal', 'business', 'partnering', 'should', 'over', 'years', 'experiences', 'in', 'work', 'years', 'of', 'experience', 'within', 'leading', 'the', 'projective', 'work', 'data', 'analytics', 'quality', 'management', 'risk', 'management', 'general', 'insurance', 'risk', 'management', 'telecommunication', 'preferred', 'advanced', 'knowledge', 'of', 'microsoft', 'office', 'suite', 'applications', 'including', 'excel', 'word', 'and', 'powerpoint', 'power', 'bi', 'and', 'other', 'reporting', 'tools', 'a', 'plus', 'experience', 'working', 'in', 'a', 'multi', 'national', 'multi', 'cultural', 'multi', 'lingual', 'and', 'diverse', 'environment', 'experience', 'applying', 'problem', 'solving', 'skills', 'to', 'complex', 'systems', 'financial', 'operational', 'and', 'associated', 'integrated', 'processes', 'strong', 'time', 'management', 'and', 'organization', 'skills'], ['go', 'python', 'ml', 'deep', 'learning', 'framework', 'scikit', 'learn', 'tensorflow', 'keras', 'sql', 'hadoop', 'mr', 'hive', 'spark', 'flink', 'presto', 'tableau', 'bi', 'tool'], ['python', 'etl'], ['python', 'node', 'js'], ['postgresql', 'oracle', 'dbms', 'cloud'], ['pm'], ['db', 'erwin', 'db', 'db', 'erwin'], ['linux', 'unix', 'database', 'docker', 'container', 'github', 'com', 'id'], ['postgresql', 'oracle', 'mysql', 'mariadb', 'db', 'sql', 'stored', 'procedure', 'er'], ['python', 'airflow'], ['python', 'aws', 'gcp', 'azure'], [], ['aws', 'db', 'sql', 'aws', 's', 'airflow', 'glue', 'emr', 'kinesis', 'redshift', 'mysql', 'elasticsearch', 'mongodb', 'spark', 'kafka', 'python', 'java', 'shell', 'script'], ['dbms', 'dba', 'dbms', 'postgresql', 'oracle', 'tibero', 'altibase', 'edb', 'experdb', 'mysql', 'db'], ['aws', 'gcp', 'etl', 'elt'], ['computer', 'vision', 'image', 'video', 'processing', 'airflow', 'kubeflow', 'kubernetes', 'docker', 'container', 'or'], [], [], ['os', 'python', 'hadoop', 'mr', 'hive', 'spark', 'db', 'third', 'party', 'log', 'streaming', 'sql', 'aws'], ['sql', 'python'], ['sql', 'python', 'python', 'scala', 'sql', 'apache', 'spark', 'delta', 'lake', 'airflow', 'kafka', 'fastapi', 'git', 'argocd', 'grafana'], ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation'], ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation'], ['dw', 'data', 'lake', 'elt'], ['oracle', 'database', 'db', 'schema', 'db', 'oracle', 'db', 'rac', 'db', 'migration'], ['aws', 'gcp', 'cloud', 'native', 'airflow', 'workflow', 'orchestrator', 'prefect', 'oozie', 'workflow', 'cs', 'os'], ['java', 'scala', 'python', 'hadoop', 'ecosystem'], ['ai', 'ml', 'data', 'science', 'shell', 'python'], [], [], ['a', 'k', 's'], [], ['etl', 'sql', 'python', 'airflow', 'workflow', 'tool'], ['aws', 'airflow'], ['dw', 'dm', 'sql', 'java', 'back', 'end', 'java', 'spring', 'python'], ['java', 'kotlin', 'scala', 'python', 'sql', 'python', 'airflow'], ['kotlin', 'java', 'python', 'go', 'scala', 'hadoop', 'kafka', 'spark', 'msa'], ['aws', 'gcp', 'cloud', 'native', 'airflow', 'workflow', 'orchestrator', 'prefect', 'oozie', 'workflow', 'cs', 'os'], ['or', 'dba', 'python', 'java', 'aws', 'db', 'sql'], ['jira', 'qa'], ['python', 'sql', 'etl', 'aws', 'git'], ['sql', 'python'], ['sql', 'python'], ['ml', 'airflow', 'prefect', 'dagster', 'etl', 'elt', 'aws', 'azure', 'gcp', 'snowflake', 'databricks', 'python', 'sql', 'git', 'github', 'docker', 'docker', 'compose'], ['etl', 'sql', 'python', 'airflow', 'workflow', 'tool'], ['python', 'scala', 'java', 'go', 'airflow'], [], ['postgresql', 'oracle', 'mysql', 'mariadb', 'db', 'sql', 'stored', 'procedure', 'er'], ['c', 'c', 'network'], [], ['rds', 'nodejs'], ['ui'], [], ['sql', 'raw', 'data', 'aws', 'etl'], ['python', 'java', 'scala', 'javascript', 'spark', 'hadoop', 'hive', 'sql'], ['spark', 'airflow', 'jenkins', 'sql', 'python'], ['python', 'bigquery', 'mysql', 'dba'], ['ci', 'cd', 'python', 'pandas', 'sql'], ['python', 'etl'], ['o', 'o'], ['python', 'node', 'js', 'sql', 'linux'], ['sql', 'nosql', 'python', 'r', 'ai', 'ai', 'ai', 'ai', 'd', 'hw'], ['etl', 'data', 'pipeline', 'python', 'sql'], ['serverless', 'sql'], ['etl', 'data', 'pipeline', 'data', 'lake', 'python', 'sql'], ['ci', 'cd'], ['spark', 'hive', 'presto', 'elastic', 'stack', 'python', 'scala', 'java', 'sql'], ['sql', 'python', 'kaggle', 'api'], ['e', 'g', 'flask', 'django', 'cloud', 'hosting', 'service', 'e', 'g', 'aws'], ['rdb', 'or', 'nosql', 'python', 'e', 'g', 'flask', 'django'], ['python', 'format'], ['kafka', 'spark', 'hadoop', 'eco', 'system', 'python', 'sql'], ['linux', 'language', 'framework', 'platform', 'etc'], ['data', 'flow', 'architecture', 'etl', 'sql', 'query', 'index', 'stored', 'procedure', 'in', 'sql', 'server', 'pm'], [], ['python'], ['python', 'sql'], [], ['java', 'python', 'hadoop', 'eco', 'system'], ['back', 'front', 'db', 'deploy', 'github', 'bitbucket', 'repository'], ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation'], ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation'], [], [], [], ['dw', 'bi', 'jvm', 'java', 'scala', 'python', 'gcp', 'aws'], [], ['python', 'airflow'], ['python', 'node', 'js'], ['rdb', 'mysql', 'nosql', 'mongodb', 'kafka', 'python', 'java', 'scala', 'datawarehouse', 'datalake'], ['dw', 'data', 'lake', 'elt'], ['python', 'java', 'scala', 'sql'], [], ['db', 'db', 'index', 'nosql', 'jupyter', 'notebook', 'pandas', 'numpy', 'scipy'], ['sql', 'data', 'pipeline', 'airflow', 'workflow', 'tool', 'django', 'fastapi', 'python'], ['kafka', 'spark', 'hadoop', 'python', 'java', 'scala'], ['hadoop', 'spark', 'aws', 'gcp', 'azure', 'sql', 'python', 'jvm'], ['java', 'python', 'back', 'end', 'sql', 'hdfs', 'nosql'], ['web', 'was', 'dbms', 'container', 'firewall', 'waf', 'vpn', 'utm']]\n"
     ]
    }
   ],
   "source": [
    "print(eng_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m elem \u001b[39m=\u001b[39m eng_lower[idx]\n\u001b[0;32m     25\u001b[0m \u001b[39m# print(postprocessor.pos(elem))\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m post_onechar \u001b[39m=\u001b[39m postprocessor\u001b[39m.\u001b[39;49mpos(elem)\n\u001b[0;32m     27\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m post_onechar:\n",
      "File \u001b[1;32mc:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\ckonlpy\\tag\\_postprocess.py:77\u001b[0m, in \u001b[0;36mPostprocessor.pos\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[39mreturn\u001b[39;00m w\n\u001b[0;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m (w_, w[\u001b[39m1\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(w_, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m w_\n\u001b[1;32m---> 77\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_tagger\u001b[39m.\u001b[39;49mpos(phrase)\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngrams:\n\u001b[0;32m     80\u001b[0m     words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_as_ngram(words)\n",
      "File \u001b[1;32mc:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\ckonlpy\\tag\\_abstract.py:16\u001b[0m, in \u001b[0;36mAbstractTagger.pos\u001b[1;34m(self, phrase, norm, stem, perfect_match)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_None\u001b[39m(wordpos_list):\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m([\u001b[39mTrue\u001b[39;00m \u001b[39mfor\u001b[39;00m _, pos, _, _ \u001b[39min\u001b[39;00m wordpos_list \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 16\u001b[0m eojeols \u001b[39m=\u001b[39m phrase\u001b[39m.\u001b[39;49msplit()\n\u001b[0;32m     17\u001b[0m tagged \u001b[39m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m eojeol \u001b[39min\u001b[39;00m eojeols:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "text_path = './custom_nouns_stacktool.txt'\n",
    "custom_nouns = text_list_load(text_path)\n",
    "\n",
    "text_path = './stopwords_stacktool_eng.txt'\n",
    "stopwords = text_list_load(text_path)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "custom_nouns = custom_nouns\n",
    "for noun in custom_nouns:\n",
    "    okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger=okt,\n",
    "    passtags={'Noun'},\n",
    "    # replace=replace,\n",
    "    stopwords=stopwords,\n",
    "    # passwords=passwords,\n",
    "    # ngrams=ngrams\n",
    ")\n",
    "\n",
    "results = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    elem = eng_lower[idx]\n",
    "    # print(postprocessor.pos(elem))\n",
    "    post_onechar = postprocessor.pos(elem)\n",
    "    result = []\n",
    "    for word, tag in post_onechar:\n",
    "        result.append(word)\n",
    "    results.append(result)\n",
    "    # print(f'{idx}. {result}')\n",
    "\n",
    "eng_final = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    a = set(results[idx])\n",
    "    b = list(a)\n",
    "    eng_final.append(b)\n",
    "    print(f'{idx}. {b}')\n",
    "# print(len(kor_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preference_KOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Preference'\n",
    "(col_list2, col_list2_df, \n",
    " eng_list, kor_list, \n",
    " eng_list_spl, eng_lower, \n",
    " custom_nouns, replace, stopwords, \n",
    " results, kor_final, kor_final_df) = Korean_preprocessing(col)\n",
    "\n",
    "df.insert(6, 'Preference_KOR', kor_final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preference_ENG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "def wordcloud_show(list):\n",
    "    join_test1 = ', '.join(list)\n",
    "    test1_list = [join_test1]\n",
    "    # print(test1_list)\n",
    "\n",
    "    font_path = '/Users/LG/AppData/Local/Microsoft/Windows/Fonts/BMJUA_ttf.ttf'\n",
    "    test1_wc = ', '.join(test1_list)\n",
    "    test1_wc = test1_wc.replace(' ', '').replace(',', ', ')\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=2500, \n",
    "        height=2500, \n",
    "        background_color='white',\n",
    "        # colormap='seismic',\n",
    "        font_path=font_path\n",
    "    ).generate(test1_wc)\n",
    "    plt.figure(figsize=(6, 6), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "# DS\n",
    "# Requirement\n",
    "print('DS: Requirement')\n",
    "test1 = df.loc[df.label == '데이터 사이언티스트'].iloc[:, 4].tolist()\n",
    "wordcloud_show(test1)\n",
    "# Preference\n",
    "print('DS: Preference')\n",
    "test1 = df.loc[df.label == '데이터 사이언티스트'].iloc[:, 6].tolist()\n",
    "wordcloud_show(test1)\n",
    "\n",
    "# DA\n",
    "# Requirement\n",
    "print('DA: Requirement')\n",
    "test1 = df.loc[df.label == '데이터 애널리스트'].iloc[:, 4].tolist()\n",
    "wordcloud_show(test1)\n",
    "# Preference\n",
    "print('DA: Preference')\n",
    "test1 = df.loc[df.label == '데이터 애널리스트'].iloc[:, 6].tolist()\n",
    "wordcloud_show(test1)\n",
    "\n",
    "# DE\n",
    "# Requirement\n",
    "print('DE: Requirement')\n",
    "test1 = df.loc[df.label == '데이터 엔지니어'].iloc[:, 4].tolist()\n",
    "wordcloud_show(test1)\n",
    "# Preference\n",
    "print('DE: Preference')\n",
    "test1 = df.loc[df.label == '데이터 엔지니어'].iloc[:, 6].tolist()\n",
    "wordcloud_show(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **StackTool**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회의 결과, `StackTool`에서의 한글은 `Requirement_KOR`, `Preference_KOR`에도 있는 내용이어서 제거하기로 함\n",
    "- `StackTool`은 한글을 지운 후에 남는 영어로 정리하기\n",
    "- `Requirement`와 `Preference`에 있는 영어엔, `StackTool`의 영어를 불용어 사전으로 적용하여 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "stack_list = df.StackTool.tolist()\n",
    "\n",
    "# 개행문자와 쉼표 제거 후, 쉼표로 join\n",
    "stack_list2 = []\n",
    "for data in stack_list:\n",
    "    new_str = data.replace('\\n', ' ').replace(',', ' ')\n",
    "    new_str2 = new_str.split()\n",
    "    new_str3 = ', '.join(new_str2)\n",
    "    stack_list2.append(new_str3)\n",
    "print(stack_list2)\n",
    "\n",
    "# 하나의 리스트로 합치기\n",
    "stack_merge = []\n",
    "for data in stack_list2:\n",
    "    spl = data.split(', ')\n",
    "    stack_merge.extend(spl)\n",
    "print(stack_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kor, Eng 분리\n",
    "kor_list = []\n",
    "eng_list = []\n",
    "for data in stack_merge:\n",
    "    kor_match = re.findall(r'[가-힣]+', data)\n",
    "    eng_match = re.findall(r'[a-zA-Z0-9.!@#$%^&*()+]+', data)\n",
    "    kor_list.extend(kor_match)\n",
    "    eng_list.extend(eng_match)\n",
    "print(kor_list)\n",
    "print(eng_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# 영어 소문자로 통일\n",
    "eng_lower = []\n",
    "for eng in eng_list:\n",
    "    eng_low = eng.lower()\n",
    "    eng_lower.append(eng_low)\n",
    "\n",
    "# 중복 요소 제거\n",
    "eng_list_remove = list(set(eng_lower))\n",
    "print(eng_list_remove)\n",
    "kor_list_remove = list(set(kor_list))\n",
    "print(kor_list_remove)\n",
    "\n",
    "# 텍스트 저장\n",
    "text_path = './stopwords_stacktool_eng.txt'\n",
    "with open(text_path, 'w', encoding='utf-8-sig') as file:\n",
    "    for data in eng_list_remove:\n",
    "        file.write(data + '\\n')\n",
    "text_path = './stopwords_stacktool_kor.txt'\n",
    "with open(text_path, 'w', encoding='utf-8-sig') as file:\n",
    "    for data in kor_list_remove:\n",
    "        file.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack_list2 소문자로 통일\n",
    "stack_list2_lower = []\n",
    "for data in stack_list2:\n",
    "    data_low = data.lower()\n",
    "    stack_list2_lower.append(data_low)\n",
    "print(stack_list2_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 파일 불러오기\n",
    "text_path = './stopwords_stacktool_kor.txt'\n",
    "stopwords = text_list_load(text_path)\n",
    "# print(stopwords)\n",
    "text_path = './custom_nouns_stacktool.txt'\n",
    "custom_nouns = text_list_load(text_path)\n",
    "# print(custom_nouns)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "custom_nouns = custom_nouns\n",
    "for noun in custom_nouns:\n",
    "    okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger=okt,\n",
    "    passtags={'Noun'},\n",
    "    # replace=replace,\n",
    "    stopwords=stopwords,\n",
    "    # passwords=passwords,\n",
    "    # ngrams=ngrams\n",
    ")\n",
    "\n",
    "results = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    elem = stack_list2_lower[idx]\n",
    "    # print(postprocessor.pos(elem))\n",
    "    post_onechar = postprocessor.pos(elem)\n",
    "    result = []\n",
    "    for word, tag in post_onechar:\n",
    "        result.append(word)\n",
    "    results.append(result)\n",
    "    print(f'{idx}. {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 생성\n",
    "eng_final_df = pd.DataFrame({'StackTool_ENG': results})\n",
    "eng_final_df['StackTool_ENG'] = eng_final_df['StackTool_ENG'].apply(lambda x: ', '.join(x))\n",
    "eng_final_df.to_csv(csv_path + 'StackTool_ENG.csv', index=False)\n",
    "\n",
    "# insert\n",
    "df.insert(8, 'StackTool_ENG', eng_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = df[df.StackTool_ENG == ''].index\n",
    "# for i in idx:\n",
    "#     print(f'{i}. {df.StackTool[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requirement 쪽으로 가기\n",
    "    - Requirement, Preference 영어 전처리 해야 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
