{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from ckonlpy.tag import Twitter as Okt\n",
    "from ckonlpy.tag import Postprocessor\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "code_path = '/VSC/JJaemni/Code/'\n",
    "csv_path = '/VSC/JJaemni/CSV/'\n",
    "\n",
    "df = pd.read_csv(csv_path + 'wanted_jobplanet_2.csv')\n",
    "\n",
    "logging.getLogger('konlpy').setLevel(logging.ERROR)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 가능한 font list 확인\n",
    "# import matplotlib.font_manager as fm\n",
    "# f = [f.name for f in fm.fontManager.ttflist]\n",
    "# print(f)\n",
    "\n",
    "# # 확인 이후\n",
    "# plt.rc('font', family='Malgun Gothic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **불용어 제거 및 표준화**\n",
    "- 한 행 씩 보면서, 불용어 표준화 진행을 위한 전처리를 먼저 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Requirement: Kor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **명사 추가, 단어 치환, 불용어 처리**\n",
    "- `customized_konlpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. ['시계열 데이터', '데이터 분석', '논문 구현', '알고리즘 개발', '데이터 분석', '통계분석', '인공지능', '머신러닝']\n",
      "1. ['데이터 사이언스', '블록체인', '데이터 분석', '머신러닝', '온체인 데이터', '데이터 분석']\n",
      "2. ['데이터 사이언스', '실무경험', '데이터 분석', '머신러닝', '데이터 분석', '문제 정의 및 해결', '알고리즘 개발', '논문 구현', '데이터 분석', '블록체인', '실무경험']\n",
      "3. ['상용 서비스 구현', '데이터 모델링', '개발역량', '데이터 분석', '데이터 분석', '고객 분석', '데이터 분석', '인사이트 도출']\n",
      "4. ['데이터 모델링']\n",
      "5. ['딥러닝', '프레임워크', '모델 개발', '모델 개발 및 배포', '데이터 분석', '데이터 시각화', '적극적', '커뮤니케이션', '주도적', '개발경험', '협업', '프로젝트 경험', '협업', '포트폴리오']\n",
      "6. ['데이터 사이언티스트', '데이터 모델링', '프로젝트 경험', '실무경험', '개인화 추천', '고객 분석', '데이터 모델링', '고객 분석', '도메인 지식', '프로그래밍 언어', '수리적 지식', '협업', '커뮤니케이션']\n",
      "7. ['통계분석', '컴퓨터공학', '산업공학', '도메인 지식', '데이터 모델링', '알고리즘 개발', '자연어 처리', '모델 개발', '가설 수립 및 검증', '최적화', '통계분석', '데이터 분석', '데이터 분석', '데이터 모델링']\n",
      "8. ['프로젝트 경험', '실무경험', '모바일 데이터', '데이터 분석', '머신러닝', '주도적', '끈질김', '인사이트 도출', '이슈 해결', '문제 정의 및 해결']\n",
      "9. ['통계분석', '컴퓨터공학', '산업공학', '데이터 분석', '데이터 시각화', '데이터 분석', '최신 기술 연구', '열정적']\n",
      "10. ['실무경험', '머신러닝', '프로젝트 경험', '리딩 능력', '논문 구현', '모델 개발', '커뮤니케이션', '협업', '문제 정의 및 해결']\n",
      "11. ['데이터 추출', '데이터 핸들링', '데이터 분석', '모델 개발', '프로젝트 경험', '데이터 전처리', '데이터 분석', '데이터 마트 구축 운영', '데이터 전처리', '데이터 분석', '모델 개발']\n",
      "12. ['데이터 모델링', '데이터 분석', '시스템 개발', '데이터 모델링', '알고리즘 개발', '머신러닝', '수리적 지식', '통계적 지식', '도메인 지식', '머신러닝', '논문 구현', '이슈 해결', '문제 정의 및 해결', '협업', '문제 정의 및 해결', '커뮤니케이션']\n",
      "13. ['실무경험', '머신러닝', '프로젝트 경험', '논문 구현', '커뮤니케이션', '협업', '문제 정의 및 해결']\n",
      "14. ['통계학과', '데이터 분석']\n",
      "15. ['데이터 사이언티스트', '통계분석', '데이터 분석', '수리적 지식', '통계적 지식', '도메인 지식', '데이터 분석', '모델 개발', '알고리즘 개발']\n",
      "16. ['데이터 분석', '데이터 모델링', '머신러닝', '수리적 지식', '통계적 지식', '도메인 지식', '최신 기술 연구', '알고리즘 개발', '문제 정의 및 해결', '커뮤니케이션', '데이터 분석', '데이터 모델링', '커뮤니케이션']\n",
      "17. []\n",
      "18. ['시계열 데이터', '데이터 분석', '논문 구현', '알고리즘 개발', '데이터 분석', '통계분석', '인공지능', '머신러닝']\n",
      "19. ['데이터 사이언티스트', '통계분석', '데이터 분석', '수리적 지식', '통계적 지식', '도메인 지식', '데이터 분석', '모델 개발', '알고리즘 개발']\n",
      "20. []\n",
      "21. ['데이터 분석', '데이터 분석', '데이터 시각화', '머신러닝']\n",
      "22. ['통계학과', '데이터 분석']\n",
      "23. ['도메인 지식', '문제 정의 및 해결', '문제 정의 및 해결', '통계분석', '데이터 기반 의사결정', '편향 제거', '테스트 설계 분석', '인과추론', '인과추론', '통계적 지식']\n",
      "24. ['데이터 추출', '데이터 분석', '정형 데이터', '비정형 데이터', '데이터 모델링']\n",
      "25. ['데이터 핸들링', '리포팅 능력', '프레임워크', '데이터 모델링', '데이터 수집', '데이터 정제', '인사이트 도출', '데이터 추출', '데이터 가공', '통계적 지식', '도메인 지식', '가설 수립 및 검증', '인사이트 도출']\n",
      "26. ['문제 정의 및 해결', '적극적', '데이터 모델링', '모델 개발', '논문 구현']\n",
      "27. ['모델 개발', '프로젝트 경험', '데이터 모델링', '문제 정의 및 해결', '데이터 분석', '커뮤니케이션', '딥러닝', '프레임워크']\n",
      "28. ['통계분석', '도메인 지식', '데이터 분석', '주도적', '데이터 파이프라인 구축', '최신 기술 연구', '데이터 분석', '꼼꼼함', '모델 개발', '커뮤니케이션']\n",
      "29. ['이미지 데이터', '도메인 지식', '머신러닝', '딥러닝', '프로젝트 경험', '최신 기술 연구', '주도적', '책임감', '리드 능력', '문제 정의 및 해결', '문제 정의 및 해결']\n",
      "30. ['도메인 지식', '영상 처리', '영상 신호처리', '논문 구현', '영상 신호처리']\n",
      "31. ['이미지 데이터', '도메인 지식', '머신러닝', '딥러닝', '프로젝트 경험', '최신 기술 연구', '주도적', '책임감', '리드 능력', '문제 정의 및 해결', '문제 정의 및 해결']\n",
      "32. ['데이터 레이블링', '데이터 레이블링']\n",
      "33. ['프로덕트 데이터 분석', '데이터 분석', '데이터 전처리', '데이터 시각화', '가설 수립 및 검증', '데이터 분석', '데이터 분석', '주도적', '인사이트 도출', '인사이트 도출']\n",
      "34. ['머신러닝', '딥러닝', '프로젝트 경험', '데이터 정제', '데이터 전처리', '데이터 분석', '데이터 모델링', '데이터 시각화', '최신 기술 연구', '주도적', '책임감', '리드 능력', '문제 정의 및 해결', '문제 정의 및 해결']\n",
      "35. ['통계적 지식', '도메인 지식', '정량 데이터', '데이터 분석', '데이터 분석', '데이터 전처리', '고객 분석', '지표 정의', '데이터 분석', '데이터 기반 의사결정', '논리적 사고', '분석적 사고', '커뮤니케이션']\n",
      "36. ['데이터베이스', '시스템 개발']\n",
      "37. ['데이터 분석', '모바일 데이터', '모바일 데이터', '데이터 분석', '고객 분석', '리텐션 분석', '코호트 분석', '데이터 분석', '프로덕트 데이터 분석', '지표 정의', '가설 수립 및 검증', '가설 수립 및 검증']\n",
      "38. ['데이터 추출', '정형 데이터', '비정형 데이터', '데이터 전처리', '데이터 시각화', '인사이트 도출', '문제 정의 및 해결', '리포트 작성', '커뮤니케이션']\n",
      "39. ['서버 설계', '문제 정의 및 해결', '적극적', '커뮤니케이션', '리포트 작성', '이미지 데이터', '문제 정의 및 해결', '적극적']\n",
      "40. ['컨설팅', '모델 개발', '데이터 분석', '실무경험', '데이터 분석', '데이터 기반 의사결정', '통계분석', '리포트 작성', '커뮤니케이션']\n",
      "41. ['적응 능력', '성실함', '커뮤니케이션']\n",
      "42. ['통계적 지식', '머신러닝', '딥러닝', '도메인 지식', '문제 정의 및 해결', '프로그래밍 언어', '데이터 분석', '데이터 추출', '데이터 가공', '데이터 사이언스', '데이터 시각화', '데이터 시각화']\n",
      "43. ['머신러닝', '딥러닝', '프로젝트 경험', '데이터 정제', '데이터 전처리', '데이터 분석', '데이터 모델링', '데이터 시각화', '최신 기술 연구', '주도적', '책임감', '리드 능력', '문제 정의 및 해결', '문제 정의 및 해결']\n",
      "44. ['데이터 분석', '모델 개발', '프로젝트 경험', '데이터 분석', '머신러닝', '비정형 데이터', '커뮤니케이션', '협업']\n",
      "45. ['컨설팅', '고객 분석', '커뮤니케이션', '클라우드 경험', '리포트 작성', '영어 커뮤니케이션', '포트폴리오']\n",
      "46. ['프로그래밍 언어', '데이터 분석', '비즈니스 인텔리전스', '리포팅 능력', '대시보드 구축', '협업', '데이터 분석', '인사이트 도출']\n",
      "47. []\n",
      "48. ['데이터 분석', '문제 정의 및 해결', '가설 수립 및 검증', '가설 수립 및 검증', '가설 수립 및 검증', '데이터 분석', '데이터 추출', '데이터 분석', '데이터 모델링', '빅데이터 분석', '데이터 수집', '데이터 핸들링', '프로젝트 경험', '리딩 능력', '데이터 분석', '실무경험', '데이터 기반 의사결정', '정량 데이터', '데이터 분석', '인사이트 도출', '데이터 시각화', '리포팅 능력']\n",
      "49. ['데이터 분석', '실무경험', '데이터 분석', '머신러닝', '도메인 지식', '데이터 추출', '데이터 전처리', '데이터 분석', '인사이트 도출', '데이터 분석', '데이터 분석', '논리적 사고', '데이터 시각화', '커뮤니케이션', '협업']\n",
      "50. ['데이터 분석', '데이터 추출', '데이터 전처리', '데이터 분석', '인사이트 도출', '데이터 분석', '문제 정의 및 해결', '논리적 사고']\n",
      "51. ['데이터 분석', '리딩 능력', '데이터 처리', '리포트 작성', '대시보드 구축', '데이터 분석', '코드 해석']\n",
      "52. ['데이터 분석', '데이터 분석', '웹 분석', '캠페인 분석', '데이터 분석', '데이터 분석', '협업', '커뮤니케이션']\n",
      "53. ['영어 커뮤니케이션', '커뮤니케이션', '꼼꼼함']\n",
      "54. ['데이터 분석', '엑셀', '파이썬', '데이터 분석', '문제 정의 및 해결', '문제 정의 및 해결', '커뮤니케이션', '리포트 작성']\n",
      "55. []\n",
      "56. []\n",
      "57. ['데이터 분석', '프로그래밍 언어', '빅데이터 분석', '데이터 분석', '수리적 지식', '도메인 지식', '최신 기술 연구', '열정적']\n",
      "58. ['꼼꼼함', '책임감', '영어 커뮤니케이션', '리포트 작성']\n",
      "59. ['데이터 분석', '데이터 추출', '데이터 정제', '데이터 추출', '데이터 가공', '데이터 시각화', '지표 정의', '지표 정의', '가설 수립 및 검증', '가설 수립 및 검증', '가설 수립 및 검증', '테스트 설계 분석', '데이터 분석', '프로덕트 데이터 분석', '모바일 데이터', '데이터 분석', '퍼널 분석', '데이터 분석', '리텐션 분석', '데이터 분석', '코호트 분석', '데이터 분석', '프로덕트 데이터 분석', '데이터 분석', '데이터 로그 설계', '통계적 지식', '도메인 지식']\n",
      "60. ['데이터 엔지니어링', '실무경험', '분산 처리 시스템', '비동기 데이터', '프로그래밍 언어', '데이터 분석']\n",
      "61. ['데이터 분석', '실무경험', '문제 정의 및 해결', '지표 정의', '구글스프레드시트', '파이썬', '데이터 가공', '데이터 추출', '데이터 분석', '리포팅 능력', '비즈니스 인텔리전스', '리포팅 능력', '대시보드 구축', '협업', '데이터 분석', '인사이트 도출']\n",
      "62. ['데이터 분석', '데이터 분석', '리딩 능력', '지표 정의', '지표 정의', '가설 수립 및 검증', '가설 수립 및 검증', '가설 수립 및 검증', '테스트 설계 분석', '데이터 분석', '프로덕트 데이터 분석', '통계분석', '인과추론', '데이터 분석', '모바일 데이터', '데이터 분석', '데이터 로그 설계', '퍼널 분석', '데이터 분석', '리텐션 분석', '데이터 분석', '코호트 분석', '데이터 분석', '프로덕트 데이터 분석', '데이터 분석']\n",
      "63. ['쿼리', '문제 정의 및 해결', '가설 수립 및 검증', '데이터 추출', '데이터 가공', '데이터 분석', '리포트 작성', '데이터 분석', '데이터 분석', '구조적 사고', '커뮤니케이션']\n",
      "64. ['적응 능력', '성실함', '커뮤니케이션']\n",
      "65. ['엑셀', '파워포인트', '데이터 분석']\n",
      "66. ['데이터 분석', '실무경험', '논리적 사고', '인과추론', '인사이트 도출', '문제 정의 및 해결', '데이터 분석', '데이터 마트 구축 운영']\n",
      "67. ['데이터 추출', '데이터 분석', '데이터 기반 의사결정', '논리적 사고', '오너십']\n",
      "68. ['통계적 지식', '통계분석', '통계분석', '데이터 분석', '데이터 추출', '데이터 정제', '문제 정의 및 해결', '데이터 분석', '데이터 기반 의사결정', '데이터 분석', '통계분석', '리포트 작성', '커뮤니케이션']\n",
      "69. ['블록체인', '협업', '대시보드 구축', '블록체인', '머신러닝']\n",
      "70. ['데이터 분석', '데이터 시각화', '대시보드 구축', '데이터 추출', '데이터 전처리', '데이터 분석', '인사이트 도출', '데이터 분석', '문제 정의 및 해결', '데이터 수집', '데이터 전처리', '데이터 분석', '데이터 모델링', '데이터 분석', '설득력', '커뮤니케이션', '협업']\n",
      "71. ['협업', '커뮤니케이션', '데이터 분석', '데이터 모델링', '프로그래밍 언어', '알고리즘 개발', '고객 분석', '데이터 로그 설계', '데이터 분석', '데이터 분석', '데이터 분석']\n",
      "72. ['데이터 추출', '데이터 가공', '데이터 시각화', '통계분석', '알고리즘 개발', '도메인 지식', '논문 구현', '리포트 작성']\n",
      "73. ['악성코드 분석', '데이터 분석', '취약점 분석', '데이터 분석', '데이터 분석', '리포트 작성', '리포트 작성', '커뮤니케이션', '협업', '커뮤니케이션']\n",
      "74. ['데이터 추출', '데이터 분석', '실무경험', '데이터 분석', '프로젝트 경험', '주도적', '데이터 분석', '논리적 사고', '인과추론', '인사이트 도출', '문제 정의 및 해결']\n",
      "75. ['연구개발', '데이터 분석', '데이터 분석', '컨설팅', '모델 개발', '코드 해석']\n",
      "76. ['프레임워크', '데이터 분석', '모델 개발', '코드 해석']\n",
      "77. []\n",
      "78. ['데이터 분석', '데이터 추출', '데이터 분석', '대시보드 구축', '데이터 시각화', '테스트 설계 분석', '데이터 분석']\n",
      "79. ['프로젝트 경험', '스마트팩토리', '데이터 솔루션', '시스템 개발', '프로젝트 경험', '공정관리', '생산관리', '품질관리', '설비관리', '고객 분석', '시스템 개발', '데이터 분석', '데이터 솔루션', '프레임워크']\n",
      "80. ['컴퓨터 비전', '최신 기술 연구', '딥러닝', '논문 구현', '딥러닝', '프레임워크', '도메인 지식']\n",
      "81. ['악성코드 분석', '데이터 분석', '취약점 분석', '데이터 분석', '데이터 분석', '리포트 작성', '리포트 작성', '커뮤니케이션', '협업', '커뮤니케이션']\n",
      "82. []\n",
      "83. ['백엔드', '데이터베이스', '데이터 모델링', '빅데이터 분석', '프레임워크', '데이터 시각화']\n",
      "84. ['클라우드 경험', '플랫폼 구축', '어플리케이션 개발', '프로그래밍 언어', '데이터 파이프라인 구축', '최신 기술 연구', '열정적', '도전적']\n",
      "85. ['웹크롤링', '데이터 엔지니어링', '협업', '커뮤니케이션', '코드 해석']\n",
      "86. []\n",
      "87. ['모바일 데이터', '데이터 어플리케이션 서비스', '플랫폼 구축', '고객 분석', '데이터 기반 의사결정', '협업', '커뮤니케이션', '데이터 분석', '프로젝트 경험']\n",
      "88. ['데이터 솔루션', '적극적', '적응 능력', '데이터 모델링', '리포트 작성']\n",
      "89. ['시스템 개발', '도메인 지식', '소프트웨어', '고가용성', '내고장성', '도메인 지식', '오픈소스 경험']\n",
      "90. ['스키마', '다이어그램', '데이터베이스', '데이터베이스', '튜닝 역량']\n",
      "91. ['최신 기술 연구', '데이터베이스', '도메인 지식', '끈기']\n",
      "92. ['데이터 파이프라인 구축', '클라우드 경험', '커뮤니케이션']\n",
      "93. []\n",
      "94. ['플랫폼 구축', '이슈 해결', '데이터 분석']\n",
      "95. []\n",
      "96. ['플랫폼 구축', '데이터 모델링', '클라우드 경험', '문제 정의 및 해결', '문제 정의 및 해결', '도메인 지식']\n",
      "97. ['업무 자동화', '데이터 파이프라인 구축', '도메인 지식', '딥러닝', '모델 개발', '적극적', '커뮤니케이션', '주도적', '개발경험', '협업', '프로젝트 경험', '협업', '포트폴리오']\n",
      "98. []\n",
      "99. ['데이터 엔지니어링', '컴퓨터공학', '파이썬', '웹크롤링', '데이터 엔지니어링', '프로젝트 경험']\n",
      "100. ['알고리즘 개발', '데이터베이스', '전산 지식', '도메인 지식', '분산 처리 시스템', '데이터 수집', '도메인 지식', '클라우드 경험', '커뮤니케이션', '최신 기술 연구']\n",
      "101. ['데이터 수집', '데이터 전처리']\n",
      "102. ['플랫폼 구축', '거버넌스', '기술통계', '데이터 분석', '프로그래밍 언어', '플랫폼 구축']\n",
      "103. ['컴퓨터공학', '통계적 지식', '경영학', '경영정보학', '산업공학', '데이터 분석', '데이터 추출', '데이터 핸들링', '데이터 분석', '모델 개발', '프로젝트 경험', '데이터 전처리', '데이터 분석', '데이터 마트 구축 운영']\n",
      "104. ['컴퓨터공학', '통계적 지식', '경영학', '경영정보학', '산업공학', '데이터 분석', '데이터 추출', '데이터 핸들링', '데이터 분석', '모델 개발', '프로젝트 경험', '데이터 전처리', '데이터 분석', '데이터 마트 구축 운영']\n",
      "105. ['데이터 스트리밍', '데이터 파이프라인 구축', '데이터베이스', '쿼리', '프로그래밍 언어']\n",
      "106. ['백업 모니터링', '장애처리', '최적화']\n",
      "107. ['인프라 구축 운영', '코드 해석', '높은 코드 퀄리티', '아키텍쳐', '플랫폼 구축', '알고리즘 개발', '데이터베이스', '전산 지식', '도메인 지식']\n",
      "108. ['인프라 구축 운영', '전산 지식', '도메인 지식']\n",
      "109. ['정보보호', '보안', '정보보호', '도메인 지식', '시스템 개발', '보안', '취약점 분석', '데이터 분석', '스크립트 언어', '적극적', '커뮤니케이션', '커뮤니케이션', '리드 능력']\n",
      "110. ['데이터 수집', '데이터 전처리', '데이터 파이프라인 구축', '프로그래밍 언어', '배치 프로세싱', '실시간 데이터 처리']\n",
      "111. ['리눅스']\n",
      "112. ['빅데이터 분석', '데이터 파이프라인 구축', '정량 데이터', '분석적 사고', '컴퓨터공학', '물리학', '산업공학', '금융공학', '도메인 지식']\n",
      "113. ['데이터베이스', '데이터베이스', '장애처리', '리포팅 능력', '적극적', '커뮤니케이션']\n",
      "114. ['문제 정의 및 해결', '업무 자동화', '데이터 파이프라인 구축', '데이터 분석', '데이터 마트 구축 운영']\n",
      "115. ['시스템 개발', '클라우드 경험', '컨테이너 경험', '데이터 시각화', '데이터 분석', '가설 수립 및 검증']\n",
      "116. ['데이터 파이프라인 구축', '데이터 모델링', '튜닝 역량', '빅데이터', '클라우드 경험', '빅데이터']\n",
      "117. ['백엔드', '서버 설계', '모델 개발', '데이터 마트 구축 운영', '주도적', '데이터 시각화', '데이터 분석', '트러블슈팅']\n",
      "118. ['어플리케이션 개발', '분산 처리 시스템', '시스템 개발', '실시간 데이터 처리']\n",
      "119. ['인프라 구축 운영', '코드 해석', '높은 코드 퀄리티', '아키텍쳐', '플랫폼 구축', '알고리즘 개발', '데이터베이스', '전산 지식', '도메인 지식']\n",
      "120. ['백엔드', '데이터 엔지니어링', '어플리케이션 개발', '플랫폼 구축', '이슈 해결', '데이터 분석']\n",
      "121. ['소프트웨어', '테스트 설계 분석', '시스템 개발', '소프트웨어', '데이터 엔지니어링']\n",
      "122. ['웹크롤링', '도메인 지식', '지표 정의']\n",
      "123. ['데이터 수집', '데이터 전처리']\n",
      "124. ['플랫폼 구축', '거버넌스', '기술통계', '데이터 분석', '프로그래밍 언어']\n",
      "125. ['소프트웨어', '데이터 엔지니어링', '데이터 파이프라인 구축', '데이터 오케스트레이션', '클라우드 경험', '데이터 솔루션', '협업', '소프트웨어', '컨테이너 경험', '데이터 오케스트레이션', '문제 정의 및 해결', '협업']\n",
      "126. ['문제 정의 및 해결', '업무 자동화', '데이터 파이프라인 구축', '데이터 분석', '데이터 마트 구축 운영', '클라우드 경험']\n",
      "127. ['시스템 개발', '클라우드 경험', '플랫폼 구축', '백엔드', '프로그래밍 언어', '워크플로우']\n",
      "128. []\n",
      "129. ['스키마', '다이어그램', '데이터베이스', '데이터베이스', '튜닝 역량']\n",
      "130. []\n",
      "131. ['비동기처리', '네트워크 구축', '웹서비스', '텍스트 데이터']\n",
      "132. ['웹서비스']\n",
      "133. ['소프트웨어', '도메인 지식', '도메인 지식', '가설 수립 및 검증']\n",
      "134. ['긍정적', '커뮤니케이션']\n",
      "135. ['데이터 추출', '데이터 가공', '데이터 분석', '클라우드 경험', '플랫폼 구축', '데이터 솔루션', '데이터 분석']\n",
      "136. ['하둡', '시스템 개발', '데이터 시각화', '데이터 수집', '업무 자동화']\n",
      "137. ['데이터 솔루션', '플랫폼 구축', '데이터 분석', '시스템 개발', '빅데이터 분석', '프로젝트 경험', '문제 정의 및 해결', '창의성']\n",
      "138. ['하이퍼 캐주얼 게임']\n",
      "139. ['데이터 프로세싱', '최적화', '분산 처리 시스템', '백엔드']\n",
      "140. ['클라우드 경험', '플랫폼 구축', '어플리케이션 개발', '프로그래밍 언어', '데이터 파이프라인 구축', '최신 기술 연구', '열정적', '도전적']\n",
      "141. ['데이터 파이프라인 구축', '인프라 구축 운영', '플랫폼 구축', '플랫폼 구축', '주도적', '데이터 분석', '프로덕트 데이터 분석']\n",
      "142. ['서버 설계', '데이터 모델링', '최적화', '커뮤니케이션', '협업']\n",
      "143. ['머신러닝', '수리적 지식', '데이터 모델링', '데이터 전처리', '데이터 분석', '데이터 시각화', '대시보드 구축', '데이터 추출', '데이터 전처리', '데이터 분석', '모델 개발', '데이터 분석', '문제 정의 및 해결', '데이터 수집', '데이터 전처리', '데이터 분석', '데이터 모델링', '데이터 분석', '모델 개발', '설득력', '데이터 가공', '커뮤니케이션', '협업']\n",
      "144. ['데이터 솔루션', '플랫폼 구축', '프로젝트 경험', '클라우드 경험', '도메인 지식', '호기심', '프로토타이핑', '데이터 분석', '데이터 사이언티스트', '커뮤니케이션']\n",
      "145. ['클라우드 경험', '클라우드 경험', '데이터베이스', '쿼리', '데이터 분석', '커뮤니케이션', '백엔드']\n",
      "146. ['데이터 솔루션', '플랫폼 구축', '프로젝트 경험', '클라우드 경험', '도메인 지식', '호기심', '프로토타이핑', '데이터 분석', '데이터 사이언티스트', '커뮤니케이션']\n",
      "147. ['소프트웨어', '프로젝트 경험', '커뮤니케이션', '프레젠테이션 기술']\n",
      "148. ['데이터 파이프라인 구축', '프로그래밍 언어']\n",
      "149. ['프로그래밍 언어', '머신러닝', '모델 개발 및 배포', '데이터 분석', '인사이트 도출', '주도적', '프로젝트 경험', '주도적']\n",
      "150. ['프레임워크', '서버 설계']\n",
      "151. ['데이터베이스', '파이썬', '프로그래밍 언어', '스크립트 언어', '프레임워크', '도메인 지식']\n",
      "152. ['영상 처리', '이미지 데이터', '데이터 레이블링', '커뮤니케이션', '긍정적', '책임감', '성실함']\n",
      "153. ['데이터 파이프라인 구축', '스트림 프로세싱', '데이터 프로세싱']\n",
      "154. ['리포트 작성', '도전적']\n",
      "155. ['데이터 파이프라인 구축', '최적화']\n",
      "156. ['실무경험', '데이터 파이프라인 구축', '분산 처리 시스템', '시스템 개발', '온체인 데이터', '데이터 엔지니어링', '실무경험']\n",
      "157. ['프로그래밍 언어', '빅데이터 분석', '플랫폼 구축', '인프라 구축 운영']\n",
      "158. ['데이터 분석', '데이터 분석']\n",
      "159. ['통계적 지식', '산업공학']\n",
      "160. ['빅데이터 분석', '책임감']\n",
      "161. ['모델 개발 및 배포', '포트폴리오']\n",
      "162. ['컴퓨터공학', '통계적 지식', '경영학', '경영정보학', '산업공학', '데이터 분석', '데이터 추출', '데이터 핸들링', '데이터 분석', '모델 개발', '프로젝트 경험', '데이터 전처리', '데이터 분석', '데이터 마트 구축 운영']\n",
      "163. ['컴퓨터공학', '통계적 지식', '경영학', '경영정보학', '산업공학', '데이터 분석', '데이터 추출', '데이터 핸들링', '데이터 분석', '모델 개발', '프로젝트 경험', '데이터 전처리', '데이터 분석', '데이터 마트 구축 운영']\n",
      "164. ['데이터 엔지니어링', '컴퓨터공학', '파이썬', '웹크롤링', '데이터 엔지니어링', '프로젝트 경험']\n",
      "165. ['엑셀']\n",
      "166. ['리눅스']\n",
      "167. ['클라우드 경험', '플랫폼 구축', '데이터 엔지니어링']\n",
      "168. ['클라우드 경험', '데이터 레이크하우스', '배치 프로세싱', '실시간 데이터 처리']\n",
      "169. ['최신 기술 연구', '데이터베이스', '도메인 지식', '끈기']\n",
      "170. ['웹크롤링', '데이터 엔지니어링', '협업', '커뮤니케이션', '코드 해석']\n",
      "171. ['클라우드 경험', '오픈소스 경험', '클라우드 경험', '데이터베이스', '플랫폼 구축', '클라우드 경험', '커뮤니케이션']\n",
      "172. ['데이터 스트리밍', '데이터 파이프라인 구축', '데이터베이스', '쿼리', '프로그래밍 언어']\n",
      "173. ['데이터 파이프라인 구축', '데이터 엔지니어링', '데이터 수집', '협업', '문제 정의 및 해결', '문제 정의 및 해결']\n",
      "174. ['플랫폼 구축', '프로그래밍 언어', '프레임워크', '코드 해석', '비판', '커뮤니케이션', '커뮤니케이션']\n",
      "175. ['정규화', '트랜잭션', '쿼리', '쿼리', '파이썬', '데이터 전처리']\n",
      "176. ['데이터베이스', '도메인 지식', '통계분석', '프레임워크']\n",
      "177. ['빅데이터 분석', '데이터 엔지니어링', '데이터 파이프라인 구축', '프로그래밍 언어', '플랫폼 구축', '문제 정의 및 해결']\n",
      "178. ['도메인 지식', '분산 처리 시스템', '시스템 개발', '클라우드 경험', '통계적 지식', '데이터 분석']\n",
      "179. ['시스템 개발', '분산 처리 시스템', '시스템 개발']\n",
      "180. ['보안', '시스템 개발']\n"
     ]
    }
   ],
   "source": [
    "# 한글, 영문 대소문자, 숫자, 공백을 제외한 문자를 제거\n",
    "req = df.Requirement.tolist()\n",
    "\n",
    "req2 = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    str = req[idx]\n",
    "    new_str = re.sub('\\n', '', str)\n",
    "    new_str2 = re.sub(r'[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]', ' ', new_str)\n",
    "    new_str3 = re.split(' ', new_str2)\n",
    "    new_str4 = ' '.join(filter(None, new_str3))\n",
    "    req2.append(new_str4)\n",
    "    # print(f'{idx}. {req2[idx]}')\n",
    "req2_df = pd.DataFrame({'req': req2})\n",
    "\n",
    "\n",
    "# Kor, Eng 분리\n",
    "eng_list = []\n",
    "kor_list = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    text = req2_df.req[idx]\n",
    "    eng_text = ' '.join(re.findall(r'[A-Za-z]+', text))\n",
    "    kor_text = re.sub(r'[A-Za-z]+', '', text).strip()\n",
    "    eng_list.append(eng_text)\n",
    "    kor_list.append(kor_text)\n",
    "\n",
    "\n",
    "# 영어 띄어쓰기 기준으로 split\n",
    "eng_split_list = []\n",
    "for eng in eng_list:\n",
    "    spl = eng.split()\n",
    "    eng_split_list.append(spl)\n",
    "# for idx, eng in enumerate(eng_split_list):\n",
    "    # print(f'{idx}. {eng}')\n",
    "\n",
    "\n",
    "# 영어 소문자로 통일\n",
    "eng_lower = []\n",
    "for sub in eng_split_list:\n",
    "    sublist = []\n",
    "    for eng in sub:\n",
    "        eng_ = eng.lower()\n",
    "        sublist.append(eng_)\n",
    "    eng_lower.append(sublist)\n",
    "# for idx, eng in enumerate(eng_lower):\n",
    "    # print(f'{idx}. {eng}')\n",
    "# print(len(eng_lower))\n",
    "\n",
    "\n",
    "# 텍스트 파일 불러오기\n",
    "with open(code_path + 'Postprocessor_Custom_Nouns.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    a = file.read()\n",
    "    spl = a.split(',')\n",
    "    custom_nouns = list()\n",
    "    for i in spl:\n",
    "        spl_ = i.strip(\" '\")\n",
    "        custom_nouns.append(spl_)\n",
    "    # print(custom_nouns)\n",
    "\n",
    "with open(code_path + 'Postprocessor_Replace.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    b = file.read()\n",
    "    spl = b.split(',')\n",
    "    replace = dict()\n",
    "    for i in spl:\n",
    "        key_val = i.split(':')\n",
    "        key = key_val[0].strip(\" '\")\n",
    "        val = key_val[1].strip(\" '\")\n",
    "        replace[key] = val\n",
    "    # print(replace)\n",
    "    \n",
    "with open(code_path + 'Postprocessor_Stopwords.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    c = file.read()\n",
    "    spl = c.split(',')\n",
    "    stopwords = set()\n",
    "    for i in spl:\n",
    "        spl_ = i.strip(\" '\")\n",
    "        stopwords.add(spl_) \n",
    "    # print(stopwords)\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 명사 추가\n",
    "custom_nouns = custom_nouns\n",
    "for noun in custom_nouns:\n",
    "    okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "# 단어 치환\n",
    "replace = replace\n",
    "\n",
    "# 불용어 처리\n",
    "stopwords = stopwords\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger=okt,\n",
    "    passtags={'Noun'},\n",
    "    replace=replace,\n",
    "    stopwords=stopwords,\n",
    "    # passwords=passwords,\n",
    "    # ngrams=ngrams\n",
    ")\n",
    "\n",
    "results = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    elem = kor_list[idx]\n",
    "    # print(postprocessor.pos(elem))\n",
    "    post_onechar = postprocessor.pos(elem)\n",
    "    result = []\n",
    "    for word, tag in post_onechar:\n",
    "        if len(word) > 1:\n",
    "            result.append(word)\n",
    "    results.append(result)\n",
    "    print(f'{idx}. {result}')\n",
    "\n",
    "\n",
    "# 리스트 중복 요소 제거\n",
    "Req_kor_final = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    a = set(results[idx])\n",
    "    b = list(a)\n",
    "    Req_kor_final.append(b)\n",
    "# print(Req_kor_final)\n",
    "# print(len(Req_kor_final))\n",
    "\n",
    "\n",
    "# 데이터프레임 생성\n",
    "Req_kor_final_df = pd.DataFrame({'Requirement_KOR': Req_kor_final})\n",
    "Req_kor_final_df.Requirement_KOR = Req_kor_final_df.Requirement_KOR.apply(lambda x: ', '.join(x))\n",
    "Req_kor_final_df.to_csv(csv_path + 'Requirement_Kor.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Requirement: Eng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. ['python']\n",
      "1. ['sql', 'hql', 'python', 'r']\n",
      "2. ['sql', 'hql', 'python', 'r', 'complex', 'network', 'modeling']\n",
      "3. ['machine', 'learnig', 'python', 'java', 'sql', 'r']\n",
      "4. []\n",
      "5. ['computer', 'vision', 'image', 'video', 'processing', 'tensorflow', 'pytorch', 'or']\n",
      "6. ['ml', 'dl', 'python', 'sql', 'ml', 'dl']\n",
      "7. ['data', 'science', 'ml', 'dl', 'nlp', 'clustering', 'ml', 'dl', 'sql', 'python']\n",
      "8. ['python', 'sql', 'r']\n",
      "9. ['data', 'science', 'python', 'r', 'sql', 'tool', 'splunk', 'tableau', 'tool']\n",
      "10. ['ai']\n",
      "11. ['python', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation', 'r']\n",
      "12. ['python', 'sql', 'spark', 'hadoop']\n",
      "13. ['ai']\n",
      "14. []\n",
      "15. ['python', 'r', 'ml', 'dl']\n",
      "16. []\n",
      "17. ['master', 's', 'degree', 'in', 'a', 'quantitative', 'discipline', 'e', 'g', 'statistics', 'operations', 'research', 'bioinformatics', 'economics', 'computational', 'biology', 'computer', 'science', 'mathematics', 'physics', 'electrical', 'engineering', 'industrial', 'engineering', 'or', 'equivalent', 'research', 'experience', 'years', 'of', 'work', 'experience', 'in', 'data', 'science', 'related', 'fields', 'distinctive', 'problem', 'solving', 'skills', 'good', 'at', 'articulating', 'product', 'questions', 'pulling', 'data', 'from', 'large', 'datasets', 'and', 'using', 'statistics', 'to', 'arrive', 'at', 'a', 'recommendation', 'excellent', 'verbal', 'and', 'written', 'communication', 'skills', 'with', 'ability', 'to', 'present', 'information', 'and', 'analysis', 'results', 'effectively', 'ability', 'to', 'build', 'positive', 'relationships', 'within', 'dsa', 'and', 'with', 'our', 'stakeholders', 'and', 'work', 'effectively', 'with', 'cross', 'functional', 'partners', 'in', 'a', 'global', 'company', 'statistics', 'must', 'have', 'strong', 'knowledge', 'and', 'experience', 'in', 'experimental', 'design', 'hypothesis', 'testing', 'and', 'various', 'statistical', 'analysis', 'techniques', 'such', 'as', 'regression', 'or', 'linear', 'models', 'machine', 'learning', 'must', 'have', 'deep', 'understanding', 'of', 'ml', 'algorithms', 'i', 'e', 'deep', 'learning', 'random', 'forest', 'gradient', 'boosted', 'trees', 'k', 'means', 'clustering', 'etc', 'and', 'its', 'development', 'validation', 'and', 'evaluation', 'programming', 'experience', 'with', 'python', 'r', 'or', 'other', 'scripting', 'language', 'and', 'database', 'language', 'e', 'g', 'sql', 'or', 'data', 'manipulation', 'e', 'g', 'pandas', 'dplyr', 'are', 'required']\n",
      "18. ['python']\n",
      "19. ['python', 'r', 'ml', 'dl']\n",
      "20. ['bi', 'r']\n",
      "21. ['sql', 'r', 'python']\n",
      "22. []\n",
      "23. ['a', 'b']\n",
      "24. ['data', 'analyst', 'data', 'scientist', 'rdb', 'sql', 'python', 'ml']\n",
      "25. ['python', 'sql', 'tensorflow', 'pytorch', 'raw', 'data', 'sql']\n",
      "26. ['ml', 'keras', 'tensorflow', 'pytorch']\n",
      "27. ['ml', 'python', 'sql', 'tensorflow', 'keras', 'pytorch']\n",
      "28. ['sql', 'python', 'pm']\n",
      "29. ['python', 'opencv', 'numpy', 'tensorflow', 'pytorch', 'cnn', 'transformer', 'python', 'opencv', 'numpy', 'pytorch', 'or', 'tensorflow']\n",
      "30. ['remote', 'sensing', 'signal', 'processing', 'earth', 'science', 'sar', 'python', 'matlab']\n",
      "31. ['python', 'opencv', 'numpy', 'tensorflow', 'pytorch', 'cnn', 'transformer', 'python', 'opencv', 'numpy', 'pytorch', 'or', 'tensorflow']\n",
      "32. []\n",
      "33. ['sql']\n",
      "34. ['python', 'prediction', 'anomaly', 'detection', 'classification', 'tensorflow', 'pytorch', 'pandas', 'matplotlib', 'tensorflow', 'pytorch']\n",
      "35. ['sql', 'python', 'saas', 'gmv', 'mrr', 'ltv', 'cohort', 'retention', 'action', 'item']\n",
      "36. ['front', 'end', 'redwoodjs', 'storybook', 'graphql', 'tailwindcss', 'app', 'flutter', 'fastlane', 'back', 'end', 'redwoodjs', 'serverless', 'aws', 'lambda', 'postgresql', 'amazon', 'rds', 'python', 'fastapi']\n",
      "37. []\n",
      "38. ['sql', 'numpy', 'pandas', 'spark', 'powerbi', 'tableau']\n",
      "39. []\n",
      "40. ['css', 'python', 'sas']\n",
      "41. ['sql', 'scripting', 'data', 'handling', 'ms', 'office']\n",
      "42. ['python', 'sql', 'framework', 'pandas', 'scikit', 'learn', 'tensorflow', 'pytorch', 'python', 'tableau', 'ml', 'process', 'data', 'collection', 'eda', 'preprocessing', 'feature', 'engineering', 'training', 'evaluation', 'serving']\n",
      "43. ['python', 'prediction', 'anomaly', 'detection', 'classification', 'tensorflow', 'pytorch', 'pandas', 'matplotlib', 'tensorflow', 'pytorch']\n",
      "44. ['gcp', 'professional', 'machine', 'learning', 'engineer', 'gcp', 'ml', 'ai', 'text', 'image', 'sound']\n",
      "45. ['isp', 'ismp', 'pi', 'dx', 'migration', 'ppt']\n",
      "46. ['sql', 'r', 'python']\n",
      "47. ['essential', 'experiences', 'skillsets', 'personal', 'competencies', 'undertaken', 'years', 'experience', 'in', 'a', 'similar', 'role', 'or', 'data', 'analysis', 'role', 'understanding', 'on', 'the', 'multi', 'cultural', 'multi', 'lingual', 'and', 'diverse', 'environment', 'advanced', 'ms', 'excel', 'and', 'sql', 'skills', 'strong', 'analytical', 'skills', 'a', 'keen', 'eye', 'for', 'details', 'and', 'has', 'a', 'systematic', 'approach', 'in', 'dealing', 'with', 'issues', 'desirable', 'personal', 'competencies', 'and', 'attributes', 'business', 'communication', 'skills', 'in', 'both', 'english', 'and', 'korean', 'ability', 'to', 'be', 'hands', 'on', 'particularly', 'with', 'data', 'collation', 'and', 'preparation']\n",
      "48. ['biz', 'insight', 'crm', 'sql', 'mysql', 'mssql', 'python', 'r', 'sql', 'pandas', 'tensorflow', 'pytorch', 'spark', 'mysql', 'mariadb', 'nosql', 'mongodb', 'pm', 'pl']\n",
      "49. ['sql', 'raw', 'data', 'tableau', 'amplitude']\n",
      "50. ['sql', 'python', 'r']\n",
      "51. ['sql', 'tableau', 'google', 'data', 'studio', 'google', 'sheets', 'ms', 'excel', 'python', 'r']\n",
      "52. ['reporting', 'ppt', 'excel', 'qa', 'digital', 'marketing', 'or', 'or', 'sales']\n",
      "53. ['aa', 'or', 'ga']\n",
      "54. ['sql', 'tableau', 'ga']\n",
      "55. []\n",
      "56. ['bi', 'r']\n",
      "57. ['biomedical', 'python', 'r']\n",
      "58. ['excel']\n",
      "59. ['sql', 'excel', 'python', 'r', 'a', 'b', 'ga', 'amplitude']\n",
      "60. ['batch', 'aws', 'architecture']\n",
      "61. ['sql', 'excel']\n",
      "62. ['sql', 'excel', 'python', 'r', 'a', 'b', 'ga', 'amplitude']\n",
      "63. ['da', 'bi', 'sql', 'python', 'cohort', 'funnel', 'a', 'b', 'test', 'ppt', 'documentation']\n",
      "64. ['crc', 'data', 'handling', 'ms', 'office']\n",
      "65. ['ms', 'office']\n",
      "66. ['excel', 'sql', 'b']\n",
      "67. ['sql', 'data', 'driven', 'business', 'solution']\n",
      "68. ['sas', 'r', 'python', 'sql']\n",
      "69. ['layer', 'or', 'layer', 'block', 'transaction', 'business', 'intelligence', 'sql', 'https', 'towardsdatascience', 'com', 'your', 'guide', 'to', 'basic', 'sql', 'while', 'learning', 'ethereum', 'at', 'the', 'same', 'time', 'eac', 'a', 'etherscan']\n",
      "70. ['sql', 'nosql', 'python', 'r']\n",
      "71. ['python', 'ml', 'dl', 'legacy', 'sql']\n",
      "72. ['t', 'o', 'sql', 'python', 'r', 'tableau', 'powerbi', 'bi', 'ml']\n",
      "73. []\n",
      "74. ['sql', 'excel']\n",
      "75. ['ai', 'mlops', 'ai', 'ml', 'big', 'data', 'ai', 'ml', 'ai', 'ml', 'structure']\n",
      "76. ['ai', 'ml', 'big', 'data', 'python', 'ml', 'pytorch', 'tensorflow', 'ai', 'ai', 'structure']\n",
      "77. ['c', 'c']\n",
      "78. ['sql', 'bi', 'google', 'data', 'studio', 'quicksight', 'metabase', 'a', 'b']\n",
      "79. ['pl', 'pl', 'it']\n",
      "80. ['tensorflow', 'pytorch', 'mxnet']\n",
      "81. []\n",
      "82. ['bachelor', 's', 'degree', 'required', 'majored', 'in', 'economics', 'statistics', 'risk', 'management', 'preferred', 'strong', 'business', 'acumen', 'and', 'communications', 'skills', 'needed', 'to', 'interface', 'with', 'both', 'executive', 'management', 'and', 'our', 'clients', 'including', 'internal', 'business', 'partnering', 'should', 'over', 'years', 'experiences', 'in', 'work', 'years', 'of', 'experience', 'within', 'leading', 'the', 'projective', 'work', 'data', 'analytics', 'quality', 'management', 'risk', 'management', 'general', 'insurance', 'risk', 'management', 'telecommunication', 'preferred', 'advanced', 'knowledge', 'of', 'microsoft', 'office', 'suite', 'applications', 'including', 'excel', 'word', 'and', 'powerpoint', 'power', 'bi', 'and', 'other', 'reporting', 'tools', 'a', 'plus', 'experience', 'working', 'in', 'a', 'multi', 'national', 'multi', 'cultural', 'multi', 'lingual', 'and', 'diverse', 'environment', 'experience', 'applying', 'problem', 'solving', 'skills', 'to', 'complex', 'systems', 'financial', 'operational', 'and', 'associated', 'integrated', 'processes', 'strong', 'time', 'management', 'and', 'organization', 'skills']\n",
      "83. ['go', 'python', 'ml', 'deep', 'learning', 'framework', 'scikit', 'learn', 'tensorflow', 'keras', 'sql', 'hadoop', 'mr', 'hive', 'spark', 'flink', 'presto', 'tableau', 'bi', 'tool']\n",
      "84. ['python', 'etl']\n",
      "85. ['python', 'node', 'js']\n",
      "86. ['postgresql', 'oracle', 'dbms', 'cloud']\n",
      "87. ['pm']\n",
      "88. ['db', 'erwin', 'db', 'db', 'erwin']\n",
      "89. ['linux', 'unix', 'database', 'docker', 'container', 'github', 'com', 'id']\n",
      "90. ['postgresql', 'oracle', 'mysql', 'mariadb', 'db', 'sql', 'stored', 'procedure', 'er']\n",
      "91. ['python', 'airflow']\n",
      "92. ['python', 'aws', 'gcp', 'azure']\n",
      "93. []\n",
      "94. ['aws', 'db', 'sql', 'aws', 's', 'airflow', 'glue', 'emr', 'kinesis', 'redshift', 'mysql', 'elasticsearch', 'mongodb', 'spark', 'kafka', 'python', 'java', 'shell', 'script']\n",
      "95. ['dbms', 'dba', 'dbms', 'postgresql', 'oracle', 'tibero', 'altibase', 'edb', 'experdb', 'mysql', 'db']\n",
      "96. ['aws', 'gcp', 'etl', 'elt']\n",
      "97. ['computer', 'vision', 'image', 'video', 'processing', 'airflow', 'kubeflow', 'kubernetes', 'docker', 'container', 'or']\n",
      "98. []\n",
      "99. []\n",
      "100. ['os', 'python', 'hadoop', 'mr', 'hive', 'spark', 'db', 'third', 'party', 'log', 'streaming', 'sql', 'aws']\n",
      "101. ['sql', 'python']\n",
      "102. ['sql', 'python', 'python', 'scala', 'sql', 'apache', 'spark', 'delta', 'lake', 'airflow', 'kafka', 'fastapi', 'git', 'argocd', 'grafana']\n",
      "103. ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation']\n",
      "104. ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation']\n",
      "105. ['dw', 'data', 'lake', 'elt']\n",
      "106. ['oracle', 'database', 'db', 'schema', 'db', 'oracle', 'db', 'rac', 'db', 'migration']\n",
      "107. ['aws', 'gcp', 'cloud', 'native', 'airflow', 'workflow', 'orchestrator', 'prefect', 'oozie', 'workflow', 'cs', 'os']\n",
      "108. ['java', 'scala', 'python', 'hadoop', 'ecosystem']\n",
      "109. ['ai', 'ml', 'data', 'science', 'shell', 'python']\n",
      "110. []\n",
      "111. []\n",
      "112. ['a', 'k', 's']\n",
      "113. []\n",
      "114. ['etl', 'sql', 'python', 'airflow', 'workflow', 'tool']\n",
      "115. ['aws', 'airflow']\n",
      "116. ['dw', 'dm', 'sql', 'java', 'back', 'end', 'java', 'spring', 'python']\n",
      "117. ['java', 'kotlin', 'scala', 'python', 'sql', 'python', 'airflow']\n",
      "118. ['kotlin', 'java', 'python', 'go', 'scala', 'hadoop', 'kafka', 'spark', 'msa']\n",
      "119. ['aws', 'gcp', 'cloud', 'native', 'airflow', 'workflow', 'orchestrator', 'prefect', 'oozie', 'workflow', 'cs', 'os']\n",
      "120. ['or', 'dba', 'python', 'java', 'aws', 'db', 'sql']\n",
      "121. ['jira', 'qa']\n",
      "122. ['python', 'sql', 'etl', 'aws', 'git']\n",
      "123. ['sql', 'python']\n",
      "124. ['sql', 'python']\n",
      "125. ['ml', 'airflow', 'prefect', 'dagster', 'etl', 'elt', 'aws', 'azure', 'gcp', 'snowflake', 'databricks', 'python', 'sql', 'git', 'github', 'docker', 'docker', 'compose']\n",
      "126. ['etl', 'sql', 'python', 'airflow', 'workflow', 'tool']\n",
      "127. ['python', 'scala', 'java', 'go', 'airflow']\n",
      "128. []\n",
      "129. ['postgresql', 'oracle', 'mysql', 'mariadb', 'db', 'sql', 'stored', 'procedure', 'er']\n",
      "130. ['c', 'c', 'network']\n",
      "131. []\n",
      "132. ['rds', 'nodejs']\n",
      "133. ['ui']\n",
      "134. []\n",
      "135. ['sql', 'raw', 'data', 'aws', 'etl']\n",
      "136. ['python', 'java', 'scala', 'javascript', 'spark', 'hadoop', 'hive', 'sql']\n",
      "137. ['spark', 'airflow', 'jenkins', 'sql', 'python']\n",
      "138. ['python', 'bigquery', 'mysql', 'dba']\n",
      "139. ['ci', 'cd', 'python', 'pandas', 'sql']\n",
      "140. ['python', 'etl']\n",
      "141. ['o', 'o']\n",
      "142. ['python', 'node', 'js', 'sql', 'linux']\n",
      "143. ['sql', 'nosql', 'python', 'r', 'ai', 'ai', 'ai', 'ai', 'd', 'hw']\n",
      "144. ['etl', 'data', 'pipeline', 'python', 'sql']\n",
      "145. ['serverless', 'sql']\n",
      "146. ['etl', 'data', 'pipeline', 'data', 'lake', 'python', 'sql']\n",
      "147. ['ci', 'cd']\n",
      "148. ['spark', 'hive', 'presto', 'elastic', 'stack', 'python', 'scala', 'java', 'sql']\n",
      "149. ['sql', 'python', 'kaggle', 'api']\n",
      "150. ['e', 'g', 'flask', 'django', 'cloud', 'hosting', 'service', 'e', 'g', 'aws']\n",
      "151. ['rdb', 'or', 'nosql', 'python', 'e', 'g', 'flask', 'django']\n",
      "152. ['python', 'format']\n",
      "153. ['kafka', 'spark', 'hadoop', 'eco', 'system', 'python', 'sql']\n",
      "154. ['linux', 'language', 'framework', 'platform', 'etc']\n",
      "155. ['data', 'flow', 'architecture', 'etl', 'sql', 'query', 'index', 'stored', 'procedure', 'in', 'sql', 'server', 'pm']\n",
      "156. []\n",
      "157. ['python']\n",
      "158. ['python', 'sql']\n",
      "159. []\n",
      "160. ['java', 'python', 'hadoop', 'eco', 'system']\n",
      "161. ['back', 'front', 'db', 'deploy', 'github', 'bitbucket', 'repository']\n",
      "162. ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation']\n",
      "163. ['r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation']\n",
      "164. []\n",
      "165. []\n",
      "166. []\n",
      "167. ['dw', 'bi', 'jvm', 'java', 'scala', 'python', 'gcp', 'aws']\n",
      "168. []\n",
      "169. ['python', 'airflow']\n",
      "170. ['python', 'node', 'js']\n",
      "171. ['rdb', 'mysql', 'nosql', 'mongodb', 'kafka', 'python', 'java', 'scala', 'datawarehouse', 'datalake']\n",
      "172. ['dw', 'data', 'lake', 'elt']\n",
      "173. ['python', 'java', 'scala', 'sql']\n",
      "174. []\n",
      "175. ['db', 'db', 'index', 'nosql', 'jupyter', 'notebook', 'pandas', 'numpy', 'scipy']\n",
      "176. ['sql', 'data', 'pipeline', 'airflow', 'workflow', 'tool', 'django', 'fastapi', 'python']\n",
      "177. ['kafka', 'spark', 'hadoop', 'python', 'java', 'scala']\n",
      "178. ['hadoop', 'spark', 'aws', 'gcp', 'azure', 'sql', 'python', 'jvm']\n",
      "179. ['java', 'python', 'back', 'end', 'sql', 'hdfs', 'nosql']\n",
      "180. ['web', 'was', 'dbms', 'container', 'firewall', 'waf', 'vpn', 'utm']\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, df.shape[0]):\n",
    "    print(f'{idx}. {eng_lower[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 17행의 공고내용은 영어였음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **불용어 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['master', 's', 'degree', 'in', 'a', 'quantitative', 'discipline', 'e', 'g', 'statistics', 'operations', 'research', 'bioinformatics', 'economics', 'computational', 'biology', 'computer', 'science', 'mathematics', 'physics', 'electrical', 'engineering', 'industrial', 'engineering', 'or', 'equivalent', 'research', 'experience', 'years', 'of', 'work', 'experience', 'in', 'data', 'science', 'related', 'fields', 'distinctive', 'problem', 'solving', 'skills', 'good', 'at', 'articulating', 'product', 'questions', 'pulling', 'data', 'from', 'large', 'datasets', 'and', 'using', 'statistics', 'to', 'arrive', 'at', 'a', 'recommendation', 'excellent', 'verbal', 'and', 'written', 'communication', 'skills', 'with', 'ability', 'to', 'present', 'information', 'and', 'analysis', 'results', 'effectively', 'ability', 'to', 'build', 'positive', 'relationships', 'within', 'dsa', 'and', 'with', 'our', 'stakeholders', 'and', 'work', 'effectively', 'with', 'cross', 'functional', 'partners', 'in', 'a', 'global', 'company', 'statistics', 'must', 'have', 'strong', 'knowledge', 'and', 'experience', 'in', 'experimental', 'design', 'hypothesis', 'testing', 'and', 'various', 'statistical', 'analysis', 'techniques', 'such', 'as', 'regression', 'or', 'linear', 'models', 'machine', 'learning', 'must', 'have', 'deep', 'understanding', 'of', 'ml', 'algorithms', 'i', 'e', 'deep', 'learning', 'random', 'forest', 'gradient', 'boosted', 'trees', 'k', 'means', 'clustering', 'etc', 'and', 'its', 'development', 'validation', 'and', 'evaluation', 'programming', 'experience', 'with', 'python', 'r', 'or', 'other', 'scripting', 'language', 'and', 'database', 'language', 'e', 'g', 'sql', 'or', 'data', 'manipulation', 'e', 'g', 'pandas', 'dplyr', 'are', 'required']\n",
      "['master', 'degree', 'quantitative', 'discipline', 'e', 'g', 'statistics', 'operations', 'research', 'bioinformatics', 'economics', 'computational', 'biology', 'computer', 'science', 'mathematics', 'physics', 'electrical', 'engineering', 'industrial', 'engineering', 'equivalent', 'research', 'experience', 'years', 'work', 'experience', 'data', 'science', 'related', 'fields', 'distinctive', 'problem', 'solving', 'skills', 'good', 'articulating', 'product', 'questions', 'pulling', 'data', 'large', 'datasets', 'using', 'statistics', 'arrive', 'recommendation', 'excellent', 'verbal', 'written', 'communication', 'skills', 'ability', 'present', 'information', 'analysis', 'results', 'effectively', 'ability', 'build', 'positive', 'relationships', 'within', 'dsa', 'stakeholders', 'work', 'effectively', 'cross', 'functional', 'partners', 'global', 'company', 'statistics', 'must', 'strong', 'knowledge', 'experience', 'experimental', 'design', 'hypothesis', 'testing', 'various', 'statistical', 'analysis', 'techniques', 'regression', 'linear', 'models', 'machine', 'learning', 'must', 'deep', 'understanding', 'ml', 'algorithms', 'e', 'deep', 'learning', 'random', 'forest', 'gradient', 'boosted', 'trees', 'k', 'means', 'clustering', 'etc', 'development', 'validation', 'evaluation', 'programming', 'experience', 'python', 'r', 'scripting', 'language', 'database', 'language', 'e', 'g', 'sql', 'data', 'manipulation', 'e', 'g', 'pandas', 'dplyr', 'required']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "a = ' '.join(eng_lower[17])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(a)\n",
    "\n",
    "results = []\n",
    "for token in word_tokens:\n",
    "    if token not in stop_words:\n",
    "        results.append(token)\n",
    "print(word_tokens)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영어는 일단 `StackTool` 정리한 후에 다시 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preference: Kor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **명사 추가, 단어 치환, 불용어 처리**\n",
    "- `customized_konlpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['데이터 분석', '머신러닝', '기상 데이터', '데이터 모델링'], ['머신러닝', '블록체인', '커뮤니케이션', '빅데이터 분석', '대회 수상자', '리포트 작성', '통계분석', '암호화폐', '영어 커뮤니케이션'], ['머신러닝', '블록체인', '커뮤니케이션', '빅데이터 분석', '대회 수상자', '리포트 작성', '통계분석', '암호화폐', '영어 커뮤니케이션'], ['통계분석', '전산 지식', '데이터 분석'], ['데이터 시각화', '클라우드 경험', '프레임워크'], ['개발경험', '인공지능', '자율주행 데이터', '이미지 데이터', '데이터 분석', '레이더 데이터', '컴퓨터공학', '통계분석'], ['머신러닝', '클라우드 경험', '통계학과', '논리적 사고', '모델 개발', '데이터 분석', '영어 커뮤니케이션', '수학과', '데이터 모델링', '데이터 사이언티스트', '컴퓨터과학'], ['문제 정의 및 해결', '가설 수립 및 검증', '데이터 분석', '구조적 사고', '데이터 모델링'], ['머신러닝', '최신 기술 연구', '통계분석', '모델 개발', '테스트 설계 분석', '이슈 해결', '데이터 마이닝', '모바일 데이터', '고객 분석', '데이터 분석', '정형 데이터', '적극적', '데이터 모델링', '커뮤니케이션', '데이터 사이언스', '통계적 지식', '도메인 지식', '문제 정의 및 해결', '주도적'], ['논문 구현', '머신러닝', '인공지능', '데이터 분석', '통계분석'], ['도메인 지식', '머신러닝', '논문 구현', '상용 서비스 구현', '커뮤니케이션', '모델 개발 및 배포', '영어 커뮤니케이션', '실무경험', '통계분석', '데이터 마이닝', '모델 개발'], ['통계적 지식', '경영학', '프레임워크', '프로젝트 경험', '경영정보학', '데이터 분석', '딥러닝', '경제학', '컴퓨터공학', '프로그래밍 언어', '산업공학'], ['도메인 지식', '최신 기술 연구', '프로덕트 데이터 분석'], ['도메인 지식', '머신러닝', '논문 구현', '상용 서비스 구현', '커뮤니케이션', '모델 개발 및 배포', '영어 커뮤니케이션', '실무경험', '통계분석', '데이터 마이닝', '모델 개발'], ['프레임워크', '데이터 솔루션', '분산 처리 시스템', '대회 수상자', '클라우드 경험', '고객 분석', '병렬 처리 시스템'], ['통계적 지식', '모델 개발', '리딩 능력', '데이터 사이언스'], ['커뮤니케이션', '프로젝트 경험', '텍스트 데이터', '클라우드 경험'], [], ['데이터 분석', '머신러닝', '기상 데이터', '데이터 모델링'], ['통계적 지식', '모델 개발', '리딩 능력', '데이터 사이언스'], ['시스템 개발', '커뮤니케이션'], ['도메인 지식', '테스트 설계 분석', '데이터 분석', '알고리즘 개발', '실무경험', '통계분석'], ['프레임워크', '데이터 솔루션', '분산 처리 시스템', '대회 수상자', '클라우드 경험', '고객 분석', '병렬 처리 시스템'], ['데이터 분석', '코드 해석', '비정형 데이터', '프로덕트 데이터 분석'], ['통계적 지식', '데이터 가공', '프로젝트 경험', '빅데이터 분석', '이미지 데이터', '데이터 분석', '자연어 처리', '데이터 모델링', '컴퓨터과학'], ['통계적 지식', '최신 기술 연구', '책임감', '주도적', '데이터 로그 설계', '데이터 가공', '커뮤니케이션', '인사이트 도출', '데이터 분석', '실무경험', '데이터 모델링', '도전적', '논리적 사고'], ['데이터 가공', '프로젝트 경험', '리딩 능력'], ['데이터 가공', '데이터 솔루션', '프로젝트 경험', '플랫폼 구축', '데이터 분석', '자연어 처리', '데이터 모델링', '포트폴리오'], ['데이터 엔지니어링', '웹크롤링', '경제학', '인프라 구축 운영', '통계분석', '데이터 수집', '클라우드 경험'], ['논문 구현', '최신 기술 연구', '머신러닝', '시계열 데이터', '이미지 데이터', '영상 처리', '알고리즘 개발', '딥러닝', '컴퓨터공학', '산업공학'], ['논문 구현'], ['논문 구현', '최신 기술 연구', '머신러닝', '시계열 데이터', '이미지 데이터', '영상 처리', '알고리즘 개발', '딥러닝', '컴퓨터공학', '산업공학'], ['데이터 레이블링'], ['프로덕트 데이터 분석', '테스트 설계 분석', '대시보드 구축', '데이터 분석', '클라우드 경험'], ['논문 구현', '최신 기술 연구', '머신러닝', '프로젝트 경험', '시계열 데이터', '이미지 데이터', '영상 처리', '알고리즘 개발', '딥러닝', '컴퓨터공학', '산업공학'], ['데이터 시각화', '도메인 지식', '테스트 설계 분석', '분산 처리 시스템', '빅데이터 분석', '데이터 분석', '데이터 엔지니어링', '클라우드 경험', '인과추론', '병렬 처리 시스템'], ['도메인 지식', '문제 정의 및 해결', '전산 지식', '알고리즘 개발', '데이터베이스', '커뮤니케이션'], ['데이터 분석', '데이터 솔루션', '데이터 시각화'], ['데이터 파이프라인 구축', '도메인 지식', '머신러닝', '데이터 분석', '알고리즘 개발', '딥러닝', '커뮤니케이션', '고객 분석'], ['정보처리기사'], ['데이터 분석', '프로젝트 경험', '최신 기술 연구', '모델 개발'], ['통계분석'], ['도메인 지식', '인공지능', '데이터 분석', '영어 커뮤니케이션', '통계분석', '커뮤니케이션', '산업공학'], ['논문 구현', '최신 기술 연구', '머신러닝', '프로젝트 경험', '시계열 데이터', '이미지 데이터', '영상 처리', '알고리즘 개발', '딥러닝', '컴퓨터공학', '산업공학'], ['도메인 지식', '최신 기술 연구', '인공지능', '프로젝트 경험', '챗봇 설계 구현', '모델 개발', '플랫폼 구축', '컴퓨터공학', '클라우드 경험', '통계분석'], ['정보처리기사', '클라우드 경험', '문제 정의 및 해결', '컨설팅', '프로젝트 경험', '데이터 솔루션', '프리젠테이션', '논리적 사고'], ['머신러닝', '전산 지식', '데이터 로그 설계', '데이터 가공', '데이터 분석', '알고리즘 개발', '통계분석'], [], ['데이터 시각화', '프로젝트 경험', '빅데이터 분석', '데이터 분석', '통계분석', '컴퓨터공학', '커뮤니케이션', '산업공학'], ['통계적 지식', '문제 정의 및 해결', '데이터분석준전문가', '빅데이터 분석', '지표 정의', '인사이트 도출', '데이터 분석', '컴퓨터공학', '꼼꼼함', '커뮤니케이션', '빅데이터분석기사', '산업공학'], ['문제 정의 및 해결', '주도적', '지표 정의', '인사이트 도출', '데이터 분석', '협업', '모바일 데이터', '커뮤니케이션'], ['통계적 지식', '도메인 지식', '머신러닝', '문제 정의 및 해결', '지표 정의', '인사이트 도출', '플랫폼 구축', '데이터 분석', '딥러닝', '리딩 능력', '데이터 모델링', '통계분석', '실무매니징', '휴먼매니징'], ['영어 커뮤니케이션', '협업'], ['프로젝트 경험'], ['플랫폼 구축', '세일즈포스'], [], ['시스템 개발', '커뮤니케이션'], ['데이터 분석', '데이터 전처리', '프로젝트 경험'], ['커뮤니케이션', '프로젝트 경험', '영어 커뮤니케이션'], ['가설 수립 및 검증', '데이터 리터러시'], ['데이터 파이프라인 구축', '머신러닝', '업무 자동화', '데이터 솔루션', '쿼리', '빅데이터 분석', '협업', '플랫폼 구축', '데이터 분석', '고가용성', '적극적', '컨테이너 경험', '인프라 구축 운영', '클라우드 경험', '튜닝 역량'], ['머신러닝', '전산 지식', '데이터 로그 설계', '데이터 가공', '데이터 분석', '알고리즘 개발', '리딩 능력', '통계분석'], ['데이터 파이프라인 구축', '데이터 마트 구축 운영', '리드 능력', '협업', '가설 수립 및 검증', '데이터 분석', '데이터 리터러시'], ['통계적 지식', '컨설팅', '문제 정의 및 해결', '가설 수립 및 검증', '데이터 분석', '구조적 사고'], ['통계분석', '영어 커뮤니케이션', '커뮤니케이션'], [], ['데이터 시각화', '지표 정의', '데이터 분석', '통계분석', '고객 분석'], ['통계적 지식', '데이터 시각화', '경영학', '스크립트 언어', '대시보드 구축', '데이터 분석', '배치 프로세싱', '컴퓨터공학'], ['통계적 지식', '텍스트 데이터', '커뮤니케이션', '데이터분석준전문가', '프로젝트 경험', '빅데이터 분석', '데이터 분석', '대회 수상자', '통계분석', '모델 개발', '빅데이터분석기사'], ['통계적 지식', '코드 해석', '블록체인', '커뮤니케이션', '프로젝트 경험', '데이터 분석', '리포트 작성', '암호화폐', '데이터 모델링', '영어 커뮤니케이션'], ['머신러닝', '문제 정의 및 해결', '분석적 사고', '데이터 모델링', '통계분석', '트러블슈팅'], ['컴퓨터공학', '데이터 분석', '통계적 지식'], ['문제 정의 및 해결', '핀테크', '빅데이터 분석', '가설 수립 및 검증', '통계분석', '커뮤니케이션'], ['정보보호', '프로젝트 경험', '보안', '데이터 분석', '대회 수상자', '전문 교육 수료'], ['데이터 시각화', '플랫폼 구축', '데이터 분석', '실무경험', '고객 분석'], ['데이터 분석', '모델 개발'], ['데이터 분석', '모델 개발'], ['컴퓨터 비전', '머신러닝', '이미지 데이터', '데이터 프로세싱', '최적화'], ['협업', '인사이트 도출', '데이터 분석', '데이터 웨어하우스', '가설 수립 및 검증', '데이터 전처리', '프로그래밍 언어', '클라우드 경험'], ['도메인 지식', '최신 기술 연구', '머신러닝', '주도적', '프로젝트 경험', '시스템 개발', '통계분석', '데이터 마이닝', '커뮤니케이션', '고객 분석', '산업공학'], ['데이터 파이프라인 구축', '머신러닝', '딥러닝', '모델 개발'], ['정보보호', '프로젝트 경험', '보안', '데이터 분석', '대회 수상자', '전문 교육 수료'], [], ['통계적 지식', '머신러닝', '프레임워크', '빅데이터 분석', '딥러닝', '실무경험', '오픈소스 경험', '오픈소스 메인테이너', '오픈소스 컨트리뷰터'], ['데이터 파이프라인 구축', '데이터 시각화', '도메인 지식', '시스템 개발', '보안', '분산 처리 시스템', '리포팅 능력'], ['데이터 파이프라인 구축', '백엔드', '프레임워크', '데이터 가공', '워크플로우', '플랫폼 구축', '데이터 분석', '데이터 수집', '클라우드 경험'], ['정보처리기사', '열정적'], ['플랫폼 구축', '데이터 분석', '상용 서비스 구현', '클라우드 경험'], ['네트워킹 경험', '데이터 솔루션', '프로젝트 경험'], ['도메인 지식', '분산 스토리지 구축 운영', '시스템 개발', '분산 처리 시스템', '인프라 구축 운영', '오픈소스 경험'], ['스크립트 언어', '데이터베이스', '프로그래밍 언어', '클라우드 경험'], ['데이터 파이프라인 구축', '데이터 마트 구축 운영', '데이터 레이크하우스', '데이터베이스', '엔진 활용 경험'], ['플랫폼 구축', '데이터 분석', '시스템 개발', '애플리케이션 구축'], ['컴퓨터공학', '전산 지식', '정보처리기사', '개발경험'], ['어플리케이션 개발', '데이터 시각화', '시스템 개발', '분산 처리 시스템', '플랫폼 구축', '클라우드 경험'], ['테스트 설계 분석'], ['데이터 파이프라인 구축', '애플리케이션 구축', '시스템 개발', '파이썬', '데이터 웨어하우스', '모델 개발'], ['인공지능', '자율주행 데이터', '이미지 데이터', '데이터 분석', '레이더 데이터', '컴퓨터공학', '데이터베이스', '통계분석'], [], ['데이터 파이프라인 구축', '코드 해석', '최신 기술 연구', '도메인 지식', '긍정적', '주도적', '책임감', '프론트엔드', '전문 교육 수료', '데이터 사이언티스트', '프로그래밍 언어', '커뮤니케이션', '병렬 처리 시스템'], ['실시간 데이터 처리', '커뮤니케이션', '시스템 개발', '분산 처리 시스템', '데이터 엔지니어링', '영어 커뮤니케이션'], ['프로젝트 경험', '온프레미스 구축', '시스템 개발', '고가용성', '네트워크 구축', '리딩 능력', '서버 설계'], ['플랫폼 구축', '데이터 전처리', '데이터 시각화', '애플리케이션 구축'], ['컴퓨터공학', '데이터 로그 설계', '통계적 지식', '커뮤니케이션'], ['컴퓨터공학', '데이터 로그 설계', '통계적 지식', '커뮤니케이션'], ['웹크롤링', '빅데이터 분석', '플랫폼 구축', '리포팅 능력', '클라우드 경험'], ['시스템 개발', '데이터 솔루션', '튜닝 역량'], ['도메인 지식', '코드 해석', '데이터 분석', '인프라 구축 운영', '데이터 엔지니어링', '소프트웨어', '딥다이브', '오픈소스 경험', '커뮤니케이션'], ['분산 처리 시스템'], ['영어 커뮤니케이션', '정보관리기술사', '정보보안기사'], ['코드 해석', '최신 기술 연구', '문제 정의 및 해결', '테스트 설계 분석', '협업', '적극적', '고객 분석'], ['리눅스'], ['데이터 스트리밍', '프로젝트 경험', '시계열 데이터', '데이터 분석', '오픈소스 경험', '영어 커뮤니케이션', '고객 분석'], ['고가용성'], ['데이터 파이프라인 구축', '데이터 시각화', '프레임워크', '데이터 품질 체크', '업무 자동화', '데이터 분석', '인프라 구축 운영', '오픈소스 경험', '트러블슈팅', '클라우드 경험'], ['데이터 시각화', '도메인 지식', '전산 지식', '웹크롤링', '시스템 개발', '데이터 오케스트레이션', '쿼리', '대시보드 구축', '알고리즘 개발', '컨테이너 경험', '데이터베이스', '최적화', '튜닝 역량'], ['데이터 파이프라인 구축', '데이터 분석', '데이터 시각화', '커뮤니케이션'], ['데이터 시각화', '백엔드', '데이터 마트 구축 운영', '주도적', '데이터 분석', '서버 설계', '트러블슈팅', '모델 개발'], ['데이터 엔지니어링', '데이터 레이크하우스'], ['도메인 지식', '코드 해석', '데이터 분석', '인프라 구축 운영', '데이터 엔지니어링', '소프트웨어', '딥다이브', '오픈소스 경험', '커뮤니케이션'], ['데이터 파이프라인 구축', '데이터 시각화', '쿠버네티스', '시스템 개발', '분산 처리 시스템', '플랫폼 구축', '클라우드 경험'], ['프로젝트 경험', '가설 수립 및 검증', '데이터 엔지니어링', '소프트웨어', '프로그래밍 언어'], ['데이터 분석', '데이터 시각화'], ['프로젝트 경험', '온프레미스 구축', '시스템 개발', '고가용성', '네트워크 구축', '리딩 능력', '서버 설계'], ['플랫폼 구축', '데이터 전처리', '데이터 시각화', '애플리케이션 구축'], ['도메인 지식', '컴퓨터 비전', '프로젝트 경험', '영상 처리', '데이터 분석', '통계분석'], ['데이터 파이프라인 구축', '데이터 시각화', '프레임워크', '데이터 품질 체크', '업무 자동화', '데이터 분석', '인프라 구축 운영', '오픈소스 경험', '트러블슈팅', '클라우드 경험'], ['실시간 데이터 처리', '클라우드 경험', '핀테크', '분산 처리 시스템', '테라폼', '리소스 관리', '인프라 구축 운영', '쿠버네티스'], ['분석적 사고', '데이터 솔루션', '쿼리', '논리적 사고'], ['스크립트 언어', '데이터베이스', '프로그래밍 언어', '클라우드 경험'], [], ['딥러닝', '모델 개발', '자연어 처리', '메시지큐'], ['프레임워크'], ['주도적', '프로젝트 경험'], ['소프트웨어', '도메인 지식', '리눅스'], ['데이터 파이프라인 구축', '프로덕트 데이터 분석', '주도적', '플랫폼 구축', '데이터 분석', '인프라 구축 운영'], ['컴퓨터공학', '데이터 수집', '데이터 레이크하우스'], ['협업', '커뮤니케이션', '문제 정의 및 해결'], ['어플리케이션 개발', '소프트웨어', '도메인 지식', '문제 정의 및 해결', '전산 지식', '긍정적', '모델 개발 및 배포', '프론트엔드', '알고리즘 개발', '적극적', '컨테이너 경험', '간단', '데이터베이스', '클라우드 경험'], ['도메인 지식', '긍정적', '데이터 표준화', '열정적', '대시보드 구축'], ['데이터 파이프라인 구축', '데이터 시각화', '도메인 지식', '시스템 개발', '보안', '분산 처리 시스템', '리포팅 능력'], ['데이터 파이프라인 구축', '프로덕트 데이터 분석', '주도적', '플랫폼 구축', '데이터 분석', '인프라 구축 운영'], ['도메인 지식', '프레임워크', '업무 자동화', '모델 개발 및 배포', '플랫폼 구축', '장애처리', '인프라 구축 운영', '실무경험', '통계분석', '클라우드 경험'], ['통계분석', '머신러닝', '트러블슈팅', '데이터 모델링'], ['데이터 파이프라인 구축', '실시간 데이터 처리', '데이터 솔루션', '프로젝트 경험', '플랫폼 구축', '데이터 분석', '리딩 능력'], ['데이터베이스', '시스템 개발', '백엔드', '실시간 데이터 처리'], ['데이터 파이프라인 구축', '실시간 데이터 처리', '데이터 솔루션', '프로젝트 경험', '플랫폼 구축', '데이터 분석', '리딩 능력'], ['대회 수상자'], ['데이터 시각화', '프로덕트 데이터 분석', '데이터 프로세싱', '데이터 분석', '인프라 구축 운영', '데이터 모델링', '클라우드 경험'], ['도메인 지식', '머신러닝', '코드 해석', '최신 기술 연구', '문제 정의 및 해결', '논리적 사고', '경진대회 참가', '열정적', '모델 개발', '지표 정의', '협업', '데이터 분석', '토론', '적극적', '딥러닝', '통계분석', '커뮤니케이션'], ['도메인 지식', '블록체인', '커뮤니케이션', '온체인 데이터', '인프라 구축 운영', '데이터베이스', '영어 커뮤니케이션'], ['데이터 파이프라인 구축', '블록체인', '업무 자동화', '커뮤니케이션', '시스템 개발', '데이터 솔루션', '빅데이터 분석', '암호화폐', '데이터 웨어하우스', '온체인 데이터', '데이터베이스', '영어 커뮤니케이션'], ['머신러닝', '데이터 레이블링', '딥러닝'], ['데이터 파이프라인 구축', '시스템 개발', '분산 처리 시스템'], ['머신러닝', '시스템 개발', '분산 처리 시스템', '협업', '인프라 구축 운영'], ['정형 데이터', '비정형 데이터', '품질관리', '데이터 표준화'], ['블록체인', '커뮤니케이션', '데이터 분석', '온체인 데이터', '리포트 작성', '암호화폐', '영어 커뮤니케이션'], ['피처 엔지니어링', '머신러닝', '데이터 엔지니어링', '데이터 모델링'], ['이미지 데이터', '엔진 활용 경험'], ['프로젝트 경험', '빅데이터 분석', '튜닝 역량'], ['최신 기술 연구', '자신감', '빅데이터 분석', '도전적', '적극적', '컴퓨터공학'], ['도메인 지식', '프레임워크', '마이크로서비스', '프로젝트 경험', '플랫폼 구축', '데이터 분석', '오픈소스 경험', '프로그래밍 언어'], ['컴퓨터공학', '데이터 로그 설계', '통계적 지식', '커뮤니케이션'], ['컴퓨터공학', '데이터 로그 설계', '통계적 지식', '커뮤니케이션'], ['데이터 파이프라인 구축', '코드 해석', '최신 기술 연구', '도메인 지식', '긍정적', '주도적', '책임감', '프론트엔드', '전문 교육 수료', '데이터 사이언티스트', '프로그래밍 언어', '커뮤니케이션', '병렬 처리 시스템'], ['엑셀'], ['정보처리기사', '리눅스'], ['도메인 지식', '프레임워크', '데이터 브릭스', '스크립트 언어', '워크플로우', '대시보드 구축', '빅데이터 분석', '데이터 분석', '배치 프로세싱', '자바', '오픈소스 경험', '커뮤니케이션'], ['머신러닝', '실시간 데이터 처리', '모델 개발 및 배포', '경진대회 참가', '데이터 분석', '대회 수상자', '딥러닝', '통계분석', '데이터 마이닝', '최적화', '모델 개발'], ['데이터 파이프라인 구축', '데이터 마트 구축 운영', '데이터 레이크하우스', '데이터베이스', '엔진 활용 경험'], ['데이터 파이프라인 구축', '백엔드', '프레임워크', '데이터 가공', '워크플로우', '플랫폼 구축', '데이터 분석', '데이터 수집', '클라우드 경험'], ['데이터 파이프라인 구축', '실시간 데이터 동기화', '실시간 데이터 처리', '데이터 분석', '배치 프로세싱', '데이터 스트리밍', '테이블 포맷', '엔진 활용 경험'], ['웹크롤링', '빅데이터 분석', '플랫폼 구축', '리포팅 능력', '클라우드 경험'], ['데이터 파이프라인 구축', '데이터 시각화', '실시간 데이터 동기화', '실시간 데이터 처리', '데이터 로그 설계', '시스템 개발', '협업', '데이터 사이언티스트', '최적화'], ['도메인 지식', '문제 정의 및 해결', '주도적', '업무 자동화', '데이터 솔루션', '플랫폼 구축', '리딩 능력', '고객 분석'], ['통계분석', '클라우드 경험'], ['컨테이너 경험', '인프라 구축 운영', '데이터 엔지니어링', '오픈소스 경험', '클라우드 경험'], ['데이터 파이프라인 구축', '클라우드 경험', '문제 정의 및 해결', '데이터 솔루션', '커뮤니케이션'], ['논문 구현', '머신러닝', '리눅스', '시스템 개발', '분산 처리 시스템', '데이터베이스', '프로그래밍 언어', '모델 개발'], ['클라우드 경험', '책임감', '시스템 개발', '협업', '커뮤니케이션'], ['클라우드 경험', '네트워크 구축']]\n"
     ]
    }
   ],
   "source": [
    "# 한글, 영문 대소문자, 숫자, 공백을 제외한 문자를 제거\n",
    "prefer = df.Preference.tolist()\n",
    "\n",
    "prefer2 = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    str = prefer[idx]\n",
    "    new_str = re.sub('\\n', '', str)\n",
    "    new_str2 = re.sub(r'[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]', ' ', new_str)\n",
    "    new_str3 = re.split(' ', new_str2)\n",
    "    new_str4 = ' '.join(filter(None, new_str3))\n",
    "    prefer2.append(new_str4)\n",
    "    # print(f'{idx}. {prefer2[idx]}')\n",
    "prefer2_df = pd.DataFrame({'prefer': prefer2})\n",
    "\n",
    "\n",
    "# Kor, Eng 분리\n",
    "eng_list = []\n",
    "kor_list = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    text = prefer2_df.prefer[idx]\n",
    "    eng_text = ' '.join(re.findall(r'[A-Za-z]+', text))\n",
    "    kor_text = re.sub(r'[A-Za-z]+', '', text).strip()\n",
    "    eng_list.append(eng_text)\n",
    "    kor_list.append(kor_text)\n",
    "\n",
    "\n",
    "# 영어 띄어쓰기 기준으로 split\n",
    "eng_split_list = []\n",
    "for eng in eng_list:\n",
    "    spl = eng.split()\n",
    "    eng_split_list.append(spl)\n",
    "# for idx, eng in enumerate(eng_split_list):\n",
    "    # print(f'{idx}. {eng}')\n",
    "\n",
    "\n",
    "# 영어 소문자로 통일\n",
    "eng_lower = []\n",
    "for sub in eng_split_list:\n",
    "    sublist = []\n",
    "    for eng in sub:\n",
    "        eng_ = eng.lower()\n",
    "        sublist.append(eng_)\n",
    "    eng_lower.append(sublist)\n",
    "# for idx, eng in enumerate(eng_lower):\n",
    "    # print(f'{idx}. {eng}')\n",
    "# print(len(eng_lower))\n",
    "\n",
    "\n",
    "# 텍스트 파일 불러오기\n",
    "with open(code_path + 'Postprocessor_Custom_Nouns.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    a = file.read()\n",
    "    spl = a.split(',')\n",
    "    custom_nouns = list()\n",
    "    for i in spl:\n",
    "        spl_ = i.strip(\" '\")\n",
    "        custom_nouns.append(spl_)\n",
    "    # print(custom_nouns)\n",
    "\n",
    "with open(code_path + 'Postprocessor_Replace.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    b = file.read()\n",
    "    spl = b.split(',')\n",
    "    replace = dict()\n",
    "    for i in spl:\n",
    "        key_val = i.split(':')\n",
    "        key = key_val[0].strip(\" '\")\n",
    "        val = key_val[1].strip(\" '\")\n",
    "        replace[key] = val\n",
    "    # print(replace)\n",
    "    \n",
    "with open(code_path + 'Postprocessor_Stopwords.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    c = file.read()\n",
    "    spl = c.split(',')\n",
    "    stopwords = set()\n",
    "    for i in spl:\n",
    "        spl_ = i.strip(\" '\")\n",
    "        stopwords.add(spl_) \n",
    "    # print(stopwords)\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 명사 추가\n",
    "custom_nouns = custom_nouns\n",
    "for noun in custom_nouns:\n",
    "    okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "# 단어 치환\n",
    "replace = replace\n",
    "\n",
    "# 불용어 처리\n",
    "stopwords = stopwords\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger=okt,\n",
    "    passtags={'Noun'},\n",
    "    replace=replace,\n",
    "    stopwords=stopwords,\n",
    "    # passwords=passwords,\n",
    "    # ngrams=ngrams\n",
    ")\n",
    "\n",
    "results = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    elem = kor_list[idx]\n",
    "    # print(postprocessor.pos(elem))\n",
    "    post_onechar = postprocessor.pos(elem)\n",
    "    result = []\n",
    "    for word, tag in post_onechar:\n",
    "        if len(word) > 1:\n",
    "            result.append(word)\n",
    "    results.append(result)\n",
    "    # print(f'{idx}. {result}')\n",
    "\n",
    "\n",
    "# 리스트 중복 요소 제거\n",
    "Prefer_kor_final = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    a = set(results[idx])\n",
    "    b = list(a)\n",
    "    Prefer_kor_final.append(b)\n",
    "print(Prefer_kor_final)\n",
    "# print(len(Prefer_kor_final))\n",
    "\n",
    "\n",
    "# 데이터프레임 생성\n",
    "Prefer_kor_final_df = pd.DataFrame({'Preference_KOR': Prefer_kor_final})\n",
    "Prefer_kor_final_df.Preference_KOR = Prefer_kor_final_df.Preference_KOR.apply(lambda x: ', '.join(x))\n",
    "Prefer_kor_final_df.to_csv(csv_path + 'Preference_Kor.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
