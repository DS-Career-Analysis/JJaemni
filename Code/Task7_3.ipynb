{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **불용어 제거 및 표준화**\n",
    "- 한 행 씩 보면서, 불용어 표준화 진행을 위한 전처리를 먼저 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from ckonlpy.tag import Twitter as Okt\n",
    "from ckonlpy.tag import Postprocessor\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "logging.getLogger('konlpy').setLevel(logging.ERROR)\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "code_path = '/VSC/JJaemni/Code/'\n",
    "csv_path = '/VSC/JJaemni/CSV/'\n",
    "\n",
    "df = pd.read_csv(csv_path + 'wanted_jobplanet_2.csv')\n",
    "\n",
    "\n",
    "\n",
    "# 리스트 형태로 불러오기\n",
    "def text_list_load(text_path):\n",
    "    with open(text_path, 'r', encoding='utf-8-sig') as file:\n",
    "        L = [line.strip() for line in file.readlines()]\n",
    "    return L\n",
    "\n",
    "# 딕셔너리 형태로 불러오기\n",
    "def text_dict_load(text_path):\n",
    "    L = dict()\n",
    "    with open(text_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            key, val = line.strip().split(': ')\n",
    "            L[key] = val\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "# 한글 전처리\n",
    "def Korean_preprocessing(col):\n",
    "    col_list = df[col].tolist()\n",
    "    \n",
    "    # 한글, 영문 대소문자, 숫자, 공백을 제외한 문자를 제거\n",
    "    col_list2 = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        str = col_list[idx]\n",
    "        new_str = re.sub('\\n', '', str)\n",
    "        new_str2 = re.sub(r'[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]', ' ', new_str)\n",
    "        new_str3 = re.split(' ', new_str2)\n",
    "        new_str4 = ' '.join(filter(None, new_str3))\n",
    "        col_list2.append(new_str4)\n",
    "        # print(f'{idx}. {col_list2[idx]}')\n",
    "    col_list2_df = pd.DataFrame({'col_list': col_list2})\n",
    "\n",
    "\n",
    "    # Kor, Eng 분리\n",
    "    eng_list = []\n",
    "    kor_list = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        text = col_list2_df.col_list[idx]\n",
    "        eng_text = ' '.join(re.findall(r'[a-zA-Z.!@#$%^&*()+]+', text))\n",
    "        kor_text = re.sub(r'[A-Za-z]+', '', text).strip()\n",
    "        eng_list.append(eng_text)\n",
    "        kor_list.append(kor_text)\n",
    "\n",
    "\n",
    "    # 영어 띄어쓰기 기준으로 split\n",
    "    eng_list_spl = []\n",
    "    for eng in eng_list:\n",
    "        spl = eng.split()\n",
    "        eng_list_spl.append(spl)\n",
    "    # for idx, eng in enumerate(eng_list_spl):\n",
    "        # print(f'{idx}. {eng}')\n",
    "\n",
    "\n",
    "    # 영어 소문자로 통일\n",
    "\n",
    "    eng_lower = []\n",
    "    for data in eng_list:\n",
    "        data_low = data.lower()\n",
    "        eng_lower.append(data_low)\n",
    "\n",
    "    # eng_lower = []\n",
    "    # for sub in eng_list_spl:\n",
    "    #     sublist = []\n",
    "    #     for eng in sub:\n",
    "    #         eng_ = eng.lower()\n",
    "    #         sublist.append(eng_)\n",
    "    #     eng_lower.append(sublist)\n",
    "    # # for idx, eng in enumerate(eng_lower):\n",
    "    #     # print(f'{idx}. {eng}')\n",
    "\n",
    "\n",
    "    # 텍스트 파일 불러오기\n",
    "    text_path = './custom_nouns.txt'\n",
    "    custom_nouns = text_list_load(text_path)\n",
    "    # print(custom_nouns)\n",
    "    text_path = './replace.txt'\n",
    "    replace = text_dict_load(text_path)\n",
    "    # print(replace)\n",
    "    text_path = './stopwords.txt'\n",
    "    stopwords = text_list_load(text_path)\n",
    "    # print(stopwords)\n",
    "\n",
    "\n",
    "    # Okt 시작\n",
    "    okt = Okt()\n",
    "\n",
    "\n",
    "    # 명사 추가\n",
    "    custom_nouns = custom_nouns\n",
    "    for noun in custom_nouns:\n",
    "        okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "\n",
    "    # 단어 치환\n",
    "    replace = replace\n",
    "\n",
    "\n",
    "    # 불용어 처리\n",
    "    stopwords = stopwords\n",
    "    postprocessor = Postprocessor(\n",
    "        base_tagger=okt,\n",
    "        passtags={'Noun'},\n",
    "        replace=replace,\n",
    "        stopwords=stopwords,\n",
    "        # passwords=passwords,\n",
    "        # ngrams=ngrams\n",
    "    )\n",
    "\n",
    "\n",
    "    # Postprocessor 처리 후, 한 글자인 단어 제거\n",
    "    results = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        elem = kor_list[idx]\n",
    "        # print(postprocessor.pos(elem))\n",
    "        post_onechar = postprocessor.pos(elem)\n",
    "        result = []\n",
    "        for word, tag in post_onechar:\n",
    "            if len(word) > 1:\n",
    "                result.append(word)\n",
    "        results.append(result)\n",
    "        # print(f'{idx}. {result}')\n",
    "\n",
    "\n",
    "    # 리스트 중복 요소 제거\n",
    "    kor_final = []\n",
    "    for idx in range(0, df.shape[0]):\n",
    "        a = set(results[idx])\n",
    "        b = list(a)\n",
    "        kor_final.append(b)\n",
    "        print(f'{idx}. {b}')\n",
    "    # print(len(kor_final))\n",
    "\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    kor_final_df = pd.DataFrame({f'{col}_KOR': kor_final})\n",
    "    kor_final_df[f'{col}_KOR'] = kor_final_df[f'{col}_KOR'].apply(lambda x: ', '.join(x))\n",
    "    kor_final_df.to_csv(csv_path + f'{col}_KOR.csv', index=False)\n",
    "\n",
    "    return col_list2, col_list2_df, eng_list, kor_list, eng_list_spl, eng_lower, custom_nouns, replace, stopwords, results, kor_final, kor_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Requirement_KOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. ['인공지능', '알고리즘 개발', '데이터 분석', '논문 구현', '통계분석', '머신러닝', '시계열 데이터']\n",
      "1. ['블록체인', '데이터 분석', '데이터 사이언스', '온체인 데이터', '머신러닝']\n",
      "2. ['블록체인', '데이터 분석', '데이터 사이언스', '알고리즘 개발', '논문 구현', '실무경험', '문제 정의 및 해결', '머신러닝']\n",
      "3. ['데이터 분석', '개발역량', '고객 분석', '인사이트 도출', '상용 서비스 구현', '데이터 모델링']\n",
      "4. ['데이터 모델링']\n",
      "5. ['프레임워크', '모델 개발 및 배포', '데이터 분석', '프로젝트 경험', '모델 개발', '협업', '포트폴리오', '데이터 시각화', '적극적', '커뮤니케이션', '주도적', '딥러닝', '개발경험']\n",
      "6. ['도메인 지식', '수리적 지식', '프로젝트 경험', '프로그래밍 언어', '실무경험', '협업', '데이터 모델링', '고객 분석', '개인화 추천', '커뮤니케이션', '데이터 사이언티스트']\n",
      "7. ['도메인 지식', '산업공학', '가설 수립 및 검증', '알고리즘 개발', '데이터 분석', '모델 개발', '최적화', '자연어 처리', '통계분석', '컴퓨터공학', '데이터 모델링']\n",
      "8. ['이슈 해결', '데이터 분석', '프로젝트 경험', '실무경험', '끈질김', '문제 정의 및 해결', '머신러닝', '모바일 데이터', '인사이트 도출', '주도적']\n",
      "9. ['산업공학', '데이터 분석', '열정적', '최신 기술 연구', '통계분석', '컴퓨터공학', '데이터 시각화']\n",
      "10. ['프로젝트 경험', '모델 개발', '논문 구현', '실무경험', '협업', '문제 정의 및 해결', '커뮤니케이션', '머신러닝', '리딩 능력']\n",
      "11. ['데이터 전처리', '데이터 마트 구축 운영', '데이터 분석', '프로젝트 경험', '모델 개발', '데이터 핸들링', '데이터 추출']\n",
      "12. ['도메인 지식', '수리적 지식', '이슈 해결', '알고리즘 개발', '데이터 분석', '논문 구현', '협업', '통계적 지식', '문제 정의 및 해결', '커뮤니케이션', '머신러닝', '데이터 모델링', '시스템 개발']\n",
      "13. ['프로젝트 경험', '논문 구현', '실무경험', '협업', '문제 정의 및 해결', '커뮤니케이션', '머신러닝']\n",
      "14. ['데이터 분석', '통계학과']\n",
      "15. ['도메인 지식', '수리적 지식', '데이터 분석', '알고리즘 개발', '모델 개발', '통계적 지식', '통계분석', '데이터 사이언티스트']\n",
      "16. ['도메인 지식', '수리적 지식', '데이터 분석', '알고리즘 개발', '통계적 지식', '최신 기술 연구', '문제 정의 및 해결', '커뮤니케이션', '머신러닝', '데이터 모델링']\n",
      "17. ['인공지능', '알고리즘 개발', '데이터 분석', '논문 구현', '통계분석', '머신러닝', '시계열 데이터']\n",
      "18. ['도메인 지식', '수리적 지식', '데이터 분석', '알고리즘 개발', '모델 개발', '통계적 지식', '통계분석', '데이터 사이언티스트']\n",
      "19. []\n",
      "20. ['머신러닝', '데이터 시각화', '데이터 분석']\n",
      "21. ['데이터 분석', '통계학과']\n",
      "22. ['도메인 지식', '편향 제거', '데이터 기반 의사결정', '인과추론', '문제 정의 및 해결', '통계적 지식', '통계분석', '테스트 설계 분석']\n",
      "23. ['데이터 분석', '비정형 데이터', '정형 데이터', '데이터 모델링', '데이터 추출']\n",
      "24. ['도메인 지식', '가설 수립 및 검증', '프레임워크', '데이터 정제', '리포팅 능력', '통계적 지식', '데이터 수집', '데이터 가공', '인사이트 도출', '데이터 핸들링', '데이터 모델링', '데이터 추출']\n",
      "25. ['모델 개발', '논문 구현', '문제 정의 및 해결', '적극적', '데이터 모델링']\n",
      "26. ['프레임워크', '데이터 분석', '프로젝트 경험', '모델 개발', '문제 정의 및 해결', '커뮤니케이션', '데이터 모델링', '딥러닝']\n",
      "27. ['도메인 지식', '데이터 분석', '모델 개발', '최신 기술 연구', '커뮤니케이션', '통계분석', '꼼꼼함', '주도적', '데이터 파이프라인 구축']\n",
      "28. ['도메인 지식', '리드 능력', '프로젝트 경험', '이미지 데이터', '문제 정의 및 해결', '최신 기술 연구', '책임감', '머신러닝', '주도적', '딥러닝']\n",
      "29. ['도메인 지식', '영상 처리', '영상 신호처리', '논문 구현']\n",
      "30. ['도메인 지식', '리드 능력', '프로젝트 경험', '이미지 데이터', '문제 정의 및 해결', '최신 기술 연구', '책임감', '머신러닝', '주도적', '딥러닝']\n",
      "31. ['데이터 레이블링']\n",
      "32. ['데이터 전처리', '가설 수립 및 검증', '데이터 분석', '프로덕트 데이터 분석', '인사이트 도출', '주도적', '데이터 시각화']\n",
      "33. ['데이터 전처리', '리드 능력', '데이터 정제', '데이터 분석', '프로젝트 경험', '문제 정의 및 해결', '최신 기술 연구', '데이터 시각화', '책임감', '머신러닝', '주도적', '데이터 모델링', '딥러닝']\n",
      "34. ['데이터 전처리', '도메인 지식', '논리적 사고', '분석적 사고', '데이터 분석', '데이터 기반 의사결정', '통계적 지식', '지표 정의', '고객 분석', '정량 데이터', '커뮤니케이션']\n",
      "35. ['데이터베이스', '시스템 개발']\n",
      "36. ['코호트 분석', '리텐션 분석', '가설 수립 및 검증', '데이터 분석', '지표 정의', '고객 분석', '프로덕트 데이터 분석', '모바일 데이터']\n",
      "37. ['데이터 전처리', '리포트 작성', '비정형 데이터', '문제 정의 및 해결', '정형 데이터', '커뮤니케이션', '인사이트 도출', '데이터 추출', '데이터 시각화']\n",
      "38. ['리포트 작성', '이미지 데이터', '문제 정의 및 해결', '서버 설계', '적극적', '커뮤니케이션']\n",
      "39. ['데이터 분석', '리포트 작성', '모델 개발', '데이터 기반 의사결정', '실무경험', '컨설팅', '커뮤니케이션', '통계분석']\n",
      "40. ['적응 능력', '성실함', '커뮤니케이션']\n",
      "41. ['도메인 지식', '데이터 분석', '데이터 사이언스', '프로그래밍 언어', '통계적 지식', '문제 정의 및 해결', '데이터 시각화', '데이터 가공', '머신러닝', '데이터 추출', '딥러닝']\n",
      "42. ['데이터 전처리', '리드 능력', '데이터 정제', '데이터 분석', '프로젝트 경험', '문제 정의 및 해결', '최신 기술 연구', '데이터 시각화', '책임감', '머신러닝', '주도적', '데이터 모델링', '딥러닝']\n",
      "43. ['데이터 분석', '프로젝트 경험', '모델 개발', '비정형 데이터', '협업', '커뮤니케이션', '머신러닝']\n",
      "44. ['리포트 작성', '포트폴리오', '고객 분석', '컨설팅', '커뮤니케이션', '영어 커뮤니케이션', '클라우드 경험']\n",
      "45. ['데이터 분석', '프로그래밍 언어', '리포팅 능력', '협업', '대시보드 구축', '비즈니스 인텔리전스', '인사이트 도출']\n",
      "46. []\n",
      "47. ['가설 수립 및 검증', '데이터 분석', '빅데이터 분석', '프로젝트 경험', '리포팅 능력', '실무경험', '데이터 기반 의사결정', '문제 정의 및 해결', '데이터 수집', '정량 데이터', '인사이트 도출', '데이터 핸들링', '데이터 모델링', '데이터 추출', '데이터 시각화', '리딩 능력']\n",
      "48. ['데이터 전처리', '도메인 지식', '논리적 사고', '데이터 분석', '실무경험', '협업', '커뮤니케이션', '머신러닝', '인사이트 도출', '데이터 추출', '데이터 시각화']\n",
      "49. ['데이터 전처리', '논리적 사고', '데이터 분석', '문제 정의 및 해결', '인사이트 도출', '데이터 추출']\n",
      "50. ['리포트 작성', '데이터 분석', '대시보드 구축', '데이터 처리', '코드 해석', '리딩 능력']\n",
      "51. ['데이터 분석', '협업', '캠페인 분석', '커뮤니케이션', '웹 분석']\n",
      "52. ['커뮤니케이션', '꼼꼼함', '영어 커뮤니케이션']\n",
      "53. ['데이터 분석', '리포트 작성', '엑셀', '문제 정의 및 해결', '커뮤니케이션', '파이썬']\n",
      "54. []\n",
      "55. []\n",
      "56. ['도메인 지식', '수리적 지식', '빅데이터 분석', '데이터 분석', '프로그래밍 언어', '열정적', '최신 기술 연구']\n",
      "57. ['책임감', '리포트 작성', '꼼꼼함', '영어 커뮤니케이션']\n",
      "58. ['도메인 지식', '가설 수립 및 검증', '리텐션 분석', '데이터 정제', '코호트 분석', '데이터 분석', '데이터 로그 설계', '퍼널 분석', '통계적 지식', '지표 정의', '프로덕트 데이터 분석', '데이터 가공', '모바일 데이터', '데이터 추출', '데이터 시각화', '테스트 설계 분석']\n",
      "59. ['비동기 데이터', '데이터 분석', '프로그래밍 언어', '실무경험', '분산 처리 시스템', '데이터 엔지니어링']\n",
      "60. ['데이터 분석', '구글스프레드시트', '실무경험', '리포팅 능력', '협업', '문제 정의 및 해결', '지표 정의', '비즈니스 인텔리전스', '대시보드 구축', '데이터 가공', '인사이트 도출', '데이터 추출', '파이썬']\n",
      "61. ['가설 수립 및 검증', '리텐션 분석', '코호트 분석', '데이터 분석', '데이터 로그 설계', '퍼널 분석', '인과추론', '지표 정의', '프로덕트 데이터 분석', '리딩 능력', '통계분석', '모바일 데이터', '테스트 설계 분석']\n",
      "62. ['가설 수립 및 검증', '데이터 분석', '리포트 작성', '문제 정의 및 해결', '쿼리', '데이터 가공', '커뮤니케이션', '구조적 사고', '데이터 추출']\n",
      "63. ['적응 능력', '성실함', '커뮤니케이션']\n",
      "64. ['엑셀', '데이터 분석', '파워포인트']\n",
      "65. ['논리적 사고', '데이터 마트 구축 운영', '데이터 분석', '실무경험', '인과추론', '문제 정의 및 해결', '인사이트 도출']\n",
      "66. ['논리적 사고', '오너십', '데이터 분석', '데이터 기반 의사결정', '데이터 추출']\n",
      "67. ['데이터 정제', '데이터 분석', '리포트 작성', '데이터 기반 의사결정', '통계적 지식', '문제 정의 및 해결', '커뮤니케이션', '통계분석', '데이터 추출']\n",
      "68. ['머신러닝', '블록체인', '협업', '대시보드 구축']\n",
      "69. ['데이터 전처리', '설득력', '데이터 분석', '협업', '대시보드 구축', '문제 정의 및 해결', '데이터 수집', '커뮤니케이션', '인사이트 도출', '데이터 모델링', '데이터 추출', '데이터 시각화']\n",
      "70. ['데이터 분석', '알고리즘 개발', '프로그래밍 언어', '데이터 로그 설계', '협업', '고객 분석', '커뮤니케이션', '데이터 모델링']\n",
      "71. ['도메인 지식', '알고리즘 개발', '리포트 작성', '논문 구현', '데이터 가공', '통계분석', '데이터 추출', '데이터 시각화']\n",
      "72. ['리포트 작성', '데이터 분석', '취약점 분석', '협업', '악성코드 분석', '커뮤니케이션']\n",
      "73. ['논리적 사고', '데이터 분석', '프로젝트 경험', '실무경험', '인과추론', '문제 정의 및 해결', '인사이트 도출', '주도적', '데이터 추출']\n",
      "74. ['데이터 분석', '모델 개발', '연구개발', '컨설팅', '코드 해석']\n",
      "75. ['모델 개발', '코드 해석', '프레임워크', '데이터 분석']\n",
      "76. []\n",
      "77. ['데이터 분석', '대시보드 구축', '데이터 추출', '데이터 시각화', '테스트 설계 분석']\n",
      "78. ['프레임워크', '데이터 분석', '데이터 솔루션', '프로젝트 경험', '품질관리', '고객 분석', '생산관리', '스마트팩토리', '공정관리', '설비관리', '시스템 개발']\n",
      "79. ['도메인 지식', '프레임워크', '컴퓨터 비전', '논문 구현', '최신 기술 연구', '딥러닝']\n",
      "80. ['리포트 작성', '데이터 분석', '취약점 분석', '협업', '악성코드 분석', '커뮤니케이션']\n",
      "81. []\n",
      "82. ['프레임워크', '빅데이터 분석', '백엔드', '데이터베이스', '데이터 모델링', '데이터 시각화']\n",
      "83. ['열정적', '프로그래밍 언어', '플랫폼 구축', '최신 기술 연구', '도전적', '클라우드 경험', '어플리케이션 개발', '데이터 파이프라인 구축']\n",
      "84. ['협업', '커뮤니케이션', '웹크롤링', '데이터 엔지니어링', '코드 해석']\n",
      "85. []\n",
      "86. ['데이터 분석', '프로젝트 경험', '데이터 기반 의사결정', '협업', '플랫폼 구축', '고객 분석', '커뮤니케이션', '모바일 데이터', '데이터 어플리케이션 서비스']\n",
      "87. ['리포트 작성', '데이터 솔루션', '적응 능력', '적극적', '데이터 모델링']\n",
      "88. ['도메인 지식', '고가용성', '내고장성', '오픈소스 경험', '소프트웨어', '시스템 개발']\n",
      "89. ['스키마', '데이터베이스', '튜닝 역량', '다이어그램']\n",
      "90. ['도메인 지식', '데이터베이스', '끈기', '최신 기술 연구']\n",
      "91. ['커뮤니케이션', '클라우드 경험', '데이터 파이프라인 구축']\n",
      "92. []\n",
      "93. ['이슈 해결', '플랫폼 구축', '데이터 분석']\n",
      "94. []\n",
      "95. ['도메인 지식', '문제 정의 및 해결', '플랫폼 구축', '데이터 모델링', '클라우드 경험']\n",
      "96. ['도메인 지식', '프로젝트 경험', '모델 개발', '업무 자동화', '개발경험', '협업', '포트폴리오', '적극적', '커뮤니케이션', '주도적', '딥러닝', '데이터 파이프라인 구축']\n",
      "97. []\n",
      "98. ['프로젝트 경험', '컴퓨터공학', '웹크롤링', '데이터 엔지니어링', '파이썬']\n",
      "99. ['도메인 지식', '알고리즘 개발', '데이터 수집', '최신 기술 연구', '분산 처리 시스템', '전산 지식', '데이터베이스', '커뮤니케이션', '클라우드 경험']\n",
      "100. ['데이터 전처리', '데이터 수집']\n",
      "101. ['데이터 분석', '거버넌스', '프로그래밍 언어', '플랫폼 구축', '기술통계']\n",
      "102. ['데이터 전처리', '산업공학', '데이터 마트 구축 운영', '데이터 분석', '데이터 핸들링', '모델 개발', '프로젝트 경험', '경영정보학', '통계적 지식', '경영학', '컴퓨터공학', '데이터 추출']\n",
      "103. ['데이터 전처리', '산업공학', '데이터 마트 구축 운영', '데이터 분석', '데이터 핸들링', '모델 개발', '프로젝트 경험', '경영정보학', '통계적 지식', '경영학', '컴퓨터공학', '데이터 추출']\n",
      "104. ['프로그래밍 언어', '데이터 스트리밍', '쿼리', '데이터베이스', '데이터 파이프라인 구축']\n",
      "105. ['백업 모니터링', '최적화', '장애처리']\n",
      "106. ['도메인 지식', '높은 코드 퀄리티', '알고리즘 개발', '플랫폼 구축', '전산 지식', '데이터베이스', '아키텍쳐', '인프라 구축 운영', '코드 해석']\n",
      "107. ['전산 지식', '인프라 구축 운영', '도메인 지식']\n",
      "108. ['도메인 지식', '리드 능력', '데이터 분석', '보안', '취약점 분석', '적극적', '커뮤니케이션', '정보보호', '스크립트 언어', '시스템 개발']\n",
      "109. ['데이터 전처리', '프로그래밍 언어', '실시간 데이터 처리', '배치 프로세싱', '데이터 수집', '데이터 파이프라인 구축']\n",
      "110. ['리눅스']\n",
      "111. ['도메인 지식', '산업공학', '분석적 사고', '빅데이터 분석', '금융공학', '물리학', '정량 데이터', '컴퓨터공학', '데이터 파이프라인 구축']\n",
      "112. ['리포팅 능력', '데이터베이스', '적극적', '커뮤니케이션', '장애처리']\n",
      "113. ['데이터 마트 구축 운영', '데이터 분석', '업무 자동화', '문제 정의 및 해결', '데이터 파이프라인 구축']\n",
      "114. ['컨테이너 경험', '가설 수립 및 검증', '데이터 분석', '데이터 시각화', '클라우드 경험', '시스템 개발']\n",
      "115. ['빅데이터', '튜닝 역량', '데이터 모델링', '클라우드 경험', '데이터 파이프라인 구축']\n",
      "116. ['트러블슈팅', '데이터 마트 구축 운영', '데이터 분석', '모델 개발', '서버 설계', '백엔드', '주도적', '데이터 시각화']\n",
      "117. ['분산 처리 시스템', '실시간 데이터 처리', '어플리케이션 개발', '시스템 개발']\n",
      "118. ['도메인 지식', '높은 코드 퀄리티', '알고리즘 개발', '플랫폼 구축', '전산 지식', '데이터베이스', '아키텍쳐', '인프라 구축 운영', '코드 해석']\n",
      "119. ['이슈 해결', '데이터 분석', '플랫폼 구축', '백엔드', '데이터 엔지니어링', '어플리케이션 개발']\n",
      "120. ['소프트웨어', '시스템 개발', '데이터 엔지니어링', '테스트 설계 분석']\n",
      "121. ['웹크롤링', '도메인 지식', '지표 정의']\n",
      "122. ['데이터 전처리', '데이터 수집']\n",
      "123. ['데이터 분석', '거버넌스', '프로그래밍 언어', '플랫폼 구축', '기술통계']\n",
      "124. ['컨테이너 경험', '데이터 솔루션', '데이터 엔지니어링', '협업', '문제 정의 및 해결', '데이터 오케스트레이션', '소프트웨어', '클라우드 경험', '데이터 파이프라인 구축']\n",
      "125. ['데이터 마트 구축 운영', '데이터 분석', '업무 자동화', '문제 정의 및 해결', '클라우드 경험', '데이터 파이프라인 구축']\n",
      "126. ['프로그래밍 언어', '플랫폼 구축', '백엔드', '워크플로우', '클라우드 경험', '시스템 개발']\n",
      "127. []\n",
      "128. ['스키마', '데이터베이스', '튜닝 역량', '다이어그램']\n",
      "129. []\n",
      "130. ['텍스트 데이터', '웹서비스', '네트워크 구축', '비동기처리']\n",
      "131. ['웹서비스']\n",
      "132. ['소프트웨어', '가설 수립 및 검증', '도메인 지식']\n",
      "133. ['커뮤니케이션', '긍정적']\n",
      "134. ['데이터 분석', '데이터 솔루션', '플랫폼 구축', '데이터 가공', '데이터 추출', '클라우드 경험']\n",
      "135. ['하둡', '업무 자동화', '데이터 수집', '데이터 시각화', '시스템 개발']\n",
      "136. ['창의성', '빅데이터 분석', '데이터 분석', '데이터 솔루션', '프로젝트 경험', '플랫폼 구축', '문제 정의 및 해결', '시스템 개발']\n",
      "137. ['하이퍼 캐주얼 게임']\n",
      "138. ['분산 처리 시스템', '데이터 프로세싱', '최적화', '백엔드']\n",
      "139. ['열정적', '프로그래밍 언어', '플랫폼 구축', '최신 기술 연구', '도전적', '클라우드 경험', '어플리케이션 개발', '데이터 파이프라인 구축']\n",
      "140. ['데이터 분석', '플랫폼 구축', '프로덕트 데이터 분석', '주도적', '인프라 구축 운영', '데이터 파이프라인 구축']\n",
      "141. ['최적화', '협업', '서버 설계', '커뮤니케이션', '데이터 모델링']\n",
      "142. ['데이터 전처리', '수리적 지식', '설득력', '데이터 분석', '모델 개발', '협업', '대시보드 구축', '문제 정의 및 해결', '데이터 수집', '데이터 가공', '커뮤니케이션', '머신러닝', '데이터 모델링', '데이터 추출', '데이터 시각화']\n",
      "143. ['도메인 지식', '호기심', '데이터 분석', '프로젝트 경험', '데이터 솔루션', '프로토타이핑', '플랫폼 구축', '커뮤니케이션', '데이터 사이언티스트', '클라우드 경험']\n",
      "144. ['데이터 분석', '쿼리', '백엔드', '데이터베이스', '커뮤니케이션', '클라우드 경험']\n",
      "145. ['도메인 지식', '호기심', '데이터 분석', '프로젝트 경험', '데이터 솔루션', '프로토타이핑', '플랫폼 구축', '커뮤니케이션', '데이터 사이언티스트', '클라우드 경험']\n",
      "146. ['소프트웨어', '프레젠테이션 기술', '커뮤니케이션', '프로젝트 경험']\n",
      "147. ['프로그래밍 언어', '데이터 파이프라인 구축']\n",
      "148. ['모델 개발 및 배포', '데이터 분석', '프로젝트 경험', '프로그래밍 언어', '머신러닝', '인사이트 도출', '주도적']\n",
      "149. ['프레임워크', '서버 설계']\n",
      "150. ['도메인 지식', '프레임워크', '프로그래밍 언어', '데이터베이스', '파이썬', '스크립트 언어']\n",
      "151. ['이미지 데이터', '영상 처리', '성실함', '데이터 레이블링', '책임감', '커뮤니케이션', '긍정적']\n",
      "152. ['스트림 프로세싱', '데이터 프로세싱', '데이터 파이프라인 구축']\n",
      "153. ['도전적', '리포트 작성']\n",
      "154. ['최적화', '데이터 파이프라인 구축']\n",
      "155. ['실무경험', '분산 처리 시스템', '데이터 파이프라인 구축', '온체인 데이터', '데이터 엔지니어링', '시스템 개발']\n",
      "156. ['프로그래밍 언어', '인프라 구축 운영', '플랫폼 구축', '빅데이터 분석']\n",
      "157. ['데이터 분석']\n",
      "158. ['산업공학', '통계적 지식']\n",
      "159. ['책임감', '빅데이터 분석']\n",
      "160. ['모델 개발 및 배포', '포트폴리오']\n",
      "161. ['데이터 전처리', '산업공학', '데이터 마트 구축 운영', '데이터 분석', '데이터 핸들링', '모델 개발', '프로젝트 경험', '경영정보학', '통계적 지식', '경영학', '컴퓨터공학', '데이터 추출']\n",
      "162. ['데이터 전처리', '산업공학', '데이터 마트 구축 운영', '데이터 분석', '데이터 핸들링', '모델 개발', '프로젝트 경험', '경영정보학', '통계적 지식', '경영학', '컴퓨터공학', '데이터 추출']\n",
      "163. ['프로젝트 경험', '컴퓨터공학', '웹크롤링', '데이터 엔지니어링', '파이썬']\n",
      "164. ['엑셀']\n",
      "165. ['리눅스']\n",
      "166. ['데이터 엔지니어링', '플랫폼 구축', '클라우드 경험']\n",
      "167. ['실시간 데이터 처리', '배치 프로세싱', '클라우드 경험', '데이터 레이크하우스']\n",
      "168. ['도메인 지식', '데이터베이스', '끈기', '최신 기술 연구']\n",
      "169. ['협업', '커뮤니케이션', '웹크롤링', '데이터 엔지니어링', '코드 해석']\n",
      "170. ['플랫폼 구축', '오픈소스 경험', '데이터베이스', '커뮤니케이션', '클라우드 경험']\n",
      "171. ['프로그래밍 언어', '데이터 스트리밍', '쿼리', '데이터베이스', '데이터 파이프라인 구축']\n",
      "172. ['협업', '문제 정의 및 해결', '데이터 수집', '데이터 엔지니어링', '데이터 파이프라인 구축']\n",
      "173. ['비판', '프레임워크', '프로그래밍 언어', '플랫폼 구축', '커뮤니케이션', '코드 해석']\n",
      "174. ['데이터 전처리', '정규화', '트랜잭션', '쿼리', '파이썬']\n",
      "175. ['도메인 지식', '데이터베이스', '프레임워크', '통계분석']\n",
      "176. ['빅데이터 분석', '프로그래밍 언어', '플랫폼 구축', '문제 정의 및 해결', '데이터 엔지니어링', '데이터 파이프라인 구축']\n",
      "177. ['도메인 지식', '데이터 분석', '통계적 지식', '분산 처리 시스템', '클라우드 경험', '시스템 개발']\n",
      "178. ['분산 처리 시스템', '시스템 개발']\n",
      "179. ['보안', '시스템 개발']\n"
     ]
    }
   ],
   "source": [
    "col = 'Requirement'\n",
    "(col_list2, col_list2_df, \n",
    " eng_list, kor_list, \n",
    " eng_list_spl, eng_lower, \n",
    " custom_nouns, replace, stopwords, \n",
    " results, kor_final, kor_final_df) = Korean_preprocessing(col)\n",
    "\n",
    "df.insert(4, 'Requirement_KOR', kor_final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Requirement_ENG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. python\n",
      "1. sql hql python r\n",
      "2. sql hql python r complex network modeling\n",
      "3. machine learnig python java sql r\n",
      "4. \n",
      "5. computer vision image video processing tensorflow pytorch or\n",
      "6. ml dl python sql ml dl\n",
      "7. data science ml dl nlp clustering ml dl sql python\n",
      "8. python sql r\n",
      "9. data science python r sql tool splunk tableau tool\n",
      "10. ai\n",
      "11. python data analysis process eda statistical analysis data mining report deployment query manipulation r\n",
      "12. python sql spark hadoop\n",
      "13. ai\n",
      "14. \n",
      "15. python r ml dl\n",
      "16. \n",
      "17. python\n",
      "18. python r ml dl\n",
      "19. bi r\n",
      "20. sql r python\n",
      "21. \n",
      "22. a b\n",
      "23. data analyst data scientist rdb sql python ml\n",
      "24. python sql tensorflow pytorch raw data sql\n",
      "25. ml keras tensorflow pytorch\n",
      "26. ml python sql tensorflow keras pytorch\n",
      "27. sql python pm\n",
      "28. python opencv numpy tensorflow pytorch cnn transformer python opencv numpy pytorch or tensorflow\n",
      "29. remote sensing signal processing earth science sar python matlab\n",
      "30. python opencv numpy tensorflow pytorch cnn transformer python opencv numpy pytorch or tensorflow\n",
      "31. \n",
      "32. sql\n",
      "33. python prediction anomaly detection classification tensorflow pytorch pandas matplotlib tensorflow pytorch\n",
      "34. sql python saas gmv mrr ltv cohort retention action item\n",
      "35. front end redwoodjs storybook graphql tailwindcss app flutter fastlane back end redwoodjs serverless aws lambda postgresql amazon rds python fastapi\n",
      "36. \n",
      "37. sql numpy pandas spark powerbi tableau\n",
      "38. \n",
      "39. css python sas\n",
      "40. sql scripting data handling ms office\n",
      "41. python sql framework pandas scikit learn tensorflow pytorch python tableau ml process data collection eda preprocessing feature engineering training evaluation serving\n",
      "42. python prediction anomaly detection classification tensorflow pytorch pandas matplotlib tensorflow pytorch\n",
      "43. gcp professional machine learning engineer gcp ml ai text image sound\n",
      "44. isp ismp pi dx migration ppt\n",
      "45. sql r python\n",
      "46. essential experiences skillsets personal competencies undertaken years experience in a similar role or data analysis role understanding on the multi cultural multi lingual and diverse environment advanced ms excel and sql skills strong analytical skills a keen eye for details and has a systematic approach in dealing with issues desirable personal competencies and attributes business communication skills in both english and korean ability to be hands on particularly with data collation and preparation\n",
      "47. biz insight crm sql mysql mssql python r sql pandas tensorflow pytorch spark mysql mariadb nosql mongodb pm pl\n",
      "48. sql raw data tableau amplitude\n",
      "49. sql python r\n",
      "50. sql tableau google data studio google sheets ms excel python r\n",
      "51. reporting ppt excel qa digital marketing or or sales\n",
      "52. aa or ga\n",
      "53. sql tableau ga\n",
      "54. \n",
      "55. bi r\n",
      "56. biomedical python r\n",
      "57. excel\n",
      "58. sql excel python r a b ga amplitude\n",
      "59. batch aws architecture\n",
      "60. sql excel\n",
      "61. sql excel python r a b ga amplitude\n",
      "62. da bi sql python cohort funnel a b test ppt documentation\n",
      "63. crc data handling ms office\n",
      "64. ms office\n",
      "65. excel sql b\n",
      "66. sql data driven business solution\n",
      "67. sas r python sql\n",
      "68. layer or layer block transaction business intelligence sql https towardsdatascience com your guide to basic sql while learning ethereum at the same time eac a etherscan\n",
      "69. sql nosql python r\n",
      "70. python ml dl legacy sql\n",
      "71. t o sql python r tableau powerbi bi ml\n",
      "72. \n",
      "73. sql excel\n",
      "74. ai mlops ai ml big data ai ml ai ml structure\n",
      "75. ai ml big data python ml pytorch tensorflow ai ai structure\n",
      "76. c c\n",
      "77. sql bi google data studio quicksight metabase a b\n",
      "78. pl pl it\n",
      "79. tensorflow pytorch mxnet\n",
      "80. \n",
      "81. bachelor s degree required majored in economics statistics risk management preferred strong business acumen and communications skills needed to interface with both executive management and our clients including internal business partnering should over years experiences in work years of experience within leading the projective work data analytics quality management risk management general insurance risk management telecommunication preferred advanced knowledge of microsoft office suite applications including excel word and powerpoint power bi and other reporting tools a plus experience working in a multi national multi cultural multi lingual and diverse environment experience applying problem solving skills to complex systems financial operational and associated integrated processes strong time management and organization skills\n",
      "82. go python ml deep learning framework scikit learn tensorflow keras sql hadoop mr hive spark flink presto tableau bi tool\n",
      "83. python etl\n",
      "84. python node js\n",
      "85. postgresql oracle dbms cloud\n",
      "86. pm\n",
      "87. db erwin db db erwin\n",
      "88. linux unix database docker container github com id\n",
      "89. postgresql oracle mysql mariadb db sql stored procedure er\n",
      "90. python airflow\n",
      "91. python aws gcp azure\n",
      "92. \n",
      "93. aws db sql aws s airflow glue emr kinesis redshift mysql elasticsearch mongodb spark kafka python java shell script\n",
      "94. dbms dba dbms postgresql oracle tibero altibase edb experdb mysql db\n",
      "95. aws gcp etl elt\n",
      "96. computer vision image video processing airflow kubeflow kubernetes docker container or\n",
      "97. \n",
      "98. \n",
      "99. os python hadoop mr hive spark db third party log streaming sql aws\n",
      "100. sql python\n",
      "101. sql python python scala sql apache spark delta lake airflow kafka fastapi git argocd grafana\n",
      "102. r sas python sap data analysis process eda statistical analysis data mining report deployment query manipulation\n",
      "103. r sas python sap data analysis process eda statistical analysis data mining report deployment query manipulation\n",
      "104. dw data lake elt\n",
      "105. oracle database db schema db oracle db rac db migration\n",
      "106. aws gcp cloud native airflow workflow orchestrator prefect oozie workflow cs os\n",
      "107. java scala python hadoop ecosystem\n",
      "108. ai ml data science shell python\n",
      "109. \n",
      "110. \n",
      "111. a k s\n",
      "112. \n",
      "113. etl sql python airflow workflow tool\n",
      "114. aws airflow\n",
      "115. dw dm sql java back end java spring python\n",
      "116. java kotlin scala python sql python airflow\n",
      "117. kotlin java python go scala hadoop kafka spark msa\n",
      "118. aws gcp cloud native airflow workflow orchestrator prefect oozie workflow cs os\n",
      "119. or dba python java aws db sql\n",
      "120. jira qa\n",
      "121. python sql etl aws git\n",
      "122. sql python\n",
      "123. sql python\n",
      "124. ml airflow prefect dagster etl elt aws azure gcp snowflake databricks python sql git github docker docker compose\n",
      "125. etl sql python airflow workflow tool\n",
      "126. python scala java go airflow\n",
      "127. \n",
      "128. postgresql oracle mysql mariadb db sql stored procedure er\n",
      "129. c c network\n",
      "130. \n",
      "131. rds nodejs\n",
      "132. ui\n",
      "133. \n",
      "134. sql raw data aws etl\n",
      "135. python java scala javascript spark hadoop hive sql\n",
      "136. spark airflow jenkins sql python\n",
      "137. python bigquery mysql dba\n",
      "138. ci cd python pandas sql\n",
      "139. python etl\n",
      "140. o o\n",
      "141. python node js sql linux\n",
      "142. sql nosql python r ai ai ai ai d hw\n",
      "143. etl data pipeline python sql\n",
      "144. serverless sql\n",
      "145. etl data pipeline data lake python sql\n",
      "146. ci cd\n",
      "147. spark hive presto elastic stack python scala java sql\n",
      "148. sql python kaggle api\n",
      "149. e g flask django cloud hosting service e g aws\n",
      "150. rdb or nosql python e g flask django\n",
      "151. python format\n",
      "152. kafka spark hadoop eco system python sql\n",
      "153. linux language framework platform etc\n",
      "154. data flow architecture etl sql query index stored procedure in sql server pm\n",
      "155. \n",
      "156. python\n",
      "157. python sql\n",
      "158. \n",
      "159. java python hadoop eco system\n",
      "160. back front db deploy github bitbucket repository\n",
      "161. r sas python sap data analysis process eda statistical analysis data mining report deployment query manipulation\n",
      "162. r sas python sap data analysis process eda statistical analysis data mining report deployment query manipulation\n",
      "163. \n",
      "164. \n",
      "165. \n",
      "166. dw bi jvm java scala python gcp aws\n",
      "167. \n",
      "168. python airflow\n",
      "169. python node js\n",
      "170. rdb mysql nosql mongodb kafka python java scala datawarehouse datalake\n",
      "171. dw data lake elt\n",
      "172. python java scala sql\n",
      "173. \n",
      "174. db db index nosql jupyter notebook pandas numpy scipy\n",
      "175. sql data pipeline airflow workflow tool django fastapi python\n",
      "176. kafka spark hadoop python java scala\n",
      "177. hadoop spark aws gcp azure sql python jvm\n",
      "178. java python back end sql hdfs nosql\n",
      "179. web was dbms container firewall waf vpn utm\n"
     ]
    }
   ],
   "source": [
    "for idx, word in enumerate(eng_lower):\n",
    "    print(f'{idx}. {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46, 68, 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'sql', 'hql', 'python', 'r', 'sql', 'hql', 'python', 'r', 'complex', 'network', 'modeling', 'machine', 'learnig', 'python', 'java', 'sql', 'r', 'computer', 'vision', 'image', 'video', 'processing', 'tensorflow', 'pytorch', 'or', 'ml', 'dl', 'python', 'sql', 'ml', 'dl', 'data', 'science', 'ml', 'dl', 'nlp', 'clustering', 'ml', 'dl', 'sql', 'python', 'python', 'sql', 'r', 'data', 'science', 'python', 'r', 'sql', 'tool', 'splunk', 'tableau', 'tool', 'ai', 'python', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation', 'r', 'python', 'sql', 'spark', 'hadoop', 'ai', 'python', 'r', 'ml', 'dl', 'python', 'python', 'r', 'ml', 'dl', 'bi', 'r', 'sql', 'r', 'python', 'a', 'b', 'data', 'analyst', 'data', 'scientist', 'rdb', 'sql', 'python', 'ml', 'python', 'sql', 'tensorflow', 'pytorch', 'raw', 'data', 'sql', 'ml', 'keras', 'tensorflow', 'pytorch', 'ml', 'python', 'sql', 'tensorflow', 'keras', 'pytorch', 'sql', 'python', 'pm', 'python', 'opencv', 'numpy', 'tensorflow', 'pytorch', 'cnn', 'transformer', 'python', 'opencv', 'numpy', 'pytorch', 'or', 'tensorflow', 'remote', 'sensing', 'signal', 'processing', 'earth', 'science', 'sar', 'python', 'matlab', 'python', 'opencv', 'numpy', 'tensorflow', 'pytorch', 'cnn', 'transformer', 'python', 'opencv', 'numpy', 'pytorch', 'or', 'tensorflow', 'sql', 'python', 'prediction', 'anomaly', 'detection', 'classification', 'tensorflow', 'pytorch', 'pandas', 'matplotlib', 'tensorflow', 'pytorch', 'sql', 'python', 'saas', 'gmv', 'mrr', 'ltv', 'cohort', 'retention', 'action', 'item', 'front', 'end', 'redwoodjs', 'storybook', 'graphql', 'tailwindcss', 'app', 'flutter', 'fastlane', 'back', 'end', 'redwoodjs', 'serverless', 'aws', 'lambda', 'postgresql', 'amazon', 'rds', 'python', 'fastapi', 'sql', 'numpy', 'pandas', 'spark', 'powerbi', 'tableau', 'css', 'python', 'sas', 'sql', 'scripting', 'data', 'handling', 'ms', 'office', 'python', 'sql', 'framework', 'pandas', 'scikit', 'learn', 'tensorflow', 'pytorch', 'python', 'tableau', 'ml', 'process', 'data', 'collection', 'eda', 'preprocessing', 'feature', 'engineering', 'training', 'evaluation', 'serving', 'python', 'prediction', 'anomaly', 'detection', 'classification', 'tensorflow', 'pytorch', 'pandas', 'matplotlib', 'tensorflow', 'pytorch', 'gcp', 'professional', 'machine', 'learning', 'engineer', 'gcp', 'ml', 'ai', 'text', 'image', 'sound', 'isp', 'ismp', 'pi', 'dx', 'migration', 'ppt', 'sql', 'r', 'python', 'essential', 'experiences', 'skillsets', 'personal', 'competencies', 'undertaken', 'years', 'experience', 'in', 'a', 'similar', 'role', 'or', 'data', 'analysis', 'role', 'understanding', 'on', 'the', 'multi', 'cultural', 'multi', 'lingual', 'and', 'diverse', 'environment', 'advanced', 'ms', 'excel', 'and', 'sql', 'skills', 'strong', 'analytical', 'skills', 'a', 'keen', 'eye', 'for', 'details', 'and', 'has', 'a', 'systematic', 'approach', 'in', 'dealing', 'with', 'issues', 'desirable', 'personal', 'competencies', 'and', 'attributes', 'business', 'communication', 'skills', 'in', 'both', 'english', 'and', 'korean', 'ability', 'to', 'be', 'hands', 'on', 'particularly', 'with', 'data', 'collation', 'and', 'preparation', 'biz', 'insight', 'crm', 'sql', 'mysql', 'mssql', 'python', 'r', 'sql', 'pandas', 'tensorflow', 'pytorch', 'spark', 'mysql', 'mariadb', 'nosql', 'mongodb', 'pm', 'pl', 'sql', 'raw', 'data', 'tableau', 'amplitude', 'sql', 'python', 'r', 'sql', 'tableau', 'google', 'data', 'studio', 'google', 'sheets', 'ms', 'excel', 'python', 'r', 'reporting', 'ppt', 'excel', 'qa', 'digital', 'marketing', 'or', 'or', 'sales', 'aa', 'or', 'ga', 'sql', 'tableau', 'ga', 'bi', 'r', 'biomedical', 'python', 'r', 'excel', 'sql', 'excel', 'python', 'r', 'a', 'b', 'ga', 'amplitude', 'batch', 'aws', 'architecture', 'sql', 'excel', 'sql', 'excel', 'python', 'r', 'a', 'b', 'ga', 'amplitude', 'da', 'bi', 'sql', 'python', 'cohort', 'funnel', 'a', 'b', 'test', 'ppt', 'documentation', 'crc', 'data', 'handling', 'ms', 'office', 'ms', 'office', 'excel', 'sql', 'b', 'sql', 'data', 'driven', 'business', 'solution', 'sas', 'r', 'python', 'sql', 'layer', 'or', 'layer', 'block', 'transaction', 'business', 'intelligence', 'sql', 'https', 'towardsdatascience', 'com', 'your', 'guide', 'to', 'basic', 'sql', 'while', 'learning', 'ethereum', 'at', 'the', 'same', 'time', 'eac', 'a', 'etherscan', 'sql', 'nosql', 'python', 'r', 'python', 'ml', 'dl', 'legacy', 'sql', 't', 'o', 'sql', 'python', 'r', 'tableau', 'powerbi', 'bi', 'ml', 'sql', 'excel', 'ai', 'mlops', 'ai', 'ml', 'big', 'data', 'ai', 'ml', 'ai', 'ml', 'structure', 'ai', 'ml', 'big', 'data', 'python', 'ml', 'pytorch', 'tensorflow', 'ai', 'ai', 'structure', 'c', 'c', 'sql', 'bi', 'google', 'data', 'studio', 'quicksight', 'metabase', 'a', 'b', 'pl', 'pl', 'it', 'tensorflow', 'pytorch', 'mxnet', 'bachelor', 's', 'degree', 'required', 'majored', 'in', 'economics', 'statistics', 'risk', 'management', 'preferred', 'strong', 'business', 'acumen', 'and', 'communications', 'skills', 'needed', 'to', 'interface', 'with', 'both', 'executive', 'management', 'and', 'our', 'clients', 'including', 'internal', 'business', 'partnering', 'should', 'over', 'years', 'experiences', 'in', 'work', 'years', 'of', 'experience', 'within', 'leading', 'the', 'projective', 'work', 'data', 'analytics', 'quality', 'management', 'risk', 'management', 'general', 'insurance', 'risk', 'management', 'telecommunication', 'preferred', 'advanced', 'knowledge', 'of', 'microsoft', 'office', 'suite', 'applications', 'including', 'excel', 'word', 'and', 'powerpoint', 'power', 'bi', 'and', 'other', 'reporting', 'tools', 'a', 'plus', 'experience', 'working', 'in', 'a', 'multi', 'national', 'multi', 'cultural', 'multi', 'lingual', 'and', 'diverse', 'environment', 'experience', 'applying', 'problem', 'solving', 'skills', 'to', 'complex', 'systems', 'financial', 'operational', 'and', 'associated', 'integrated', 'processes', 'strong', 'time', 'management', 'and', 'organization', 'skills', 'go', 'python', 'ml', 'deep', 'learning', 'framework', 'scikit', 'learn', 'tensorflow', 'keras', 'sql', 'hadoop', 'mr', 'hive', 'spark', 'flink', 'presto', 'tableau', 'bi', 'tool', 'python', 'etl', 'python', 'node', 'js', 'postgresql', 'oracle', 'dbms', 'cloud', 'pm', 'db', 'erwin', 'db', 'db', 'erwin', 'linux', 'unix', 'database', 'docker', 'container', 'github', 'com', 'id', 'postgresql', 'oracle', 'mysql', 'mariadb', 'db', 'sql', 'stored', 'procedure', 'er', 'python', 'airflow', 'python', 'aws', 'gcp', 'azure', 'aws', 'db', 'sql', 'aws', 's', 'airflow', 'glue', 'emr', 'kinesis', 'redshift', 'mysql', 'elasticsearch', 'mongodb', 'spark', 'kafka', 'python', 'java', 'shell', 'script', 'dbms', 'dba', 'dbms', 'postgresql', 'oracle', 'tibero', 'altibase', 'edb', 'experdb', 'mysql', 'db', 'aws', 'gcp', 'etl', 'elt', 'computer', 'vision', 'image', 'video', 'processing', 'airflow', 'kubeflow', 'kubernetes', 'docker', 'container', 'or', 'os', 'python', 'hadoop', 'mr', 'hive', 'spark', 'db', 'third', 'party', 'log', 'streaming', 'sql', 'aws', 'sql', 'python', 'sql', 'python', 'python', 'scala', 'sql', 'apache', 'spark', 'delta', 'lake', 'airflow', 'kafka', 'fastapi', 'git', 'argocd', 'grafana', 'r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation', 'r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation', 'dw', 'data', 'lake', 'elt', 'oracle', 'database', 'db', 'schema', 'db', 'oracle', 'db', 'rac', 'db', 'migration', 'aws', 'gcp', 'cloud', 'native', 'airflow', 'workflow', 'orchestrator', 'prefect', 'oozie', 'workflow', 'cs', 'os', 'java', 'scala', 'python', 'hadoop', 'ecosystem', 'ai', 'ml', 'data', 'science', 'shell', 'python', 'a', 'k', 's', 'etl', 'sql', 'python', 'airflow', 'workflow', 'tool', 'aws', 'airflow', 'dw', 'dm', 'sql', 'java', 'back', 'end', 'java', 'spring', 'python', 'java', 'kotlin', 'scala', 'python', 'sql', 'python', 'airflow', 'kotlin', 'java', 'python', 'go', 'scala', 'hadoop', 'kafka', 'spark', 'msa', 'aws', 'gcp', 'cloud', 'native', 'airflow', 'workflow', 'orchestrator', 'prefect', 'oozie', 'workflow', 'cs', 'os', 'or', 'dba', 'python', 'java', 'aws', 'db', 'sql', 'jira', 'qa', 'python', 'sql', 'etl', 'aws', 'git', 'sql', 'python', 'sql', 'python', 'ml', 'airflow', 'prefect', 'dagster', 'etl', 'elt', 'aws', 'azure', 'gcp', 'snowflake', 'databricks', 'python', 'sql', 'git', 'github', 'docker', 'docker', 'compose', 'etl', 'sql', 'python', 'airflow', 'workflow', 'tool', 'python', 'scala', 'java', 'go', 'airflow', 'postgresql', 'oracle', 'mysql', 'mariadb', 'db', 'sql', 'stored', 'procedure', 'er', 'c', 'c', 'network', 'rds', 'nodejs', 'ui', 'sql', 'raw', 'data', 'aws', 'etl', 'python', 'java', 'scala', 'javascript', 'spark', 'hadoop', 'hive', 'sql', 'spark', 'airflow', 'jenkins', 'sql', 'python', 'python', 'bigquery', 'mysql', 'dba', 'ci', 'cd', 'python', 'pandas', 'sql', 'python', 'etl', 'o', 'o', 'python', 'node', 'js', 'sql', 'linux', 'sql', 'nosql', 'python', 'r', 'ai', 'ai', 'ai', 'ai', 'd', 'hw', 'etl', 'data', 'pipeline', 'python', 'sql', 'serverless', 'sql', 'etl', 'data', 'pipeline', 'data', 'lake', 'python', 'sql', 'ci', 'cd', 'spark', 'hive', 'presto', 'elastic', 'stack', 'python', 'scala', 'java', 'sql', 'sql', 'python', 'kaggle', 'api', 'e', 'g', 'flask', 'django', 'cloud', 'hosting', 'service', 'e', 'g', 'aws', 'rdb', 'or', 'nosql', 'python', 'e', 'g', 'flask', 'django', 'python', 'format', 'kafka', 'spark', 'hadoop', 'eco', 'system', 'python', 'sql', 'linux', 'language', 'framework', 'platform', 'etc', 'data', 'flow', 'architecture', 'etl', 'sql', 'query', 'index', 'stored', 'procedure', 'in', 'sql', 'server', 'pm', 'python', 'python', 'sql', 'java', 'python', 'hadoop', 'eco', 'system', 'back', 'front', 'db', 'deploy', 'github', 'bitbucket', 'repository', 'r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation', 'r', 'sas', 'python', 'sap', 'data', 'analysis', 'process', 'eda', 'statistical', 'analysis', 'data', 'mining', 'report', 'deployment', 'query', 'manipulation', 'dw', 'bi', 'jvm', 'java', 'scala', 'python', 'gcp', 'aws', 'python', 'airflow', 'python', 'node', 'js', 'rdb', 'mysql', 'nosql', 'mongodb', 'kafka', 'python', 'java', 'scala', 'datawarehouse', 'datalake', 'dw', 'data', 'lake', 'elt', 'python', 'java', 'scala', 'sql', 'db', 'db', 'index', 'nosql', 'jupyter', 'notebook', 'pandas', 'numpy', 'scipy', 'sql', 'data', 'pipeline', 'airflow', 'workflow', 'tool', 'django', 'fastapi', 'python', 'kafka', 'spark', 'hadoop', 'python', 'java', 'scala', 'hadoop', 'spark', 'aws', 'gcp', 'azure', 'sql', 'python', 'jvm', 'java', 'python', 'back', 'end', 'sql', 'hdfs', 'nosql', 'web', 'was', 'dbms', 'container', 'firewall', 'waf', 'vpn', 'utm']\n",
      "['serverless', 'network', 'systems', 'studio', 'presto', 'role', 'argocd', 'problem', 'eda', 'query', 'tensorflow', 'datawarehouse', 'pytorch', 'systematic', 'oozie', 'scala', 'flow', 'procedure', 'hadoop', 'including', 'shell', 'kubeflow', 'cs', 'risk', 'transformer', 'storybook', 'eac', 'engineer', 'bitbucket', 'learnig', 'prefect', 'transaction', 'management', 'dealing', 'architecture', 'marketing', 'd', 'associated', 'grafana', 'understanding', 'attributes', 'postgresql', 'da', 'diverse', 'aa', 'nosql', 'elastic', 'hql', 'national', 'third', 'lake', 'kinesis', 'eco', 'emr', 'mongodb', 'nlp', 'block', 'insight', 'data', 'dbms', 'insurance', 'economics', 'kafka', 'analytical', 'jira', 'system', 'internal', 'sheets', 'within', 'feature', 'lingual', 'approach', 'on', 'clustering', 'ui', 'pandas', 'plus', 'back', 'altibase', 'api', 'sas', 'airflow', 'isp', 'utm', 'processes', 'issues', 'e', 'acumen', 'experiences', 'crm', 'deploy', 'sap', 'legacy', 'qa', 's', 'native', 'elasticsearch', 'sound', 'evaluation', 'of', 'lambda', 'dw', 'structure', 'over', 'processing', 'deployment', 'vision', 'integrated', 'retention', 'and', 'majored', 'log', 'python', 'has', 'quality', 'etl', 'opencv', 'statistics', 'sql', 'needed', 'framework', 'serving', 'ai', 'analyst', 'hive', 'b', 'microsoft', 'fastapi', 'edb', 'mariadb', 'or', 'tibero', 'java', 'a', 'desirable', 'driven', 'quicksight', 'with', 'particularly', 'compose', 'firewall', 'vpn', 'hands', 'er', 'text', 'ecosystem', 'applications', 'advanced', 'scientist', 'details', 'stack', 'guide', 'docker', 'migration', 'jvm', 'reporting', 'power', 'operational', 'c', 'dagster', 'script', 'remote', 'towardsdatascience', 'personal', 'snowflake', 'jupyter', 'g', 'analysis', 'end', 'professional', 'similar', 'item', 'communication', 'cohort', 'cultural', 'flask', 'dl', 'css', 'tailwindcss', 'gmv', 'statistical', 'numpy', 'workflow', 'hosting', 'matlab', 'time', 'graphql', 'redwoodjs', 'ci', 'digital', 'at', 'dm', 'eye', 'multi', 'both', 'interface', 'unix', 'node', 'complex', 'bachelor', 'cnn', 'oracle', 'javascript', 'mxnet', 'hdfs', 'gcp', 'orchestrator', 'degree', 'com', 'mssql', 'training', 'tableau', 'sales', 'layer', 'preferred', 'index', 't', 'collection', 'intelligence', 'spring', 'ismp', 'same', 'kaggle', 'datalake', 'amplitude', 'office', 'powerbi', 'schema', 'collation', 'applying', 'cd', 'documentation', 'suite', 'kotlin', 'etc', 'https', 'ethereum', 'classification', 'google', 'stored', 'work', 'tools', 'pm', 'splunk', 'biz', 'ms', 'scikit', 'korean', 'analytics', 'db', 'sensing', 'financial', 'git', 'delta', 'image', 'web', 'to', 'elt', 'clients', 'server', 'repository', 'strong', 'experdb', 'your', 'it', 'raw', 'other', 'general', 'r', 'matplotlib', 'scipy', 'english', 'while', 'metabase', 'be', 'os', 'should', 'django', 'keras', 'ability', 'cloud', 'nodejs', 'deep', 'word', 'sar', 'database', 'front', 'mysql', 'notebook', 'powerpoint', 'azure', 'skills', 'knowledge', 'language', 'amazon', 'anomaly', 'handling', 'crc', 'batch', 'dx', 'apache', 'pipeline', 'for', 'executive', 'jenkins', 'scripting', 'action', 'video', 'ppt', 'redshift', 'leading', 'erwin', 'computer', 'biomedical', 'required', 'solving', 'test', 'organization', 'msa', 'essential', 'format', 'waf', 'working', 'preprocessing', 'competencies', 'machine', 'funnel', 'modeling', 'learning', 'service', 'ml', 'ltv', 'mlops', 'ga', 'bi', 'telecommunication', 'js', 'dba', 'our', 'databricks', 'glue', 'in', 'streaming', 'id', 'manipulation', 'science', 'big', 'spark', 'flutter', 'signal', 'mrr', 'pi', 'excel', 'go', 'k', 'flink', 'business', 'rds', 'tool', 'mining', 'app', 'partnering', 'solution', 'github', 'fastlane', 'mr', 'rac', 'projective', 'platform', 'undertaken', 'party', 'preparation', 'detection', 'engineering', 'hw', 'skillsets', 'process', 'o', 'bigquery', 'report', 'rdb', 'pl', 'learn', 'was', 'communications', 'saas', 'environment', 'years', 'keen', 'prediction', 'the', 'earth', 'linux', 'container', 'etherscan', 'aws', 'basic', 'experience', 'kubernetes']\n"
     ]
    }
   ],
   "source": [
    "eng_lower_list = []\n",
    "for data in eng_lower:\n",
    "    eng_lower_list.extend(data.split())\n",
    "print(eng_lower_list)\n",
    "\n",
    "eng_lower_remove = list(set(eng_lower_list))\n",
    "print(eng_lower_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('custom_nouns_requirement_eng.txt', 'w') as file:\n",
    "    for data in eng_lower_remove:\n",
    "        words = data.split()\n",
    "        for word in words:\n",
    "            file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LG\\anaconda3\\envs\\crawling\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. []\n",
      "1. []\n",
      "2. []\n",
      "3. []\n",
      "4. []\n",
      "5. []\n",
      "6. []\n",
      "7. []\n",
      "8. []\n",
      "9. []\n",
      "10. []\n",
      "11. []\n",
      "12. []\n",
      "13. []\n",
      "14. []\n",
      "15. []\n",
      "16. []\n",
      "17. []\n",
      "18. []\n",
      "19. []\n",
      "20. []\n",
      "21. []\n",
      "22. []\n",
      "23. []\n",
      "24. []\n",
      "25. []\n",
      "26. []\n",
      "27. []\n",
      "28. []\n",
      "29. []\n",
      "30. []\n",
      "31. []\n",
      "32. []\n",
      "33. []\n",
      "34. []\n",
      "35. []\n",
      "36. []\n",
      "37. []\n",
      "38. []\n",
      "39. []\n",
      "40. []\n",
      "41. []\n",
      "42. []\n",
      "43. []\n",
      "44. []\n",
      "45. []\n",
      "46. []\n",
      "47. []\n",
      "48. []\n",
      "49. []\n",
      "50. []\n",
      "51. []\n",
      "52. []\n",
      "53. []\n",
      "54. []\n",
      "55. []\n",
      "56. []\n",
      "57. []\n",
      "58. []\n",
      "59. []\n",
      "60. []\n",
      "61. []\n",
      "62. []\n",
      "63. []\n",
      "64. []\n",
      "65. []\n",
      "66. []\n",
      "67. []\n",
      "68. []\n",
      "69. []\n",
      "70. []\n",
      "71. []\n",
      "72. []\n",
      "73. []\n",
      "74. []\n",
      "75. []\n",
      "76. []\n",
      "77. []\n",
      "78. []\n",
      "79. []\n",
      "80. []\n",
      "81. []\n",
      "82. []\n",
      "83. []\n",
      "84. []\n",
      "85. []\n",
      "86. []\n",
      "87. []\n",
      "88. []\n",
      "89. []\n",
      "90. []\n",
      "91. []\n",
      "92. []\n",
      "93. []\n",
      "94. []\n",
      "95. []\n",
      "96. []\n",
      "97. []\n",
      "98. []\n",
      "99. []\n",
      "100. []\n",
      "101. []\n",
      "102. []\n",
      "103. []\n",
      "104. []\n",
      "105. []\n",
      "106. []\n",
      "107. []\n",
      "108. []\n",
      "109. []\n",
      "110. []\n",
      "111. []\n",
      "112. []\n",
      "113. []\n",
      "114. []\n",
      "115. []\n",
      "116. []\n",
      "117. []\n",
      "118. []\n",
      "119. []\n",
      "120. []\n",
      "121. []\n",
      "122. []\n",
      "123. []\n",
      "124. []\n",
      "125. []\n",
      "126. []\n",
      "127. []\n",
      "128. []\n",
      "129. []\n",
      "130. []\n",
      "131. []\n",
      "132. []\n",
      "133. []\n",
      "134. []\n",
      "135. []\n",
      "136. []\n",
      "137. []\n",
      "138. []\n",
      "139. []\n",
      "140. []\n",
      "141. []\n",
      "142. []\n",
      "143. []\n",
      "144. []\n",
      "145. []\n",
      "146. []\n",
      "147. []\n",
      "148. []\n",
      "149. []\n",
      "150. []\n",
      "151. []\n",
      "152. []\n",
      "153. []\n",
      "154. []\n",
      "155. []\n",
      "156. []\n",
      "157. []\n",
      "158. []\n",
      "159. []\n",
      "160. []\n",
      "161. []\n",
      "162. []\n",
      "163. []\n",
      "164. []\n",
      "165. []\n",
      "166. []\n",
      "167. []\n",
      "168. []\n",
      "169. []\n",
      "170. []\n",
      "171. []\n",
      "172. []\n",
      "173. []\n",
      "174. []\n",
      "175. []\n",
      "176. []\n",
      "177. []\n",
      "178. []\n",
      "179. []\n"
     ]
    }
   ],
   "source": [
    "text_path = './custom_nouns_stacktool.txt'\n",
    "custom_nouns = text_list_load(text_path)\n",
    "\n",
    "text_path = './stopwords_stacktool_eng.txt'\n",
    "stopwords = text_list_load(text_path)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "custom_nouns = custom_nouns\n",
    "for noun in custom_nouns:\n",
    "    okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger=okt,\n",
    "    passtags={'Noun'},\n",
    "    # replace=replace,\n",
    "    stopwords=stopwords,\n",
    "    # passwords=passwords,\n",
    "    # ngrams=ngrams\n",
    ")\n",
    "\n",
    "results = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    elem = eng_lower[idx]\n",
    "    # print(postprocessor.pos(elem))\n",
    "    post_onechar = postprocessor.pos(elem)\n",
    "    result = []\n",
    "    for word, tag in post_onechar:\n",
    "        result.append(word)\n",
    "    results.append(result)\n",
    "    # print(f'{idx}. {result}')\n",
    "\n",
    "eng_final = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    a = set(results[idx])\n",
    "    b = list(a)\n",
    "    eng_final.append(b)\n",
    "    print(f'{idx}. {b}')\n",
    "# print(len(kor_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preference_KOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Preference'\n",
    "(col_list2, col_list2_df, \n",
    " eng_list, kor_list, \n",
    " eng_list_spl, eng_lower, \n",
    " custom_nouns, replace, stopwords, \n",
    " results, kor_final, kor_final_df) = Korean_preprocessing(col)\n",
    "\n",
    "df.insert(6, 'Preference_KOR', kor_final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preference_ENG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "def wordcloud_show(list):\n",
    "    join_test1 = ', '.join(list)\n",
    "    test1_list = [join_test1]\n",
    "    # print(test1_list)\n",
    "\n",
    "    font_path = '/Users/LG/AppData/Local/Microsoft/Windows/Fonts/BMJUA_ttf.ttf'\n",
    "    test1_wc = ', '.join(test1_list)\n",
    "    test1_wc = test1_wc.replace(' ', '').replace(',', ', ')\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=2500, \n",
    "        height=2500, \n",
    "        background_color='white',\n",
    "        # colormap='seismic',\n",
    "        font_path=font_path\n",
    "    ).generate(test1_wc)\n",
    "    plt.figure(figsize=(6, 6), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "# DS\n",
    "# Requirement\n",
    "print('DS: Requirement')\n",
    "test1 = df.loc[df.label == '데이터 사이언티스트'].iloc[:, 4].tolist()\n",
    "wordcloud_show(test1)\n",
    "# Preference\n",
    "print('DS: Preference')\n",
    "test1 = df.loc[df.label == '데이터 사이언티스트'].iloc[:, 6].tolist()\n",
    "wordcloud_show(test1)\n",
    "\n",
    "# DA\n",
    "# Requirement\n",
    "print('DA: Requirement')\n",
    "test1 = df.loc[df.label == '데이터 애널리스트'].iloc[:, 4].tolist()\n",
    "wordcloud_show(test1)\n",
    "# Preference\n",
    "print('DA: Preference')\n",
    "test1 = df.loc[df.label == '데이터 애널리스트'].iloc[:, 6].tolist()\n",
    "wordcloud_show(test1)\n",
    "\n",
    "# DE\n",
    "# Requirement\n",
    "print('DE: Requirement')\n",
    "test1 = df.loc[df.label == '데이터 엔지니어'].iloc[:, 4].tolist()\n",
    "wordcloud_show(test1)\n",
    "# Preference\n",
    "print('DE: Preference')\n",
    "test1 = df.loc[df.label == '데이터 엔지니어'].iloc[:, 6].tolist()\n",
    "wordcloud_show(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **StackTool**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회의 결과, `StackTool`에서의 한글은 `Requirement_KOR`, `Preference_KOR`에도 있는 내용이어서 제거하기로 함\n",
    "- `StackTool`은 한글을 지운 후에 남는 영어로 정리하기\n",
    "- `Requirement`와 `Preference`에 있는 영어엔, `StackTool`의 영어를 불용어 사전으로 적용하여 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "stack_list = df.StackTool.tolist()\n",
    "\n",
    "# 개행문자와 쉼표 제거 후, 쉼표로 join\n",
    "stack_list2 = []\n",
    "for data in stack_list:\n",
    "    new_str = data.replace('\\n', ' ').replace(',', ' ')\n",
    "    new_str2 = new_str.split()\n",
    "    new_str3 = ', '.join(new_str2)\n",
    "    stack_list2.append(new_str3)\n",
    "print(stack_list2)\n",
    "\n",
    "# 하나의 리스트로 합치기\n",
    "stack_merge = []\n",
    "for data in stack_list2:\n",
    "    spl = data.split(', ')\n",
    "    stack_merge.extend(spl)\n",
    "print(stack_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kor, Eng 분리\n",
    "kor_list = []\n",
    "eng_list = []\n",
    "for data in stack_merge:\n",
    "    kor_match = re.findall(r'[가-힣]+', data)\n",
    "    eng_match = re.findall(r'[a-zA-Z0-9.!@#$%^&*()+]+', data)\n",
    "    kor_list.extend(kor_match)\n",
    "    eng_list.extend(eng_match)\n",
    "print(kor_list)\n",
    "print(eng_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# 영어 소문자로 통일\n",
    "eng_lower = []\n",
    "for eng in eng_list:\n",
    "    eng_low = eng.lower()\n",
    "    eng_lower.append(eng_low)\n",
    "\n",
    "# 중복 요소 제거\n",
    "eng_list_remove = list(set(eng_lower))\n",
    "print(eng_list_remove)\n",
    "kor_list_remove = list(set(kor_list))\n",
    "print(kor_list_remove)\n",
    "\n",
    "# 텍스트 저장\n",
    "text_path = './stopwords_stacktool_eng.txt'\n",
    "with open(text_path, 'w', encoding='utf-8-sig') as file:\n",
    "    for data in eng_list_remove:\n",
    "        file.write(data + '\\n')\n",
    "text_path = './stopwords_stacktool_kor.txt'\n",
    "with open(text_path, 'w', encoding='utf-8-sig') as file:\n",
    "    for data in kor_list_remove:\n",
    "        file.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack_list2 소문자로 통일\n",
    "stack_list2_lower = []\n",
    "for data in stack_list2:\n",
    "    data_low = data.lower()\n",
    "    stack_list2_lower.append(data_low)\n",
    "print(stack_list2_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 파일 불러오기\n",
    "text_path = './stopwords_stacktool_kor.txt'\n",
    "stopwords = text_list_load(text_path)\n",
    "# print(stopwords)\n",
    "text_path = './custom_nouns_stacktool.txt'\n",
    "custom_nouns = text_list_load(text_path)\n",
    "# print(custom_nouns)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "custom_nouns = custom_nouns\n",
    "for noun in custom_nouns:\n",
    "    okt.add_dictionary(noun, 'Noun')\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger=okt,\n",
    "    passtags={'Noun'},\n",
    "    # replace=replace,\n",
    "    stopwords=stopwords,\n",
    "    # passwords=passwords,\n",
    "    # ngrams=ngrams\n",
    ")\n",
    "\n",
    "results = []\n",
    "for idx in range(0, df.shape[0]):\n",
    "    elem = stack_list2_lower[idx]\n",
    "    # print(postprocessor.pos(elem))\n",
    "    post_onechar = postprocessor.pos(elem)\n",
    "    result = []\n",
    "    for word, tag in post_onechar:\n",
    "        result.append(word)\n",
    "    results.append(result)\n",
    "    print(f'{idx}. {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 생성\n",
    "eng_final_df = pd.DataFrame({'StackTool_ENG': results})\n",
    "eng_final_df['StackTool_ENG'] = eng_final_df['StackTool_ENG'].apply(lambda x: ', '.join(x))\n",
    "eng_final_df.to_csv(csv_path + 'StackTool_ENG.csv', index=False)\n",
    "\n",
    "# insert\n",
    "df.insert(8, 'StackTool_ENG', eng_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = df[df.StackTool_ENG == ''].index\n",
    "# for i in idx:\n",
    "#     print(f'{i}. {df.StackTool[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requirement 쪽으로 가기\n",
    "    - Requirement, Preference 영어 전처리 해야 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
